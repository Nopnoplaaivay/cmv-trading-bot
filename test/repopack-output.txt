This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-08-14T05:02:50.685Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
backend/__init__.py
backend/api.py
backend/app.py
backend/common/configs/dnse.py
backend/common/configs/telegram.py
backend/common/consts.py
backend/common/responses/__init__.py
backend/common/responses/base.py
backend/common/responses/exceptions/__init__.py
backend/common/responses/exceptions/base_exceptions.py
backend/common/responses/pagination.py
backend/common/responses/success.py
backend/db/connectors/__init__.py
backend/db/connectors/sql_server.py
backend/db/sessions/__init__.py
backend/db/sessions/backend.py
backend/db/sessions/lake.py
backend/db/sessions/mart.py
backend/modules/admin/dtos/__init__.py
backend/modules/admin/dtos/admin.py
backend/modules/admin/handlers/__init__.py
backend/modules/admin/handlers/admin.py
backend/modules/admin/handlers/routers.py
backend/modules/admin/services/__init__.py
backend/modules/admin/services/admin.py
backend/modules/auth/cache/__init__.py
backend/modules/auth/cache/redis.py
backend/modules/auth/decorators/__init__.py
backend/modules/auth/decorators/payload.py
backend/modules/auth/dtos/__init__.py
backend/modules/auth/dtos/auth.py
backend/modules/auth/entities/__init__.py
backend/modules/auth/entities/sessions.py
backend/modules/auth/entities/users.py
backend/modules/auth/guards/__init__.py
backend/modules/auth/guards/auth.py
backend/modules/auth/guards/roles.py
backend/modules/auth/handlers/__init__.py
backend/modules/auth/handlers/auth.py
backend/modules/auth/handlers/routers.py
backend/modules/auth/repositories/__init__.py
backend/modules/auth/repositories/sessions.py
backend/modules/auth/repositories/users.py
backend/modules/auth/services/__init__.py
backend/modules/auth/services/auth.py
backend/modules/auth/types/__init__.py
backend/modules/auth/types/auth.py
backend/modules/base_daily.py
backend/modules/base_monthly.py
backend/modules/base_trading_client.py
backend/modules/base/dto.py
backend/modules/base/entities.py
backend/modules/base/query_builder.py
backend/modules/base/repositories.py
backend/modules/dnse/entities/__init__.py
backend/modules/dnse/entities/trading_tokens.py
backend/modules/dnse/repositories/__init__.py
backend/modules/dnse/repositories/trading_tokens.py
backend/modules/dnse/storage/__init__.py
backend/modules/dnse/storage/base.py
backend/modules/dnse/storage/redis.py
backend/modules/dnse/storage/sql_server.py
backend/modules/dnse/trading_api/__init__.py
backend/modules/dnse/trading_api/auth_client.py
backend/modules/dnse/trading_api/orders_client.py
backend/modules/dnse/trading_api/users_client.py
backend/modules/dnse/trading_session.py
backend/modules/notifications/__init__.py
backend/modules/notifications/service.py
backend/modules/notifications/telegram.py
backend/modules/portfolio/core/__init__.py
backend/modules/portfolio/core/portfolio_optimizer.py
backend/modules/portfolio/core/strategies/__init__.py
backend/modules/portfolio/core/strategies/base.py
backend/modules/portfolio/core/strategies/factory.py
backend/modules/portfolio/core/strategies/long_only.py
backend/modules/portfolio/core/strategies/market_neutral.py
backend/modules/portfolio/core/value_objects.py
backend/modules/portfolio/dtos/__init__.py
backend/modules/portfolio/dtos/accounts.py
backend/modules/portfolio/dtos/portfolio.py
backend/modules/portfolio/entities/__init__.py
backend/modules/portfolio/entities/accounts.py
backend/modules/portfolio/entities/balances.py
backend/modules/portfolio/entities/deals.py
backend/modules/portfolio/entities/orders.py
backend/modules/portfolio/entities/portfolio_metadata.py
backend/modules/portfolio/entities/portfolio.py
backend/modules/portfolio/entities/process_tracking.py
backend/modules/portfolio/entities/universe_top_monthly.py
backend/modules/portfolio/handlers/__init__.py
backend/modules/portfolio/handlers/accounts.py
backend/modules/portfolio/handlers/portfolio.py
backend/modules/portfolio/handlers/routers.py
backend/modules/portfolio/infrastructure/__init__.py
backend/modules/portfolio/infrastructure/report_generator.py
backend/modules/portfolio/infrastructure/trade_calendar.py
backend/modules/portfolio/repositories/__init__.py
backend/modules/portfolio/repositories/accounts.py
backend/modules/portfolio/repositories/balances.py
backend/modules/portfolio/repositories/deals.py
backend/modules/portfolio/repositories/orders.py
backend/modules/portfolio/repositories/portfolio_metadata.py
backend/modules/portfolio/repositories/portfolio.py
backend/modules/portfolio/repositories/process_tracking.py
backend/modules/portfolio/repositories/universe_top_monthly.py
backend/modules/portfolio/services/__init__.py
backend/modules/portfolio/services/data_providers/__init__.py
backend/modules/portfolio/services/data_providers/account_data_provider.py
backend/modules/portfolio/services/data_providers/portfolio_data_provider.py
backend/modules/portfolio/services/data_providers/price_data_provider.py
backend/modules/portfolio/services/portfolio_accounts_service.py
backend/modules/portfolio/services/portfolio_analysis_service.py
backend/modules/portfolio/services/portfolio_balance_service.py
backend/modules/portfolio/services/portfolio_daily_pipeline_service.py
backend/modules/portfolio/services/portfolio_deals_service.py
backend/modules/portfolio/services/portfolio_notification_service.py
backend/modules/portfolio/services/portfolio_scheduler_service.py
backend/modules/portfolio/services/portfolio_service.py
backend/modules/portfolio/services/portfolio_universe_service.py
backend/modules/portfolio/services/processors/__init__.py
backend/modules/portfolio/services/processors/portfolio_pnl_calculator.py
backend/modules/portfolio/services/processors/portfolio_processor.py
backend/modules/portfolio/services/processors/portfolio_risk_calculator.py
backend/modules/portfolio/services/processors/recommendation_engine.py
backend/modules/portfolio/utils/balance_utils.py
backend/modules/portfolio/utils/deals_utils.py
backend/modules/portfolio/utils/portfolio_utils.py
backend/redis/client.py
backend/utils/data_utils.py
backend/utils/json_utils.py
backend/utils/jwt_utils.py
backend/utils/logger.py
backend/utils/time_utils.py
frontend.py
frontend/__init__.py
frontend/components/__init__.py
frontend/components/analysis/__init__.py
frontend/components/analysis/account_comparision.py
frontend/components/analysis/account_positions.py
frontend/components/analysis/account_summary.py
frontend/components/analysis/analysis.py
frontend/components/analysis/portfolio_detailed_comparision.py
frontend/components/analysis/portfolio_selector.py
frontend/components/analysis/portfolio_summary_comparision.py
frontend/components/dashboard.py
frontend/components/footer.py
frontend/components/login.py
frontend/components/management/__init__.py
frontend/components/management/analysis_cache.py
frontend/components/management/portfolio_create.py
frontend/components/management/portfolio_edit.py
frontend/components/management/portfolio_list.py
frontend/components/management/portfolio_selector.py
frontend/components/sidebar.py
frontend/pages/__init__.py
frontend/pages/account_management.py
frontend/pages/order_history.py
frontend/pages/portfolio_analysis.py
frontend/pages/portfolio_management.py
frontend/pages/trade_execution.py
frontend/services/api.py
frontend/services/auth.py
frontend/services/portfolio.py
frontend/services/trading.py
frontend/styles/main.py
frontend/utils/__init__.py
frontend/utils/config.py
frontend/utils/helpers.py
pipeline_daily.py
pipeline_manual.py
server.py
test/export_portfolio.py
test/test_cmv.py
test/test_daily_portfolio_notification.py
test/test_dnse_session.py
test/test_pnl_chart.py
test/test_redis.py
test/test_service_update_data_monthly.py
test/test_time.py

================================================================
Repository Files
================================================================

================
File: backend/__init__.py
================
from dotenv import load_dotenv
import warnings

warnings.filterwarnings("ignore", category=RuntimeWarning)
load_dotenv(".env")

================
File: backend/api.py
================
from typing import List

from fastapi import APIRouter
from starlette.responses import JSONResponse

from backend.common.consts import MessageConsts
from backend.modules.base.dto import BaseDTO
from backend.modules.auth.handlers import auth_router
from backend.modules.admin.handlers import admin_router
from backend.modules.portfolio.handlers import portfolio_router, accounts_router
# from backend.modules.investors.handlers import investors_router
# from backend.modules.orders.handlers import orders_router


class ErrorDetailModel(BaseDTO):
    field: List[str]


class ErrorResponseModel(BaseDTO):
    statusCode: int
    message: str
    error: ErrorDetailModel


api_router = APIRouter(
    default_response_class=JSONResponse,
    responses={
        400: {"model": ErrorResponseModel},
        401: {"model": ErrorResponseModel},
        422: {"model": ErrorResponseModel},
        500: {"model": ErrorResponseModel},
    },
)


# api_router.include_router(investors_router, prefix="/investors-service", tags=["investors"])
# api_router.include_router(orders_router, prefix="/orders-service", tags=["orders"])
api_router.include_router(admin_router, prefix="/admin-service", tags=["admin"])
api_router.include_router(auth_router, prefix="/auth-service", tags=["auth"])
api_router.include_router(portfolio_router, prefix="/portfolio-service", tags=["portfolio"])
api_router.include_router(accounts_router, prefix="/accounts-service", tags=["accounts"])


@api_router.get("/healthcheck", include_in_schema=False)
def healthcheck():
    return JSONResponse(status_code=200, content={"message": MessageConsts.SUCCESS})

================
File: backend/app.py
================
import json
from fastapi import FastAPI, Request
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from starlette.responses import JSONResponse

from backend.api import api_router
from backend.common.consts import MessageConsts, CommonConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.utils.logger import LOGGER


app = FastAPI(
    title="PAPER TRADING APP",
    description="Welcome to API documentation",
    docs_url="/docs" if CommonConsts.DEBUG else None,
    redoc_url="/docs" if CommonConsts.DEBUG else None,
)
app_cors = CORSMiddleware(
    app, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)


async def pydantic_exception_handler(request: Request, exception: RequestValidationError):
    errors = {}
    for error in exception.errors():
        field = []
        _errors = errors
        if len(error["loc"]) == 1:
            if "ctx" in error and "discriminator_key" in error["ctx"]:
                field = error["ctx"]["discriminator_key"]
                if field not in _errors:
                    _errors[field] = []
                _errors[field].append(error["msg"])
                continue
        for i in range(1, len(error["loc"])):
            field = error["loc"][i]
            if field not in _errors:
                _errors[field] = [] if i == len(error["loc"]) - 1 else {}
            _errors = _errors[field]
        _errors.append(error["msg"])
    exception = BaseExceptionResponse(
        http_code=400,
        status_code=400,
        message=MessageConsts.BAD_REQUEST,
        errors=errors,
    )
    error_response = exception.to_dict()
    LOGGER.error(json.dumps(error_response))
    return JSONResponse(
        status_code=exception.http_code,
        content=error_response,
    )


async def response_exception_handler(request: Request, exception):
    if isinstance(exception, BaseExceptionResponse):
        error_response = exception.to_dict()
    else:
        errors = (
            None if not CommonConsts.DEBUG else {"key": MessageConsts.INTERNAL_SERVER_ERROR, "message": str(exception)}
        )
        exception = BaseExceptionResponse(
            http_code=500,
            status_code=500,
            message=MessageConsts.INTERNAL_SERVER_ERROR,
            errors=errors,
        )
        error_response = exception.to_dict()
    LOGGER.error(json.dumps(error_response))
    return JSONResponse(
        status_code=exception.http_code,
        content=error_response,
    )


@app.exception_handler(RequestValidationError)
async def exception_handler(request, exception):
    return await pydantic_exception_handler(request=request, exception=exception)


@app.exception_handler(Exception)
async def exception_handler(request, exception):
    return await response_exception_handler(request=request, exception=exception)


app.include_router(prefix="/api/v1", router=api_router)

================
File: backend/common/configs/dnse.py
================
import random
from typing import Dict, Any, Optional
from dataclasses import dataclass, field

from backend.common.consts import DNSEConsts


@dataclass
class TradingAPIConfig:
    base_url: str = DNSEConsts.BASE_URL
    timeout: int = 30
    max_retries: int = 3
    retry_delay: float = 1.0
    concurrent_limit: int = 10


@dataclass
class MarketDataConfig:
    broker: str = DNSEConsts.BROKER
    port: int = 443
    client_id: str = f"python-json-mqtt-ws-sub-{random.randint(0, 1000)}"
    first_reconnect_delay: int = 1
    reconnect_rate: int = 2
    max_reconnect_count: int = 12
    max_reconnect_delay: int = 60

================
File: backend/common/configs/telegram.py
================
import os
from typing import Optional


class TelegramConfig:
    def __init__(self):
        self.bot_token: Optional[str] = os.getenv("TELEGRAM_BOT_TOKEN")
        self.chat_id: Optional[str] = os.getenv("TELEGRAM_CHAT_ID")
        self.admin_chat_id: Optional[str] = os.getenv("TELEGRAM_ADMIN_CHAT_ID")

        self.enable_trade_alerts: bool = (os.getenv("TELEGRAM_TRADE_ALERTS", "true").lower() == "true")
        self.enable_portfolio_updates: bool = (os.getenv("TELEGRAM_PORTFOLIO_UPDATES", "true").lower() == "true")
        self.enable_system_alerts: bool = (os.getenv("TELEGRAM_SYSTEM_ALERTS", "true").lower() == "true")
        self.enable_market_alerts: bool = (os.getenv("TELEGRAM_MARKET_ALERTS", "false").lower() == "true")

        self.max_retries: int = int(os.getenv("TELEGRAM_MAX_RETRIES", "3"))
        self.retry_delay: float = float(os.getenv("TELEGRAM_RETRY_DELAY", "1.0"))

        self.silent_hours_start: int = int(os.getenv("TELEGRAM_SILENT_START", "22"))
        self.silent_hours_end: int = int(os.getenv("TELEGRAM_SILENT_END", "7")) 

    def is_enabled(self) -> bool:
        return bool(self.bot_token and self.chat_id)

    def is_silent_hour(self) -> bool:
        from datetime import datetime

        current_hour = datetime.now().hour

        if self.silent_hours_start <= self.silent_hours_end:
            return not (self.silent_hours_start <= current_hour < self.silent_hours_end)
        else:
            return not (
                current_hour >= self.silent_hours_start
                or current_hour < self.silent_hours_end
            )


telegram_config = TelegramConfig()

================
File: backend/common/consts.py
================
import os
import datetime

class SQLServerConsts:
    AUTH_SCHEMA = "BotAuth"
    PORTFOLIO_SCHEMA = "BotPortfolio"
    BROKERS_SCHEMA = "BotBrokers"


    TRADING_TIME_FORMAT = "%Y-%m-%d %H:%M:%S"
    GMT_7_NOW = f"SWITCHOFFSET(SYSUTCDATETIME(), '+07:00')"
    GMT_7_NOW_VARCHAR = f"FORMAT(SWITCHOFFSET(SYSUTCDATETIME(), '+07:00'), 'yyyy-MM-dd HH:mm:ss')"

    DATE_FORMAT = "%Y-%m-%d"
    START_TRADING_MONTH = '2025-01'
    # START_TRADING_DAY = datetime.datetime.strptime(os.environ.get("START_TRADING_DAY", None), "%Y-%m-%d")
    START_TRADING_DAY = datetime.datetime.strptime('2025-01-01', "%Y-%m-%d")
    TRADING_DAY_FORMAT = "%Y-%m-%d"

class CommonConsts:
    ROOT_FOLDER = os.path.abspath(os.path.join(os.path.abspath(__file__), 3 * "../"))

    SALT = os.getenv("SALT")
    AT_SECRET_KEY = os.getenv("AT_SECRET_KEY")
    RT_SECRET_KEY = os.getenv("RT_SECRET_KEY")
    ACCESS_TOKEN_EXPIRES_IN = 3600 * 8  # 8 hours
    REFRESH_TOKEN_EXPIRES_IN = 86400

    DEBUG = os.getenv("DEBUG") 

class MessageConsts:
    CREATED = "Created"
    SUCCESS = "Success"
    VALIDATION_FAILED = "Validation failed"
    UNAUTHORIZED = "Unauthorized"
    BAD_REQUEST = "Bad request"
    FORBIDDEN = "Forbidden"
    NOT_FOUND = "Not found"
    CONFLICT = "Conflict"
    INVALID_OBJECT_ID = "Invalid object id"
    INVALID_INPUT = "Invalid input"
    INTERNAL_SERVER_ERROR = "Unknown internal server error"
    TRADING_API_ERROR = "Trading API error"

class DNSEConsts:
    BASE_URL = "https://api.dnse.com.vn"
    BROKER = 'datafeed-lts-krx.dnse.com.vn'


class TradingConsts:
    LIMIT_WEIGHT_PCT = 10

================
File: backend/common/responses/__init__.py
================
from .base import BaseResponse
from .success import SuccessResponse
from .pagination import PaginationResponse

================
File: backend/common/responses/base.py
================
from typing import List, Dict, Union, Any


class BaseResponse:
    def __init__(
        self,
        http_code: int,
        status_code: int,
        message: str,
        data: Union[List, Dict, Any] = None,
        errors: Dict = None,
    ):
        self.http_code = http_code
        self.status_code = status_code
        self.message = message
        self.data = data
        self.errors = errors

    def to_dict(self) -> Dict:
        result = {"statusCode": self.status_code, "message": self.message}
        if self.data:
            result["data"] = self.data
        if self.errors:
            result["errors"] = self.errors
        return result

================
File: backend/common/responses/exceptions/__init__.py
================
from .base_exceptions import BaseExceptionResponse

================
File: backend/common/responses/exceptions/base_exceptions.py
================
from backend.common.responses import BaseResponse


class BaseExceptionResponse(BaseResponse, Exception):
    def __init__(self, http_code, status_code, message, errors=None):
        BaseResponse.__init__(self, http_code=http_code, status_code=status_code, message=message, errors=errors)
        Exception.__init__(self, str({"message": message, "errors": errors}))

================
File: backend/common/responses/pagination.py
================
from typing import Dict
from backend.common.responses import BaseResponse


class PaginationResponse(BaseResponse):
    def __init__(self, http_code, status_code, message, data, page, page_size, total):
        super().__init__(http_code=http_code, status_code=status_code, message=message, data=data)
        self.page = page
        self.page_size = page_size
        self.total = total

    def to_dict(self) -> Dict:
        result = super().to_dict()
        if self.page is not None:
            result["page"] = self.page
        if self.page_size is not None:
            result["pageSize"] = self.page_size
        if self.total is not None:
            result["total"] = self.total
        return result

================
File: backend/common/responses/success.py
================
from backend.common.responses import BaseResponse


class SuccessResponse(BaseResponse):
    def __init__(self, http_code, status_code, message, data=None):
        super().__init__(http_code=http_code, status_code=status_code, message=message, data=data)

================
File: backend/db/connectors/__init__.py
================
from .sql_server import SQLServerConnectorPool
import contextvars

CONTEXTVAR = contextvars.ContextVar("var", default=None)

================
File: backend/db/connectors/sql_server.py
================
import pyodbc

from sqlalchemy import create_engine
from sqlalchemy.orm import Session
from sqlalchemy.pool import QueuePool

from backend.utils.logger import LOGGER

pyodbc.pooling = False


LOGGER_PREFIX = "[SQLServer]"

class SQLServerConnectorPool:
    def __init__(self, dns, max_conn, min_conn):
        self.dns = dns
        self.max_conn = max_conn
        self.min_conn = min_conn
        self.engine = create_engine(
            # DNS,
            "mssql+pyodbc://",
            poolclass=QueuePool,
            pool_pre_ping=True,
            pool_size=self.max_conn - self.min_conn,
            max_overflow=self.min_conn,
            pool_timeout=60 * 60,
            creator=self.__get_conn__,
            fast_executemany=True,
        )
        try:
            session = self.get()
            self.put(session)
            LOGGER.info(f"{LOGGER_PREFIX} Successfully created connection...")
        except Exception as e:
            raise Exception(f"Could not create connection to {dns}", e)
            pass

    def __get_conn__(self):
        c = pyodbc.connect(self.dns)
        return c

    def get(self) -> Session:
        return Session(bind=self.engine.connect())

    @classmethod
    def put(cls, session):
        session.close()

        
    def close(self):
        """Close the connection pool"""
        self.engine.dispose()

================
File: backend/db/sessions/__init__.py
================
from .backend import backend_session_scope
from .mart import mart_session_scope
from .lake import lake_session_scope

================
File: backend/db/sessions/backend.py
================
import os
from contextlib import contextmanager
from typing import ContextManager
import uuid

from sqlalchemy.orm import Session

from backend.db.connectors import CONTEXTVAR, SQLServerConnectorPool
from backend.utils.logger import LOGGER


PREFIX = "LOCAL_BACKEND" if os.getenv("TEST") == "1" else "PROD_BACKEND"
SESSIONS = {}
DNS = os.environ.get(f"{PREFIX}_DNS", None)
MIN_CONN = 5
MAX_CONN = 20

POOL = SQLServerConnectorPool(dns=DNS, max_conn=MAX_CONN, min_conn=MIN_CONN)


def set_session(session):
    global SESSIONS
    context_id = CONTEXTVAR.get()
    if session is None:
        del SESSIONS[context_id]
        return
    if context_id is None:
        context_id = uuid.uuid4()
    CONTEXTVAR.set(context_id)
    SESSIONS[context_id] = session


def get_session():
    global SESSIONS
    context_id = CONTEXTVAR.get()
    return SESSIONS.get(context_id, None)


@contextmanager
def backend_session_scope(new=False) -> ContextManager[Session]:
    """
    Provide a transactional scope around a series of operations.
    Shouldn't keep session alive too long, it will block a connection of pool connections.
    """
    if not new:
        session: Session
        reuse_session = get_session()
        if reuse_session is None:
            session = POOL.get()
            set_session(session=session)
        else:
            session = reuse_session
        try:
            yield session
            if reuse_session is None:
                session.commit()
        except Exception as exception:
            LOGGER.error(exception, exc_info=True)
            if reuse_session is None:
                session.rollback()
            raise exception
        finally:
            if reuse_session is None:
                POOL.put(session)
                set_session(session=None)
    else:
        session = POOL.get()
        try:
            yield session
            session.commit()
        except Exception as exception:
            LOGGER.error(exception, exc_info=True)
            session.rollback()
            raise exception
        finally:
            POOL.put(session)

================
File: backend/db/sessions/lake.py
================
import os
from contextlib import contextmanager
from typing import ContextManager
import uuid

from sqlalchemy.orm import Session

from backend.db.connectors import CONTEXTVAR, SQLServerConnectorPool
from backend.utils.logger import LOGGER


PREFIX = "LOCAL_LAKE" if os.getenv("TEST") == "1" else "PROD_LAKE"
SESSIONS = {}
DNS = os.environ.get(f"{PREFIX}_DNS", None)
MIN_CONN = 5
MAX_CONN = 20

POOL = SQLServerConnectorPool(dns=DNS, max_conn=MAX_CONN, min_conn=MIN_CONN)


def set_session(session):
    global SESSIONS
    context_id = CONTEXTVAR.get()
    if session is None:
        del SESSIONS[context_id]
        return
    if context_id is None:
        context_id = uuid.uuid4()
    CONTEXTVAR.set(context_id)
    SESSIONS[context_id] = session


def get_session():
    global SESSIONS
    context_id = CONTEXTVAR.get()
    return SESSIONS.get(context_id, None)


@contextmanager
def lake_session_scope(new=False) -> ContextManager[Session]:
    """
    Provide a transactional scope around a series of operations.
    Shouldn't keep session alive too long, it will block a connection of pool connections.
    """
    if not new:
        session: Session
        reuse_session = get_session()
        if reuse_session is None:
            session = POOL.get()
            set_session(session=session)
        else:
            session = reuse_session
        try:
            yield session
            if reuse_session is None:
                session.commit()
        except Exception as exception:
            LOGGER.error(exception, exc_info=True)
            if reuse_session is None:
                session.rollback()
            raise exception
        finally:
            if reuse_session is None:
                POOL.put(session)
                set_session(session=None)
    else:
        session = POOL.get()
        try:
            yield session
            session.commit()
        except Exception as exception:
            LOGGER.error(exception, exc_info=True)
            session.rollback()
            raise exception
        finally:
            POOL.put(session)

================
File: backend/db/sessions/mart.py
================
import os
from contextlib import contextmanager
from typing import ContextManager
import uuid

from sqlalchemy.orm import Session

from backend.db.connectors import CONTEXTVAR, SQLServerConnectorPool
from backend.utils.logger import LOGGER


PREFIX = "LOCAL_MART" if os.getenv("TEST") == "1" else "PROD_MART"
SESSIONS = {}
DNS = os.environ.get(f"{PREFIX}_DNS", None)
MIN_CONN = 5
MAX_CONN = 20

POOL = SQLServerConnectorPool(dns=DNS, max_conn=MAX_CONN, min_conn=MIN_CONN)


def set_session(session):
    global SESSIONS
    context_id = CONTEXTVAR.get()
    if session is None:
        del SESSIONS[context_id]
        return
    if context_id is None:
        context_id = uuid.uuid4()
    CONTEXTVAR.set(context_id)
    SESSIONS[context_id] = session


def get_session():
    global SESSIONS
    context_id = CONTEXTVAR.get()
    return SESSIONS.get(context_id, None)


@contextmanager
def mart_session_scope(new=False) -> ContextManager[Session]:
    """
    Provide a transactional scope around a series of operations.
    Shouldn't keep session alive too long, it will block a connection of pool connections.
    """
    if not new:
        session: Session
        reuse_session = get_session()
        if reuse_session is None:
            session = POOL.get()
            set_session(session=session)
        else:
            session = reuse_session
        try:
            yield session
            if reuse_session is None:
                session.commit()
        except Exception as exception:
            LOGGER.error(exception, exc_info=True)
            if reuse_session is None:
                session.rollback()
            raise exception
        finally:
            if reuse_session is None:
                POOL.put(session)
                set_session(session=None)
    else:
        session = POOL.get()
        try:
            yield session
            session.commit()
        except Exception as exception:
            LOGGER.error(exception, exc_info=True)
            session.rollback()
            raise exception
        finally:
            POOL.put(session)

================
File: backend/modules/admin/dtos/__init__.py
================
from .admin import UpdateRoleDTO

================
File: backend/modules/admin/dtos/admin.py
================
from backend.modules.base.dto import BaseDTO


class UpdateRoleDTO(BaseDTO):
    target_account: str
    new_role: str

================
File: backend/modules/admin/handlers/__init__.py
================
from .routers import admin_router
from .admin import (
    get_users,
    update_balances,
    update_deals,
    update_user_role,
    run_daily_pipeline,
    send_system_notification
)

================
File: backend/modules/admin/handlers/admin.py
================
from fastapi import Depends
from starlette.responses import JSONResponse

from backend.common.consts import MessageConsts
from backend.common.responses.base import BaseResponse
from backend.common.responses import SuccessResponse
from backend.modules.admin.handlers import admin_router
from backend.modules.admin.services import AdminService
from backend.modules.portfolio.services import (
    BalanceService, 
    DealsService,
    DailyDataPipelineService,
    PortfolioNotificationService
)
from backend.modules.auth.decorators import UserPayload
from backend.modules.auth.guards import auth_guard, admin_guard
from backend.modules.auth.types import JwtPayload
from backend.modules.admin.dtos import UpdateRoleDTO


@admin_router.get("/get-users", dependencies=[Depends(auth_guard), Depends(admin_guard)])
async def get_users(user: JwtPayload = Depends(UserPayload)):
    users = await AdminService.get_all_users()
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=users,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@admin_router.get("/update-balances", dependencies=[Depends(auth_guard), Depends(admin_guard)])
async def update_balances(user: JwtPayload = Depends(UserPayload)):
    success = await BalanceService.update_newest_balances_daily()
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=success,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@admin_router.get("/update-deals", dependencies=[Depends(auth_guard), Depends(admin_guard)])
async def update_deals(user: JwtPayload = Depends(UserPayload)):
    success = await DealsService.update_newest_deals_daily()
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=success,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@admin_router.post("/update-role", dependencies=[Depends(auth_guard), Depends(admin_guard)])
async def update_user_role(payload: UpdateRoleDTO, user: JwtPayload = Depends(UserPayload)):
    updated_user = await AdminService.update_user_role(admin_user=user, payload=payload)
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=updated_user,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@admin_router.get("/send-system-notification", dependencies=[Depends(auth_guard), Depends(admin_guard)])
async def send_system_notification(user: JwtPayload = Depends(UserPayload)):
    try:
        notification_results = await PortfolioNotificationService.send_daily_system_portfolio()
        response = SuccessResponse(
            http_code=200,
            status_code=200,
            message=MessageConsts.SUCCESS,
            data=notification_results,
        )
        return JSONResponse(status_code=response.http_code, content=response.to_dict())
    except Exception as e:
        response = BaseResponse(
            http_code=500,
            status_code=500,
            message=str(e),
            errors=None
        )
        return JSONResponse(status_code=response.http_code, content=response.to_dict())


@admin_router.get("/run-pipeline", dependencies=[Depends(auth_guard), Depends(admin_guard)])
async def run_daily_pipeline(user: JwtPayload = Depends(UserPayload)):
    try:
        pipeline_results = await DailyDataPipelineService.run_manual()
        response = SuccessResponse(
            http_code=200,
            status_code=200,
            message=MessageConsts.SUCCESS,
            data=pipeline_results,
        )
        return JSONResponse(status_code=response.http_code, content=response.to_dict())
    except Exception as e:
        response = BaseResponse(
            http_code=500,
            status_code=500,
            message=str(e),
            errors=None
        )
        return JSONResponse(status_code=response.http_code, content=response.to_dict())

================
File: backend/modules/admin/handlers/routers.py
================
from fastapi import APIRouter

admin_router = APIRouter()

================
File: backend/modules/admin/services/__init__.py
================
from .admin import AdminService

================
File: backend/modules/admin/services/admin.py
================
from typing import Dict

from backend.common.responses.exceptions import BaseExceptionResponse
from backend.common.consts import SQLServerConsts, MessageConsts
from backend.modules.base.query_builder import TextSQL
from backend.modules.auth.types import JwtPayload
from backend.modules.auth.entities import Users, Sessions
from backend.modules.auth.repositories import UsersRepo, SessionsRepo
from backend.modules.admin.dtos import UpdateRoleDTO
from backend.utils.logger import LOGGER


class AdminService:
    @classmethod
    async def get_all_users(cls):
        records = await UsersRepo.get_all()
        return records

    @classmethod
    async def update_user_role(
        cls, admin_user: JwtPayload, payload: UpdateRoleDTO
    ) -> Dict[str, str]:
        if admin_user.role != "admin":
            raise BaseExceptionResponse(
                http_code=403,
                status_code=403,
                message=MessageConsts.FORBIDDEN,
                errors="Only admin users can update roles",
            )

        # Validate the new role
        valid_roles = ["free", "premium", "admin"]
        if payload.new_role not in valid_roles:
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message=MessageConsts.BAD_REQUEST,
                errors=f"Invalid role. Valid roles are: {', '.join(valid_roles)}",
            )

        # Check if target user exists
        target_users = await UsersRepo.get_by_condition(
            {Users.account.name: payload.target_account}
        )
        if not target_users:
            raise BaseExceptionResponse(
                http_code=404,
                status_code=404,
                message=MessageConsts.NOT_FOUND,
                errors="Target user not found",
            )

        target_user = target_users[0]

        # Prevent admin from demoting themselves
        if (
            target_user[Users.id.name] == admin_user.userId
            and payload.new_role != "admin"
        ):
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message=MessageConsts.BAD_REQUEST,
                errors="Admin cannot demote their own account",
            )

        # Update the user's role
        await UsersRepo.update(
            record={
                Users.id.name: target_user[Users.id.name],
                Users.account.name: payload.target_account,
                Users.role.name: payload.new_role,
            },
            identity_columns=[Users.id.name],
            returning=False,
            text_clauses={"__updatedAt__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)},
        )

        # Update all active sessions of the target user with new role
        target_user_sessions = await SessionsRepo.get_by_condition(
            {Sessions.userId.name: target_user[Users.id.name]}
        )
        if len(target_user_sessions) > 0:
            sessions = [
                {
                    Sessions.id.name: session[Sessions.id.name],
                    Sessions.role.name: payload.new_role,
                }
                for session in target_user_sessions
            ]
            await SessionsRepo.update_many(
                records=sessions,
                identity_columns=[Sessions.id.name],
                returning=False,
                text_clauses={"__updatedAt__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)},
            )

        LOGGER.info(
            f"Admin {admin_user.userId} updated user {payload.target_account} role to {payload.new_role}"
        )

        return {
            "message": f"Successfully updated {payload.target_account} role to {payload.new_role}",
            "target_account": payload.target_account,
            "new_role": payload.new_role,
            "updated_by": admin_user.userId,
        }

================
File: backend/modules/auth/cache/__init__.py
================
from .redis import RedisBlacklist

================
File: backend/modules/auth/cache/redis.py
================
from typing import Optional, Union
from uuid import uuid4

from backend.common.consts import SQLServerConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.dnse.entities import TradingTokens
from backend.modules.dnse.storage.base import BaseTokenStorage
from backend.redis.client import REDIS_CLIENT
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils

try:
    conn = REDIS_CLIENT.get_conn()
    REDIS_AVAILABLE = True
except Exception as e:
    LOGGER.error(f"Failed to connect to Redis: {e}")
    LOGGER.info("Redis is not available, falling back to SQL Server")
    REDIS_AVAILABLE = False


class RedisBlacklist:
    @staticmethod
    def set_session(key: str, value: int) -> None:
        if REDIS_AVAILABLE:
            try:
                conn.hset(f"SESSION_BLACKLIST:{key}", mapping={"exp": value})
            except Exception as e:
                LOGGER.error(f"Failed to set Redis blacklist item: {e}")

    @staticmethod
    def get_session(key: str) -> Optional[str]:
        if REDIS_AVAILABLE:
            try:
                return conn.hget(f"SESSION_BLACKLIST:{key}", "exp")
            except Exception as e:
                LOGGER.error(f"Failed to get Redis blacklist item: {e}")
        return None

================
File: backend/modules/auth/decorators/__init__.py
================
from .payload import UserPayload

================
File: backend/modules/auth/decorators/payload.py
================
from fastapi import Request

from backend.modules.auth.types.auth import JwtPayload
from backend.common.responses.exceptions import BaseExceptionResponse


def UserPayload(request: Request) -> JwtPayload:
    user = getattr(request.state, "user", None)
    if not user:
        raise BaseExceptionResponse(
            http_code=401,
            status_code=401,
            message="Unauthorized",
            errors="User not authenticated"
        )
    return user

================
File: backend/modules/auth/dtos/__init__.py
================
from .auth import RegisterDTO, LoginDTO, LogoutDTO, RefreshDTO

================
File: backend/modules/auth/dtos/auth.py
================
from typing import Optional
from backend.modules.base.dto import BaseDTO


class RegisterDTO(BaseDTO):
    account: str
    password: str
    confirm_password: str


class LoginDTO(BaseDTO):
    account: str
    password: str


class LogoutDTO(BaseDTO):
    sessionId: str
    userId: int
    role: str
    iat: Optional[int] = None
    exp: Optional[int] = None


class RefreshDTO(LogoutDTO):
    signature: str


class LoginResDTO(BaseDTO):
    access_token: str
    refresh_token: str


class RefreshResDTO(BaseDTO):
    access_token: str

================
File: backend/modules/auth/entities/__init__.py
================
from .users import Users
from .sessions import Sessions

================
File: backend/modules/auth/entities/sessions.py
================
from sqlalchemy import Column, Integer, String, ForeignKey, DateTime
from sqlalchemy.orm import relationship

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class Sessions(Base):
    __tablename__ = "sessions"
    __table_args__ = ({"schema": SQLServerConsts.AUTH_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.AUTH_SCHEMA}].[{__tablename__}]"

    id = Column(String, primary_key=True, index=True, nullable=False)
    signature = Column(String, nullable=False)
    expires_at = Column(DateTime, nullable=False)
    role = Column(String)
    userId = Column(
        Integer,
        ForeignKey(f"{SQLServerConsts.AUTH_SCHEMA}.users.id", ondelete="CASCADE"),
        nullable=False,
    )

    # Relationship
    user = relationship("Users", back_populates="sessions")

================
File: backend/modules/auth/entities/users.py
================
from sqlalchemy import Column, Integer, String, Enum as SQLAlchemyEnum
from sqlalchemy.orm import relationship
from enum import Enum

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class Role(Enum):
    ADMIN = "admin"
    FREE = "free"
    PREMIUM = "premium"


class Users(Base):
    __tablename__ = 'users'
    __table_args__ = (
        {"schema": SQLServerConsts.AUTH_SCHEMA},
        )
    __sqlServerType__ = f"[{SQLServerConsts.AUTH_SCHEMA}].[{__tablename__}]"
    id = Column(Integer, primary_key=True, nullable=False, autoincrement=True, index=True)
    account = Column(String, unique=True, index=True, nullable=False)
    password = Column(String, nullable=False)
    role = Column(SQLAlchemyEnum(Role), nullable=False)
    mobile = Column(String)
    email = Column(String)
    sessions = relationship('Sessions', back_populates='user', cascade='all, delete-orphan')
    accounts = relationship('Accounts', back_populates='user', cascade='all, delete-orphan')

================
File: backend/modules/auth/guards/__init__.py
================
from .auth import auth_guard
from .roles import admin_guard

================
File: backend/modules/auth/guards/auth.py
================
from fastapi import Request
from typing import Optional

from backend.common.responses.exceptions import BaseExceptionResponse
from backend.common.consts import MessageConsts
from backend.modules.auth.services import AuthService


class AuthGuard:
    @classmethod
    async def activate(cls, request: Request):
        token = cls.extract_token_from_header(request)
        if not token:
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Missing or invalid token"
            )

        request.state.user = await AuthService.verify_access_token(access_token=token)
        return True

    def extract_token_from_header(request: Request) -> Optional[str]:
        auth_header = request.headers.get("Authorization")
        if not auth_header:
            return None
        
        parts = auth_header.split()
        if len(parts) == 2 and parts[0].lower() == "bearer":
            return parts[1]
        return None
    
async def auth_guard(request: Request):
    return await AuthGuard.activate(request)

================
File: backend/modules/auth/guards/roles.py
================
from typing import Optional
from fastapi import Request

from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.auth.services import AuthService


class RoleGuard:
    @classmethod
    async def activate(cls, request: Request, required_role: str = None):
        if not required_role:
            return True

        token = cls.extract_token_from_header(request)
        if not token:
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message="Unauthorized",
                errors="Missing or invalid token"
            )

        request.state.user = await AuthService.verify_access_token(access_token=token)
        user = request.state.user
        if not user:
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message="Unauthorized",
                errors="Missing or invalid token"
            )
        if user.role != required_role:
            raise BaseExceptionResponse(
                http_code=403,
                status_code=403,
                message="Forbidden",
                errors=f"User does not have the required role: {required_role}"
            )
        return True

    def extract_token_from_header(request: Request) -> Optional[str]:
        auth_header = request.headers.get("Authorization")
        if not auth_header:
            return None

        parts = auth_header.split()
        if len(parts) == 2 and parts[0].lower() == "bearer":
            return parts[1]
        return None

    
async def admin_guard(request: Request):
    return await RoleGuard.activate(request, required_role="admin")

================
File: backend/modules/auth/handlers/__init__.py
================
from .routers import auth_router
from .auth import register, login, logout, refresh_token

================
File: backend/modules/auth/handlers/auth.py
================
from fastapi import Depends, Request
from starlette.responses import JSONResponse

from backend.common.consts import MessageConsts
from backend.common.responses.base import BaseResponse
from backend.common.responses import SuccessResponse
from backend.modules.auth.dtos import (
    RegisterDTO,
    LoginDTO,
    LogoutDTO,
    RefreshDTO
)
from backend.modules.auth.handlers import auth_router
from backend.modules.auth.services import AuthService
from backend.modules.auth.guards.roles import admin_guard


@auth_router.post("/register")
async def register(payload: RegisterDTO):
    await AuthService.register(payload=payload)
    return BaseResponse(http_code=200, status_code=200, message=MessageConsts.SUCCESS)


@auth_router.post("/login")
async def login(payload: LoginDTO):
    token_pair = await AuthService.login(payload=payload)
    response = SuccessResponse(
        http_code=200, status_code=200, message=MessageConsts.SUCCESS, data=token_pair
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@auth_router.post("/logout")
async def logout(payload: LogoutDTO):
    await AuthService.logout(payload=payload)
    return BaseResponse(http_code=200, status_code=200, message=MessageConsts.SUCCESS)


@auth_router.post("/refresh")
async def refresh_token(payload: RefreshDTO):
    token_pair = await AuthService.refresh_token(payload=payload)
    response = SuccessResponse(
        http_code=200, status_code=200, message=MessageConsts.SUCCESS, data=token_pair
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())

================
File: backend/modules/auth/handlers/routers.py
================
from fastapi import APIRouter

auth_router = APIRouter()

================
File: backend/modules/auth/repositories/__init__.py
================
from .users import UsersRepo
from .sessions import SessionsRepo

================
File: backend/modules/auth/repositories/sessions.py
================
from typing import Dict

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.auth.entities import Sessions


class SessionsRepo(BaseRepo[Dict]):
    entity = Sessions
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

================
File: backend/modules/auth/repositories/users.py
================
from typing import Dict

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.auth.entities import Users


class UsersRepo(BaseRepo[Dict]):
    entity = Users
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

================
File: backend/modules/auth/services/__init__.py
================
from .auth import AuthService

================
File: backend/modules/auth/services/auth.py
================
import os
import hashlib
import uuid
import jwt
from datetime import timedelta
from typing import Dict

from backend.common.consts import MessageConsts, CommonConsts, SQLServerConsts
from backend.modules.base.query_builder import TextSQL
from backend.common.responses.exceptions.base_exceptions import BaseExceptionResponse
from backend.modules.auth.dtos import (
    RegisterDTO,
    LoginDTO,
    LogoutDTO,
    RefreshDTO
)
from backend.modules.auth.types import JwtPayload, RefreshPayload
from backend.modules.auth.entities import Users, Sessions
from backend.modules.auth.repositories import UsersRepo, SessionsRepo
from backend.modules.auth.cache import RedisBlacklist
from backend.utils.jwt_utils import JWTUtils
from backend.utils.time_utils import TimeUtils
from backend.utils.logger import LOGGER

# Initialize RedisBlacklist with TTLCache fallback
# black_list = RedisBlacklist(maxsize=1000, ttl=24 * 60 * 60)


class AuthService:
    @classmethod
    async def register(cls, payload: RegisterDTO) -> None:
        if await UsersRepo.get_by_condition({Users.account.name: payload.account}):
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message=MessageConsts.BAD_REQUEST,
                errors="Account already exists",
            )
        if payload.password != payload.confirm_password:
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message=MessageConsts.BAD_REQUEST,
                errors="Passwords do not match",
            )
        salted_password = f"{CommonConsts.SALT}{payload.password}"
        await UsersRepo.insert(
            record={
                Users.account.name: payload.account,
                Users.password.name: hashlib.sha256(
                    salted_password.encode("utf-8")
                ).hexdigest(),
            },
            returning=False,
        )

        LOGGER.info(f"User {payload.account} has been created")

    @classmethod
    async def login(cls, payload: LoginDTO) -> Dict[str, str]:
        records = await UsersRepo.get_by_condition(
            {Users.account.name: payload.account}
        )
        user = records[0] if records else None
        if not user:
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message=MessageConsts.BAD_REQUEST,
                errors="Account does not exist",
            )
        salted_password = f"{CommonConsts.SALT}{payload.password}"
        if (
            user[Users.password.name]
            != hashlib.sha256(salted_password.encode("utf-8")).hexdigest()
        ):
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Invalid credentials",
            )
        return await cls.create_token_pair(user)

    @classmethod
    async def create_token_pair(cls, user: Dict) -> Dict[str, str]:
        session_id = str(uuid.uuid4())
        signature = os.urandom(16).hex()
        expires_at = TimeUtils.get_current_vn_time() + timedelta(
            seconds=CommonConsts.REFRESH_TOKEN_EXPIRES_IN
        )
        session = await SessionsRepo.insert(
            record={
                Sessions.id.name: session_id,
                Sessions.userId.name: user[Users.id.name],
                Sessions.signature.name: signature,
                Sessions.expires_at.name: expires_at,
                Sessions.role.name: user[Users.role.name],
            },
            returning=True,
        )

        at_payload = JwtPayload(
            sessionId=session[Sessions.id.name],
            userId=user[Users.id.name],
            role=user[Users.role.name],
        )
        rt_payload = RefreshPayload(
            sessionId=session[Sessions.id.name],
            userId=user[Users.id.name],
            role=user[Users.role.name],
            signature=signature,
        )

        access_token = JWTUtils.create_access_token(payload=at_payload)
        refresh_token = JWTUtils.create_refresh_token(payload=rt_payload)

        return {"accessToken": access_token, "refreshToken": refresh_token}

    @classmethod
    async def logout(cls, payload: LogoutDTO):
        key = f"{payload.userId}:{payload.sessionId}"
        exp = payload.exp if payload.exp else 0
        RedisBlacklist.set_session(key, exp)
        session = await SessionsRepo.get_by_condition(
            {Sessions.id.name: payload.sessionId}
        )
        if session:
            await SessionsRepo.delete({Sessions.id.name: payload.sessionId})

        # Show blacklist status and storage info
        LOGGER.info(f"User {payload.userId} has been logged out")

    @classmethod
    async def refresh_token(cls, payload: RefreshDTO):
        sessions = await SessionsRepo.get_by_condition(
            {Sessions.id.name: payload.sessionId}
        )
        if len(sessions) == 0:
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Invalid session",
            )
        session = sessions[0]
        if session[Sessions.signature.name] != payload.signature:
            # Remove all sessions of the user
            await SessionsRepo.delete({Sessions.userId.name: payload.userId})
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Invalid signature",
            )

        new_signature = os.urandom(16).hex()
        await SessionsRepo.update(
            record={
                Sessions.id.name: payload.sessionId,
                Sessions.signature.name: new_signature,
            },
            identity_columns=[Sessions.id.name],
            returning=False,
            text_clauses={"__updated_at__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)},
        )

        access_token_payload = JwtPayload(
            sessionId=session[Sessions.id.name],
            userId=payload.userId,
            role=payload.role,
        )
        refresh_token_payload = RefreshPayload(
            sessionId=session[Sessions.id.name],
            userId=payload.userId,
            role=payload.role,
            signature=new_signature,
        )
        access_token = JWTUtils.create_access_token(payload=access_token_payload)
        refresh_token = JWTUtils.create_refresh_token(
            payload=refresh_token_payload
        )  
        return {"accessToken": access_token, "refreshToken": refresh_token}

    @classmethod
    async def verify_access_token(cls, access_token: str):
        try:
            decoded_payload = JWTUtils.decode_token(
                token=access_token, secret_key=CommonConsts.AT_SECRET_KEY
            )

            session_id = decoded_payload.get("sessionId")
            user_id = decoded_payload.get("userId")
            role = decoded_payload.get("role")
            iat = decoded_payload.get("iat")
            exp = decoded_payload.get("exp")

            # Check if the token is blacklisted
            blacklist_key = f"{user_id}:{session_id}"
            if RedisBlacklist.get_session(blacklist_key):
                sessions = await SessionsRepo.delete({Sessions.userId.name: user_id})
                raise BaseExceptionResponse(
                    http_code=401,
                    status_code=401,
                    message=MessageConsts.UNAUTHORIZED,
                    errors="Token has been revoked",
                )

            # Validate the session associated with the token
            sessions = await SessionsRepo.get_by_condition(
                {Sessions.id.name: session_id}
            )
            if not sessions:
                raise BaseExceptionResponse(
                    http_code=401,
                    status_code=401,
                    message=MessageConsts.UNAUTHORIZED,
                    errors="Invalid session",
                )

            session = sessions[0]
            if session[Sessions.userId.name] != user_id:
                raise BaseExceptionResponse(
                    http_code=401,
                    status_code=401,
                    message=MessageConsts.UNAUTHORIZED,
                    errors="Session does not match user",
                )

            return JwtPayload(
                sessionId=session_id, userId=user_id, role=role, iat=iat, exp=exp
            )

        except jwt.ExpiredSignatureError:
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Token has expired",
            )
        except jwt.InvalidTokenError:
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Invalid token",
            )

    @classmethod
    async def verify_refresh_token(cls, refresh_token: str):
        try:
            decoded_payload = JWTUtils.decode_token(
                token=refresh_token, secret_key=CommonConsts.RT_SECRET_KEY
            )

            session_id = decoded_payload.get("sessionId")
            user_id = decoded_payload.get("userId")
            role = decoded_payload.get("role")
            iat = decoded_payload.get("iat")
            exp = decoded_payload.get("exp")

            sessions = await SessionsRepo.get_by_condition(
                {Sessions.id.name: session_id}
            )
            if not sessions:
                raise BaseExceptionResponse(
                    http_code=401,
                    status_code=401,
                    message=MessageConsts.UNAUTHORIZED,
                    errors="Invalid session",
                )

            session = sessions[0]
            if session[Sessions.userId.name] != user_id:
                raise BaseExceptionResponse(
                    http_code=401,
                    status_code=401,
                    message=MessageConsts.UNAUTHORIZED,
                    errors="Session does not match user",
                )

            # Return the decoded payload as a RefreshPayload object
            return RefreshPayload(
                sessionId=session_id, userId=user_id, role=role, iat=iat, exp=exp
            )

        except jwt.ExpiredSignatureError:
            # Token has expired
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Token has expired",
            )
        except jwt.InvalidTokenError:
            # Token is invalid (e.g., tampered or malformed)
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.UNAUTHORIZED,
                errors="Invalid token",
            )

================
File: backend/modules/auth/types/__init__.py
================
from .auth import JwtPayload, RefreshPayload

================
File: backend/modules/auth/types/auth.py
================
from typing import Dict, Optional
from pydantic import BaseModel

class JwtPayload(BaseModel):
    sessionId: str
    userId: int
    role: str
    iat: Optional[int] = None
    exp: Optional[int] = None

class RefreshPayload(JwtPayload):
    signature: str

================
File: backend/modules/base_daily.py
================
import datetime
from abc import abstractmethod
from typing import Dict
import time
import numpy as np
import pandas as pd
from dateutil.relativedelta import relativedelta

from backend.common.consts import SQLServerConsts
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import ProcessTracking
from backend.modules.portfolio.repositories import ProcessTrackingRepo
from backend.modules.base.query_builder import TextSQL


class BaseDailyService:
    repo: BaseRepo
    process_tracking_repo = ProcessTrackingRepo

    @classmethod
    async def update_newest_data_all_daily(cls) -> bool:
        with cls.repo.session_scope() as session:
            conditions = {
                ProcessTracking.schemaName.name: cls.repo.query_builder.schema,
                ProcessTracking.tableName.name: cls.repo.query_builder.table,
                ProcessTracking.keyName.name: "lastTradingDay",
            }
            tracking_records = await cls.process_tracking_repo.get_by_condition(conditions=conditions)
            if len(tracking_records) == 0:
                tracking_records = await cls.process_tracking_repo.insert_many(records=[conditions], returning=True)
                session.commit()
            if len(tracking_records) > 1:
                raise Exception(f"duplicate tracking records {[t[ProcessTracking.id.name] for t in tracking_records]}")
            tracking_record = tracking_records[0]
            key_val = tracking_record[ProcessTracking.keyValue.name]
            if key_val is not None:
                last_trading_day = datetime.datetime.strptime(key_val, SQLServerConsts.TRADING_DAY_FORMAT)
                continue_trading_day = last_trading_day - datetime.timedelta(days=1)  # ly li li 1 ngy
            else:
                continue_trading_day = SQLServerConsts.START_TRADING_DAY
            await cls.update_newest_data_from_date(from_date=continue_trading_day, process_tracking=tracking_record)
            time.sleep(3)
        return True


    @classmethod
    async def update_newest_data_from_date(cls, from_date: datetime.datetime, process_tracking: Dict):
        from_date_quarter_back_str = (from_date - relativedelta(months=3)).strftime(SQLServerConsts.DATE_FORMAT)
        from_date_str = from_date.strftime(SQLServerConsts.DATE_FORMAT)
        data = await cls.update_data(from_date=from_date_quarter_back_str)
        data = data[data['date'] >= from_date_str].reset_index(drop=True)
        last_key_value = data['date'].iloc[-1]

        with cls.repo.session_scope() as session:
            temp_table = f"#{cls.repo.query_builder.table}"
            await cls.repo.upsert(
                temp_table=temp_table,
                records=data,
                identity_columns=["date", "symbol"],
                text_clauses={"__updatedAt__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)},
            )
            await cls.process_tracking_repo.update(
                record={
                    ProcessTracking.id.name: process_tracking[ProcessTracking.id.name],
                    ProcessTracking.keyValue.name: last_key_value,
                },
                identity_columns=[ProcessTracking.id.name],
                returning=False,
                text_clauses={"updatedAt": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)},
            )
            session.commit()
        return True


    @classmethod
    @abstractmethod
    def update_data(cls, from_date: str) -> pd.DataFrame:
        pass

================
File: backend/modules/base_monthly.py
================
import datetime
import pandas as pd
import numpy as np
from abc import abstractmethod
from typing import Dict
import time
from dateutil.relativedelta import relativedelta

from backend.common.consts import SQLServerConsts
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import ProcessTracking
from backend.modules.portfolio.repositories import ProcessTrackingRepo
from backend.modules.base.query_builder import TextSQL

class BaseMonthlyService:
    repo: BaseRepo
    process_tracking_repo = ProcessTrackingRepo

    @classmethod
    async def update_newest_data_all_monthly(cls) -> bool:
        with cls.repo.session_scope() as session:
            conditions = {
                ProcessTracking.schemaName.name: cls.repo.query_builder.schema,
                ProcessTracking.tableName.name: cls.repo.query_builder.table,
                ProcessTracking.keyName.name: "lastTradingDay",
            }
            tracking_records = await cls.process_tracking_repo.get_by_condition(conditions=conditions)
            if len(tracking_records) == 0:
                tracking_records = await cls.process_tracking_repo.insert_many(records=[conditions], returning=True)
                session.commit()
            if len(tracking_records) > 1:
                raise Exception(f"duplicate tracking records {[t[ProcessTracking.id.name] for t in tracking_records]}")
            tracking_record = tracking_records[0]
            key_val = tracking_record[ProcessTracking.keyValue.name]
            if key_val is not None:
                continue_trading_day = key_val
            else:
                continue_trading_day = SQLServerConsts.START_TRADING_MONTH
            await cls.update_newest_data_from_month_year(from_month_year=continue_trading_day, process_tracking=tracking_record)
            time.sleep(3)
        return True

    @classmethod
    @abstractmethod
    async def update_newest_data_from_month_year(cls, from_month_year: str, process_tracking: Dict):
        from_date = datetime.datetime.strptime(from_month_year, '%Y-%m')
        from_date_ = (from_date - relativedelta(months=6)).strftime(SQLServerConsts.DATE_FORMAT)
        from_date_str = from_date.strftime(SQLServerConsts.DATE_FORMAT)
        data = await cls.update_data(from_date=from_date_)
        data = data[data['date'] >= from_date_str].reset_index(drop=True)
        last_date = data['date'].iloc[-1]
        last_key_value = last_date[:7]
        data = data.drop(columns=['date'], errors='ignore')

        with cls.repo.session_scope() as session:
            temp_table = f"#{cls.repo.query_builder.table}"
            await cls.repo.upsert(
                temp_table=temp_table,
                records=data,
                identity_columns=["symbol", "year", "month"],
                text_clauses={"__updatedAt__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)},
            )
            await cls.process_tracking_repo.update(
                record={
                    ProcessTracking.id.name: process_tracking[ProcessTracking.id.name],
                    ProcessTracking.keyValue.name: last_key_value,
                },
                identity_columns=[ProcessTracking.id.name],
                returning=False,
                text_clauses={"updatedAt": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)},
            )
            session.commit()

    @classmethod
    @abstractmethod
    async def update_data(cls, from_date) -> pd.DataFrame:
        pass

================
File: backend/modules/base_trading_client.py
================
import asyncio
import aiohttp
import json
from typing import Optional, Dict, Any, Union

from enum import Enum
from contextlib import asynccontextmanager

from backend.common.consts import MessageConsts
from backend.common.configs.dnse import TradingAPIConfig
from backend.common.responses.exceptions.base_exceptions import BaseExceptionResponse
from backend.utils.logger import LOGGER


class OrderSide(Enum):
    BUY = "BUY"
    SELL = "SELL"


class OrderType(Enum):
    MARKET = "MARKET"
    LIMIT = "LIMIT"


class BaseTradingClient:
    def __init__(
        self,
        config: Optional[TradingAPIConfig] = None,
        jwt_token: Optional[str] = None,
        trading_token: Optional[str] = None,
    ):
        self.jwt_token = jwt_token
        self.trading_token = trading_token
        self.config = config or TradingAPIConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(self.config.concurrent_limit)

    @asynccontextmanager
    async def get_session(self):
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout)
        try:
            yield self.session
        finally:
            pass

    async def cleanup(self):
        if self.session and not self.session.closed:
            await self.session.close()

    def get_headers(
        self, include_jwt_token: bool = True, include_trading_token: bool = False
    ) -> Dict[str, str]:
        if not include_jwt_token:
            return {}
        headers = {"Authorization": f"Bearer {self.jwt_token}"}
        if include_trading_token:
            headers["Trading-Token"] = self.trading_token
        return headers

    async def make_request(
        self,
        method: str,
        url: str,
        params: Optional[Dict] = None,
        json_data: Optional[Dict] = None,
        include_jwt_token: bool = True,
        include_trading_token: bool = False,
        retries: Optional[int] = None,
        expect_json: bool = True,
    ) -> Union[Dict[str, Any], str]:
        if retries is None:
            retries = self.config.max_retries

        headers = self.get_headers(
            include_jwt_token=include_jwt_token,
            include_trading_token=include_trading_token,
        )

        async with self.semaphore:
            for attempt in range(retries + 1):
                try:
                    async with self.get_session() as session:
                        async with session.request(
                            method=method,
                            url=url,
                            headers=headers,
                            params=params,
                            json=json_data,
                        ) as response:

                            if response.status == 200:
                                if expect_json:
                                    response_data = await response.json()
                                    return response_data
                                else:
                                    response_text = await response.text()
                                    return response_text
                            else:
                                # Read response data for error cases
                                try:
                                    if expect_json:
                                        error_response_data = await response.json()
                                    else:
                                        error_response_data = await response.text()
                                except:
                                    error_response_data = f"Could not parse response body (status: {response.status})"

                                error_msg = (
                                    f"API request failed with status {response.status}"
                                )
                                if attempt == retries:
                                    raise BaseExceptionResponse(
                                        http_code=response.status,
                                        status_code=MessageConsts.TRADING_API_ERROR,
                                        message=error_msg,
                                        errors=error_response_data,
                                    )
                                LOGGER.warning(
                                    f"{error_msg}. Attempt {attempt + 1}/{retries + 1}. Retrying..."
                                )

                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    error_msg = f"Network error: {str(e)}"
                    if attempt == retries:
                        raise BaseExceptionResponse(
                            http_code=500,
                            status_code=MessageConsts.TRADING_API_ERROR,
                            message=error_msg,
                            errors=str(e),
                        )

                    LOGGER.warning(
                        f"{error_msg}. Attempt {attempt + 1}/{retries + 1}. Retrying..."
                    )

                if attempt < retries:
                    await asyncio.sleep(
                        self.config.retry_delay * (2**attempt)
                    )  # Exponential backoff

        raise BaseExceptionResponse(
            http_code=500,
            status_code=MessageConsts.TRADING_API_ERROR,
            message="Max retries exceeded",
            errors="Failed to complete the request after multiple attempts",
        )

    def log_success(self, operation: str, data: Dict[str, Any]):
        LOGGER.info(f"{operation} successfully!")
        # LOGGER.info(json.dumps(data, indent=4, ensure_ascii=False))

    def log_error(self, operation: str, error: Union[BaseExceptionResponse, Exception]):
        LOGGER.error(f"{operation} failed: {str(error)}")

================
File: backend/modules/base/dto.py
================
from typing import Dict, TypeVar, Generic

from pydantic import BaseModel, ValidationError, Extra


T = TypeVar("T")


class BaseDTO(BaseModel, Generic[T]):
    class Config:
        populate_by_name = True
        from_attributes = False
        arbitrary_types_allowed = True
        extra = Extra.ignore

    def __init__(self, **data):
        try:
            for attr in self.__class__.__fields__:
                value = data.get(attr, None)
                if value is None:
                    default_factory = self.__class__.__fields__[attr].default_factory
                    if default_factory is not None:
                        data[attr] = default_factory()
                    default = self.__class__.__fields__[attr].default
                    if default is not None:
                        data[attr] = default
            # print("new DTO worked", data)
            super().__init__(**data)
        except ValidationError as exception:
            raise exception
        except Exception as exception:
            raise exception

================
File: backend/modules/base/entities.py
================
from sqlalchemy.orm import declarative_base

Base = declarative_base()

================
File: backend/modules/base/query_builder.py
================
from typing import Union, List, Dict, Optional, Tuple

import numpy as np
import pandas as pd

from backend.modules.base.entities import Base


class TextSQL:
    def __init__(self, text: str):
        self.text = text


class QueryProduct:
    def __init__(self, sql, params, columns):
        self.sql: str = sql
        self.params: Union[Tuple, List] = params
        self.columns: List[str] = columns


class BaseQueryBuilder:
    def __init__(self, entity: Base):
        self.entity = entity
        self.schema = self.entity.__table__.schema
        self.table = self.entity.__tablename__
        self.full_table_name = f"[{self.schema}].[{self.table}]"

    @classmethod
    def generate_values(cls, records: Union[List[Dict], pd.DataFrame], text_clauses: Optional[Dict[str, TextSQL]]):
        if len(records) == 0:
            return None
        data = pd.DataFrame(records, dtype=np.dtype("O")).replace({np.nan: None})
        row = data.shape[0]
        col = data.shape[1]
        params = data.values.flatten().tolist()
        columns = list(data.columns)
        sql_value = ["?"] * col
        if text_clauses is not None:
            text_columns = list(text_clauses.keys())
            text_values = [text_clauses[i].text for i in text_columns]
            columns += text_columns
            sql_value += text_values
        sql_value = ", ".join(sql_value)
        sql_value = "(%s)" % sql_value
        sql_values = ", ".join([sql_value] * row)
        sql_values = "VALUES %s" % sql_values
        return QueryProduct(sql=sql_values, params=params, columns=columns)

    def insert_many(self, records: List[Dict], returning, text_clauses: Dict[str, TextSQL] = None):
        query_values = self.generate_values(records=records, text_clauses=text_clauses)
        sql_columns = ", ".join(f"[{col}]" for col in query_values.columns)
        sql_output = "OUTPUT Inserted.*" if returning else ""
        sql = """
            INSERT INTO %s (%s)
            %s
            %s
        """ % (self.full_table_name, sql_columns, sql_output, query_values.sql,)
        return QueryProduct(sql=sql, params=query_values.params, columns=query_values.columns)

    @classmethod
    def where(cls, conditions: Dict, alias=None):
        sql = []
        params = []
        if alias is not None and alias != "":
            alias = "%s." % alias
        else:
            alias = ""
        for field in conditions:
            value = conditions[field]
            if value is None:
                sql.append("%s[%s] IS NULL" % (alias, field))
            elif isinstance(value, (list, tuple)):
                in_value_query = ", ".join(["?" for _ in value])
                sql.append("%s[%s] in (%s)" % (alias, field, in_value_query))
                params += value
            elif isinstance(value, TextSQL):
                sql.append("%s[%s] = %s" % (alias, field, value.text))
            else:
                sql.append("%s[%s] = ?" % (alias, field))
                params.append(value)
        if len(sql) == 0:
            return QueryProduct(sql="", params=[], columns=None)
        sql = " AND ".join(sql)
        return QueryProduct(sql=sql, params=params, columns=None)

================
File: backend/modules/base/repositories.py
================
from typing import List, TypeVar, Generic, Dict, Union, Callable

import numpy as np
import pandas as pd
from sqlalchemy.orm import Session

from backend.modules.base.entities import Base
from backend.modules.base.query_builder import BaseQueryBuilder, TextSQL

T = TypeVar("T", bound=Base)


class BaseRepo(Generic[T]):
    entity: T
    query_builder: BaseQueryBuilder
    session_scope: Callable[..., Session]

    @classmethod
    async def data_frame_factory(cls, cur) -> pd.DataFrame:
        if cur.description is None:
            return pd.DataFrame()
        columns = [column[0] for column in cur.description]
        results = [list(row) for row in cur.fetchall()]
        return pd.DataFrame(results, columns=columns, dtype=np.dtype("O"))

    @classmethod
    async def row_factory(cls, cur) -> List[Dict]:
        if cur.description is None:
            return []
        columns = [column[0] for column in cur.description]
        results = []
        for row in cur.fetchall():
            results.append(dict(zip(columns, row)))
        return results

    @classmethod
    async def insert_many(cls, records: List[Dict], returning):
        with cls.session_scope() as session:
            insert_query = cls.query_builder.insert_many(records=records, returning=returning)
            cur = session.connection().exec_driver_sql(insert_query.sql, tuple(insert_query.params)).cursor
            if returning:
                return await cls.row_factory(cur=cur)
            else:
                return None

    @classmethod
    async def insert(cls, record: Dict, returning) -> Dict:
        results = await cls.insert_many(records=[record], returning=returning)
        if returning:
            return results[0]
        else:
            return None

    @classmethod
    async def update_many(
        cls, records: List[Dict], identity_columns: List[str], returning, text_clauses: Dict[str, TextSQL] = None
    ):
        if len(identity_columns) == 0:
            raise Exception("missing require identity columns")
        with cls.session_scope() as session:
            query_values = cls.query_builder.generate_values(records=records, text_clauses=text_clauses)
            update_columns = query_values.columns.copy()
            for col in identity_columns:
                update_columns.remove(col)
            sql_set_columns = ", ".join([f"t.[{col}] = s.[{col}]" for col in update_columns])
            sql_select_columns = ", ".join(f"[{col}]" for col in query_values.columns)
            sql_conditions = " AND ".join([f"t.[{col}] = s.[{col}]" for col in identity_columns])
            sql_returning = "OUTPUT INSERTED.*" if returning else ""
            sql = f"""
                UPDATE t
                SET {sql_set_columns}
                {sql_returning}
                FROM (
                    SELECT *
                    from (
                        {query_values.sql}
                    ) _ ({sql_select_columns})
                ) s
                inner join {cls.query_builder.full_table_name} t on {sql_conditions}
            """
            cur = session.connection().exec_driver_sql(sql, tuple(query_values.params)).cursor
            if returning:
                results = await cls.row_factory(cur=cur)
            else:
                results = None
        return results

    @classmethod
    async def update(
        cls, record: Dict, identity_columns: List[str], returning, text_clauses: Dict[str, TextSQL] = None
    ) -> T:
        results = await cls.update_many(
            records=[record], identity_columns=identity_columns, returning=returning, text_clauses=text_clauses
        )
        if returning:
            return results[0]
        else:
            return None

    @classmethod
    async def get_all(cls) -> List[Dict]:
        with cls.session_scope() as session:
            sql = (
                """
                SELECT *
                FROM %s
            """
                % cls.query_builder.full_table_name
            )
            cur = session.connection().exec_driver_sql(sql).cursor
            results = await cls.row_factory(cur=cur)
        return results

    @classmethod
    async def insert_into_temp(
        cls, records: Union[List[Dict], pd.DataFrame], temp_table: str, text_clauses: Dict[str, TextSQL] = None
    ):
        if len(records) == 0:
            return None
        records = pd.DataFrame(records, dtype=np.dtype("O")).replace({np.nan: None})
        chunk_size = 1
        query_values = cls.query_builder.generate_values(records=records.iloc[:chunk_size], text_clauses=text_clauses)
        sql_columns = ", ".join(f"[{col}]" for col in query_values.columns)
        params = records.values.tolist()
        with cls.session_scope() as session:
            session.connection().exec_driver_sql(
                f"""
                    IF OBJECT_ID('tempdb..{temp_table}') IS NULL
                    BEGIN
                        declare @temp {cls.entity.__sqlServerType__}
                        select *
                        into {temp_table}
                        from @temp
                    END
                """
            )
            sql = f"""
                INSERT INTO {temp_table} ({sql_columns})
                {query_values.sql}
            """
            for i in range(0, len(params), 10000):
                debug_tuple = tuple(params[i : i + 10000])
                session.connection().exec_driver_sql(sql, tuple(params[i : i + 10000]))
        return query_values

    @classmethod
    async def upsert_from_source_table(
        cls, source_table, identity_columns: List[str], upsert_columns: List[str], is_update=True, is_insert=True
    ):
        sql_join_conditions = " AND ".join([f"t.[{col}] = s.[{col}]" for col in identity_columns])
        # sql update
        sql_set_columns = ", ".join([f"t.[{col}] = s.[{col}]" for col in upsert_columns if col not in identity_columns])
        sql_update = f"""
            UPDATE t
            SET {sql_set_columns}
            FROM {cls.query_builder.full_table_name} t
            JOIN {source_table} s ON {sql_join_conditions}
        """
        # sql insert
        sql_select_columns = ", ".join(f"s.[{col}]" for col in upsert_columns)
        sql_insert_columns = ", ".join(f"[{col}]" for col in upsert_columns)
        sql_insert_conditions = " or ".join([f"t.[{col}] is null" for col in identity_columns])
        sql_insert_conditions = (
            f"""
            LEFT JOIN {cls.query_builder.full_table_name} t ON {sql_join_conditions}
            WHERE {sql_insert_conditions}
        """
            if identity_columns
            else ""
        )
        sql_insert = f"""
            INSERT INTO {cls.query_builder.full_table_name} ({sql_insert_columns})
            SELECT {sql_select_columns}
            FROM {source_table} s
            {sql_insert_conditions}
        """
        list_sql = [sql_update] if is_update and len(identity_columns) != 0 else []
        if is_insert:
            list_sql += [sql_insert]
        list_sql = (
            [
                "SET NOCOUNT ON",
                # f"SET IDENTITY_INSERT  {cls.query_builder.full_table_name} ON"
            ]
            + list_sql
            + [
                # f"SET IDENTITY_INSERT  {cls.query_builder.full_table_name} OFF"
            ]
        )
        sql = ";\n".join(list_sql)
        with cls.session_scope() as session:
            session.connection().exec_driver_sql(sql)
        return

    @classmethod
    async def get_by_id(cls, _id: int):
        return await cls.get_by_condition({cls.entity.id.name: _id})

    @classmethod
    async def get_by_condition(cls, conditions: Dict):
        condition_query = cls.query_builder.where(conditions)
        sql = """
            SELECT *
            FROM
            %s
            WHERE %s
        """ % (
            cls.query_builder.full_table_name,
            condition_query.sql,
        )
        with cls.session_scope() as session:
            cur = session.connection().exec_driver_sql(sql, tuple(condition_query.params)).cursor
            records = await cls.row_factory(cur=cur)
            return records

    @classmethod
    async def insert_on_conflict_do_nothing(
        cls,
        temp_table: str,
        records: Union[List[Dict], pd.DataFrame],
        identity_columns: List[str],
        text_clauses: Dict[str, TextSQL] = None,
    ):
        with cls.session_scope() as session:
            # temp_table = f"#{cls.query_builder.table}"
            query = await cls.insert_into_temp(records=records, temp_table=temp_table, text_clauses=text_clauses)
            await cls.upsert_from_source_table(
                source_table=temp_table,
                upsert_columns=query.columns,
                identity_columns=identity_columns,
                is_update=False,
            )
            session.connection().exec_driver_sql(f"DELETE FROM {temp_table}")
        return

    @classmethod
    async def upsert(
        cls,
        temp_table: str,
        records: Union[List[Dict], pd.DataFrame],
        identity_columns: List[str],
        text_clauses: Dict[str, TextSQL] = None,
    ):
        with cls.session_scope() as session:
            # temp_table = f"#{cls.query_builder.table}"
            query = await cls.insert_into_temp(records=records, temp_table=temp_table, text_clauses=text_clauses)
            await cls.upsert_from_source_table(
                source_table=temp_table, upsert_columns=query.columns, identity_columns=identity_columns
            )
            session.connection().exec_driver_sql(f"DROP TABLE {temp_table}")
        return

    @classmethod
    async def get_order_columns(cls):
        with cls.session_scope() as session:
            sql = """
                SELECT *
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?
                ORDER BY COLUMN_NAME ASC
            """
            cur = session.connection().exec_driver_sql(sql, (cls.query_builder.schema, cls.query_builder.table)).cursor
            order_columns = await cls.row_factory(cur)
            return [col["COLUMN_NAME"] for col in order_columns]


    @classmethod
    async def delete(cls, conditions: Dict):
        condition_query = cls.query_builder.where(conditions)
        sql = """
            DELETE FROM %s
            WHERE %s
        """ % (
            cls.query_builder.full_table_name,
            condition_query.sql,
        )
        with cls.session_scope() as session:
            session.connection().exec_driver_sql(sql, tuple(condition_query.params))

================
File: backend/modules/dnse/entities/__init__.py
================
from .trading_tokens import TradingTokens

================
File: backend/modules/dnse/entities/trading_tokens.py
================
import datetime
from sqlalchemy import Column, Integer, String, Float
from typing import Optional, Dict, Any

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base
from backend.utils.time_utils import TimeUtils


class TradingTokens(Base):
    __tablename__ = "tradingTokens"
    __table_args__ = ({"schema": SQLServerConsts.BROKERS_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.BROKERS_SCHEMA}].[{__tablename__}]"

    id = Column(String, primary_key=True, index=True, nullable=False)
    account = Column(String, nullable=False, index=True)
    jwtToken = Column(String, nullable=False)
    tradingToken = Column(String)
    broker = Column(String, nullable=False)
    
    jwtCreatedAt = Column(String)      # When JWT was created
    tradingCreatedAt = Column(String)  # When trading token was created
    createdAt = Column(String)         # When record was first created
    updatedAt = Column(String)         # When record was last updated

    def is_jwt_valid(self) -> bool:
        try:
            if not self.jwtCreatedAt:
                return False
            
            jwt_expire_at = datetime.datetime.strptime(
                self.jwtCreatedAt, SQLServerConsts.TRADING_TIME_FORMAT
            ) + datetime.timedelta(hours=7)
            
            return TimeUtils.get_current_vn_time() < jwt_expire_at
        except (ValueError, TypeError):
            return False
    
    def is_trading_token_valid(self) -> bool:
        try:
            if not self.tradingCreatedAt or not self.tradingToken:
                return False
            
            trading_expire_at = datetime.datetime.strptime(
                self.tradingCreatedAt, SQLServerConsts.TRADING_TIME_FORMAT
            ) + datetime.timedelta(hours=7)
            
            return TimeUtils.get_current_vn_time() < trading_expire_at
        except (ValueError, TypeError):
            return False

    def is_valid(self) -> bool:
        return self.is_jwt_valid() and self.is_trading_token_valid()
    
    def is_partially_valid(self) -> bool:
        return self.is_jwt_valid()

    def jwt_time_remaining(self) -> Optional[datetime.timedelta]:
        if not self.jwtCreatedAt:
            return None
        try:
            jwt_expire_at = datetime.datetime.strptime(
                self.jwtCreatedAt, SQLServerConsts.TRADING_TIME_FORMAT
            ) + datetime.timedelta(hours=6)
            
            remaining = jwt_expire_at - TimeUtils.get_current_vn_time()
            return remaining if remaining > datetime.timedelta(0) else None
        except (ValueError, TypeError):
            return None

    def trading_time_remaining(self) -> Optional[datetime.timedelta]:
        if not self.tradingCreatedAt:
            return None
        try:
            trading_expire_at = datetime.datetime.strptime(
                self.tradingCreatedAt, SQLServerConsts.TRADING_TIME_FORMAT
            ) + datetime.timedelta(hours=6)
            
            remaining = trading_expire_at - TimeUtils.get_current_vn_time()
            return remaining if remaining > datetime.timedelta(0) else None
        except (ValueError, TypeError):
            return None

    def time_remaining(self) -> Optional[datetime.timedelta]:
        jwt_remaining = self.jwt_time_remaining()
        trading_remaining = self.trading_time_remaining()
        
        if jwt_remaining and trading_remaining:
            return min(jwt_remaining, trading_remaining)
        elif jwt_remaining:
            return jwt_remaining
        elif trading_remaining:
            return trading_remaining
        else:
            return None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "account": self.account,
            "jwtToken": self.jwtToken,
            "tradingToken": self.tradingToken,
            "broker": self.broker,
            "jwtCreatedAt": self.jwtCreatedAt,
            "tradingCreatedAt": self.tradingCreatedAt,
            "createdAt": self.createdAt,
            "updatedAt": self.updatedAt,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TradingTokens":
        return cls(
            id=data.get("id"),
            account=data.get("account"),
            jwtToken=data.get("jwtToken"),
            tradingToken=data.get("tradingToken"),
            broker=data.get("broker"),
            jwtCreatedAt=data.get("jwtCreatedAt"),
            tradingCreatedAt=data.get("tradingCreatedAt"),
            createdAt=data.get("createdAt"),
            updatedAt=data.get("updatedAt"),
        )
    


    def __repr__(self) -> str:
        return f"<TradingTokens(id={self.id}, account='{self.account}', broker='{self.broker}', valid={self.is_valid()})>"

================
File: backend/modules/dnse/repositories/__init__.py
================
from .trading_tokens import TradingTokensRepo

================
File: backend/modules/dnse/repositories/trading_tokens.py
================
from typing import Dict

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.dnse.entities import TradingTokens


class TradingTokensRepo(BaseRepo[Dict]):
    entity = TradingTokens
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

================
File: backend/modules/dnse/storage/__init__.py
================
from .redis import RedisTokenStorage, REDIS_AVAILABLE
from .sql_server import SQLServerTokenStorage

================
File: backend/modules/dnse/storage/base.py
================
from abc import ABC, abstractmethod
from typing import Optional

from backend.modules.dnse.entities import TradingTokens



class BaseTokenStorage(ABC):
    @abstractmethod
    async def save_token(self, token_data: TradingTokens) -> None:
        pass
    
    @abstractmethod
    async def load_token(self, account: str, broker: str = 'DNSE') -> Optional[TradingTokens]:
        pass
    
    @abstractmethod
    async def delete_token(self, account: str, broker: str = 'DNSE') -> None:
        pass
    
    @abstractmethod
    async def cleanup(self) -> None:
        pass

================
File: backend/modules/dnse/storage/redis.py
================
from typing import Optional, Dict
from uuid import uuid4

from backend.common.consts import SQLServerConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.dnse.entities import TradingTokens
from backend.modules.dnse.storage.base import BaseTokenStorage
from backend.redis.client import REDIS_CLIENT
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils


LOGGER_PREFIX = "[Redis]"


try:
    conn = REDIS_CLIENT.get_conn()
    REDIS_AVAILABLE = True
except Exception as e:
    LOGGER.error(f"Failed to connect to Redis: {e}")
    LOGGER.info("Redis is not available, falling back to SQL Server")
    REDIS_AVAILABLE = False


class RedisTokenStorage(BaseTokenStorage):
    def _get_token_key(self, account: str, broker: str) -> str:
        """Generate Redis key for token storage"""
        return f"token:{account}:{broker}"

    def _get_pattern_key(self, account: str) -> str:
        """Generate Redis pattern key to find all tokens for an account"""
        return f"token:{account}:*"

    async def save_token(self, token_data: TradingTokens) -> None:
        if not REDIS_AVAILABLE:
            LOGGER.warning(f"{LOGGER_PREFIX} Redis is not available, cannot save token")
            return

        try:
            token_key = self._get_token_key(token_data.account, token_data.broker)

            existing_data = conn.hgetall(token_key)
            token_data_dict = token_data.to_dict()
            current_time = TimeUtils.get_current_vn_time().strftime(
                SQLServerConsts.TRADING_TIME_FORMAT
            )

            if existing_data:
                token_data_dict["updatedAt"] = current_time
                if existing_data.get("createdAt"):
                    token_data_dict["createdAt"] = existing_data["createdAt"]
                else:
                    token_data_dict["createdAt"] = current_time

            else:
                token_data_dict["id"] = str(uuid4())
                token_data_dict["createdAt"] = current_time
                token_data_dict["updatedAt"] = current_time

            # Filter out None values for Redis storage
            redis_data = {k: v for k, v in token_data_dict.items() if v is not None}
            
            conn.hset(token_key, mapping=redis_data)
            LOGGER.info(
                f"{LOGGER_PREFIX} Token for account {token_data.account} saved successfully."
            )

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Error saving token to Redis: {e}")
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message="Failed to save token to Redis",
                errors=str(e),
            )

    async def load_token(
        self, account: str, broker: str = "DNSE"
    ) -> Optional[TradingTokens]:
        if not REDIS_AVAILABLE:
            LOGGER.warning("Redis is not available, cannot load token")
            return None

        try:
            token_key = self._get_token_key(account, broker)
            data = conn.hgetall(token_key)

            if not data:
                LOGGER.info(f"{LOGGER_PREFIX} No token found for account {account} in broker {broker}")
                return None

            pattern_key = self._get_pattern_key(account)
            all_keys = conn.keys(pattern_key)
            broker_keys = [key for key in all_keys if key.endswith(f":{broker}")]

            if len(broker_keys) > 1:
                LOGGER.error(
                    f"Multiple tokens found for account {account} in broker {broker}"
                )
                raise BaseExceptionResponse(
                    http_code=400,
                    status_code=400,
                    message="Duplicate trading tokens found",
                    errors=f"Multiple tokens found for account {account} in broker {broker}",
                )

            token = TradingTokens.from_dict(data)
            LOGGER.info(f"{LOGGER_PREFIX} Token loaded for account {account} from Redis successfully")
            return token

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Error loading token from Redis: {e}")
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message="Failed to load token from Redis",
                errors=str(e),
            )

    async def delete_token(self, account: str, broker: str = "DNSE") -> None:
        if not REDIS_AVAILABLE:
            LOGGER.warning(f"{LOGGER_PREFIX} Redis is not available, cannot delete token")
            return

        try:
            token_key = self._get_token_key(account, broker)

            existing_data = conn.hgetall(token_key)
            if not existing_data:
                LOGGER.warning(
                    f"{LOGGER_PREFIX} No token found for account {account} in broker {broker}."
                )
                return

            pattern_key = self._get_pattern_key(account)
            all_keys = conn.keys(pattern_key)
            broker_keys = [key for key in all_keys if key.endswith(f":{broker}")]

            if len(broker_keys) > 1:
                LOGGER.error(
                    f"Multiple tokens found for account {account} in broker {broker}"
                )
                raise BaseExceptionResponse(
                    http_code=400,
                    status_code=400,
                    message="Duplicate trading tokens found",
                    errors=f"Multiple tokens found for account {account} in broker {broker}",
                )

            result = conn.delete(token_key)
            if result > 0:
                LOGGER.info(
                    f"{LOGGER_PREFIX} Token for account {account} deleted successfully."
                )
            else:
                LOGGER.warning(
                    f"{LOGGER_PREFIX} Token for account {account} was not found during deletion."
                )

        except BaseExceptionResponse:
            raise
        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Error deleting token from Redis: {e}")
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message="Failed to delete token from Redis",
                errors=str(e),
            )

    async def cleanup(self) -> None:
        if not REDIS_AVAILABLE:
            LOGGER.warning(f"{LOGGER_PREFIX} Redis is not available, cannot cleanup")
            return
        try:
            conn.close()
            LOGGER.info(f"{LOGGER_PREFIX} Closed connection to Redis.")
        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Error closing connection to Redis: {e}")
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message="Failed to cleanup Redis connection",
                errors=str(e),
            )

================
File: backend/modules/dnse/storage/sql_server.py
================
from typing import Optional, Dict

from backend.common.consts import SQLServerConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.dnse.entities import TradingTokens
from backend.modules.dnse.repositories import TradingTokensRepo
from backend.modules.dnse.storage.base import BaseTokenStorage
from backend.modules.base.query_builder import TextSQL
from backend.utils.logger import LOGGER


class SQLServerTokenStorage(BaseTokenStorage):
    async def save_token(self, token_data: TradingTokens) -> None:
        with TradingTokensRepo.session_scope() as session:
            conditions = {
                TradingTokens.account.name: token_data.account,
                TradingTokens.broker.name: token_data.broker
            }
            existing_tokens = await TradingTokensRepo.get_by_condition(conditions)
            if len(existing_tokens) == 0:
                await TradingTokensRepo.insert(
                    record={
                        TradingTokens.account.name: token_data.account,
                        TradingTokens.jwtToken.name: token_data.jwtToken,
                        TradingTokens.tradingToken.name: token_data.tradingToken,
                        TradingTokens.broker.name: token_data.broker,
                        TradingTokens.jwtCreatedAt.name: token_data.jwtCreatedAt,
                        TradingTokens.tradingCreatedAt.name: token_data.tradingCreatedAt
                    },
                    returning=False
                )
                session.commit()
                return True
            if len(existing_tokens) > 1:
                raise BaseExceptionResponse(
                    http_code=400,
                    status_code=400,
                    message="Duplicate trading tokens found",
                    errors=f"Multiple tokens found: {[t[TradingTokens.id.name] for t in existing_tokens]}"
                )
            existed_token = existing_tokens[0]
            await TradingTokensRepo.update(
                record={
                    TradingTokens.id.name: existed_token[TradingTokens.id.name],
                    TradingTokens.jwtToken.name: token_data.jwtToken,
                    TradingTokens.tradingToken.name: token_data.tradingToken,
                    TradingTokens.broker.name: token_data.broker,
                    TradingTokens.jwtCreatedAt.name: token_data.jwtCreatedAt,
                    TradingTokens.tradingCreatedAt.name: token_data.tradingCreatedAt,
                },
                identity_columns=[TradingTokens.id.name],
                returning=False,
                text_clauses={"updatedAt": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)}
            )
            session.commit()
        LOGGER.info(f"Token for account {token_data.account} saved to SQL Server successfully.")

    async def load_token(self, account: str, broker: str = 'DNSE') -> Optional[TradingTokens]:
        conditions = {
            TradingTokens.account.name: account,
            TradingTokens.broker.name: broker
        }
        existing_tokens = await TradingTokensRepo.get_by_condition(conditions)
        if not existing_tokens:
            return None
        if len(existing_tokens) > 1:
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message="Duplicate trading tokens found",
                errors=f"Multiple tokens found: {[t[TradingTokens.id.name] for t in existing_tokens]}"
            )
        return TradingTokens.from_dict(data=existing_tokens[0])

    async def delete_token(self, account: str, broker: str = 'DNSE') -> None:
        conditions = {
            TradingTokens.account.name: account,
            TradingTokens.broker.name: broker
        }
        existing_tokens = await TradingTokensRepo.get_by_condition(conditions)
        if not existing_tokens:
            LOGGER.warning(f"No token found for account {account} in broker {broker}.")
            return
        if len(existing_tokens) > 1:
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message="Duplicate trading tokens found",
                errors=f"Multiple tokens found: {[t[TradingTokens.id.name] for t in existing_tokens]}"
            )
        await TradingTokensRepo.delete(conditions=conditions)
        LOGGER.info(f"Token for account {account} deleted from SQL Server successfully.")

    async def cleanup(self) -> None:
        pass

================
File: backend/modules/dnse/trading_api/__init__.py
================
from .orders_client import OrdersClient
from .auth_client import AuthClient
from .users_client import UsersClient

================
File: backend/modules/dnse/trading_api/auth_client.py
================
from typing import Optional
from enum import Enum
import os
from typing import Optional, Dict, Union
from enum import Enum


from backend.common.consts import MessageConsts, SQLServerConsts
from backend.common.configs.dnse import TradingAPIConfig
from backend.common.responses.exceptions.base_exceptions import BaseExceptionResponse
from backend.modules.dnse.entities import TradingTokens
from backend.modules.base_trading_client import BaseTradingClient
from backend.modules.dnse.storage import (
    RedisTokenStorage,
    SQLServerTokenStorage,
    REDIS_AVAILABLE,
)
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils


LOGGER_PREFIX = "[DNSE_API]"


class OTPType(Enum):
    SMART = "smart"
    EMAIL = "email"


class AuthClient(BaseTradingClient):
    def __init__(
        self,
        account: Optional[str] = None,
        jwt_token: Optional[str] = None,
        trading_token: Optional[str] = None,
        config: Optional[TradingAPIConfig] = None,
    ):
        super().__init__(config)
        self.account = account
        self.jwt_token = jwt_token
        self.trading_token = trading_token
        self.storage = (
            RedisTokenStorage() if REDIS_AVAILABLE else SQLServerTokenStorage()
        )

    async def login(self, account: str, password: str) -> bool:
        try:
            url = f"{self.config.base_url}/auth-service/login"
            data = {"username": account, "password": password}
            response_data = await self.make_request(
                method="POST", url=url, include_jwt_token=False, json_data=data
            )

            jwt_token = response_data.get("token")
            self.account = account
            self.jwt_token = jwt_token

            current_time = TimeUtils.get_current_vn_time().strftime(
                SQLServerConsts.TRADING_TIME_FORMAT
            )

            existing_token = None
            try:
                existing_token = await self.storage.load_token(
                    account=account, broker="DNSE"
                )
            except:
                pass

            token_entity = TradingTokens(
                account=account,
                jwtToken=jwt_token,
                tradingToken=(
                    existing_token.tradingToken
                    if existing_token and existing_token.is_trading_token_valid()
                    else None
                ),
                broker="DNSE",
                jwtCreatedAt=current_time,
                tradingCreatedAt=(
                    existing_token.tradingCreatedAt
                    if existing_token and existing_token.is_trading_token_valid()
                    else None
                ),
            )

            await self.storage.save_token(token_data=token_entity)

            LOGGER.info(
                f"{LOGGER_PREFIX} Authentication successful for account: {account}"
            )
            return True

        except Exception as e:
            raise BaseExceptionResponse(
                http_code=401,
                status_code=401,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e),
            )

    async def send_email_otp(self) -> bool:
        if not self.jwt_token or not self.account:
            LOGGER.error("No valid token available for OTP request")
            return False
        try:
            url = f"{self.config.base_url}/auth-service/api/email-otp"
            response_text = await self.make_request(
                "GET", url, include_jwt_token=True, expect_json=False
            )

            if isinstance(response_text, str) and "OK" in response_text.upper():
                LOGGER.info(f"{LOGGER_PREFIX} Email OTP sent successfully!")
                return True
            else:
                LOGGER.warning(
                    f"{LOGGER_PREFIX} Unexpected response from OTP endpoint: {response_text}"
                )
                return False

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Failed to send email OTP: {e}")
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e),
            )

    async def get_trading_token(
        self, otp: str, otp_type: OTPType = OTPType.EMAIL.value
    ) -> bool:
        if not self.jwt_token:
            LOGGER.error("No valid token available for trading token request")
            return False

        try:
            url = f"{self.config.base_url}/order-service/trading-token"
            data = {}
            LOGGER.info(f"{LOGGER_PREFIX} OTP Type: {otp_type}")
            if otp_type == OTPType.SMART.value:
                data["smart-otp"] = otp
            elif otp_type == OTPType.EMAIL.value:
                data["otp"] = otp
            LOGGER.info(f"{LOGGER_PREFIX} Data to send: {data}")

            response_data = await self.make_request(
                method="POST", url=url, json_data=data
            )

            trading_token = response_data.get("tradingToken")
            if not trading_token:
                raise BaseExceptionResponse(
                    http_code=401,
                    status_code=401,
                    message=MessageConsts.TRADING_API_ERROR,
                    errors="Authentication failed",
                )

            self.trading_token = trading_token

            current_time = TimeUtils.get_current_vn_time().strftime(
                SQLServerConsts.TRADING_TIME_FORMAT
            )

            existing_token = await self.storage.load_token(
                account=self.account, broker="DNSE"
            )

            token_entity = TradingTokens(
                account=self.account,
                jwtToken=self.jwt_token,
                tradingToken=trading_token,
                broker="DNSE",
                jwtCreatedAt=(
                    existing_token.jwtCreatedAt if existing_token else current_time
                ),
                tradingCreatedAt=current_time,
                createdAt=existing_token.createdAt if existing_token else current_time,
                updatedAt=current_time,
            )

            if self.account:
                await self.storage.save_token(token_data=token_entity)

            LOGGER.info(
                f"{LOGGER_PREFIX} Trading token obtained and saved successfully!"
            )
            return True

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Failed to get trading token: {e}")
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e),
            )

    async def load_token(self, account: str, broker: str = "DNSE") -> bool:
        try:
            token_data = await self.storage.load_token(account=account, broker=broker)

            if not token_data:
                LOGGER.info(
                    f"{LOGGER_PREFIX} No token found for account {account} in broker {broker}"
                )
                return False

            if token_data.jwtToken and token_data.is_jwt_valid():
                self.account = account
                self.jwt_token = token_data.jwtToken

                if token_data.tradingToken and token_data.is_trading_token_valid():
                    self.trading_token = token_data.tradingToken

                    # jwt_remaining = token_data.jwt_time_remaining()
                    # trading_remaining = token_data.trading_time_remaining()

                    LOGGER.info(f"{LOGGER_PREFIX} Token {account} loaded successfully.")
                    # LOGGER.info(f"{LOGGER_PREFIX} JWT remaining: {jwt_remaining}")
                    # LOGGER.info(f"{LOGGER_PREFIX} Trading token remaining: {trading_remaining}")
                else:
                    LOGGER.info(
                        f"{LOGGER_PREFIX} JWT loaded, but trading token is expired or missing"
                    )
                    self.trading_token = None

                return True
            else:
                if token_data.jwtToken and not token_data.is_jwt_valid():
                    LOGGER.warning(
                        f"{LOGGER_PREFIX} JWT token for {account} is expired"
                    )
                else:
                    LOGGER.warning(
                        f"{LOGGER_PREFIX} No valid JWT token for {account} found"
                    )
                return False
        except BaseExceptionResponse:
            raise
        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Failed to load token: {e}")
            return False

================
File: backend/modules/dnse/trading_api/orders_client.py
================
import asyncio
from typing import Optional, Dict, Union
from enum import Enum

from backend.common.consts import MessageConsts, DNSEConsts
from backend.common.responses.exceptions.base_exceptions import BaseExceptionResponse
from backend.modules.base_trading_client import BaseTradingClient


LOGGER_PREFIX = "[DNSE_API]"


class OrderSide(Enum):
    BUY = "NB"
    SELL = "NS"


class OrderType(Enum):
    MARKET = "MP"
    LIMIT = "LO"


class OrdersClient(BaseTradingClient):
    async def place_order(
        self,
        account_no: str,
        side: Union[OrderSide, str],
        order_type: Union[OrderType, str],
        symbol: str,
        price: float,
        quantity: int,
        loan_package_id: str = "1036"
    ) -> Optional[Dict]:
        url = f"{self.config.base_url}/order-service/v2/orders"

        data = {
            "symbol": symbol,
            "side": side,
            "orderType": order_type,
            "price": price,
            "quantity": quantity,
            "loanPackageId": loan_package_id,
            "accountNo": account_no,
        }
        
        try:
            result = await self.make_request("POST", url, json_data=data, include_trading_token=True)
            self.log_success(f"{LOGGER_PREFIX} Place order successfully: {side} {order_type} {quantity} {symbol}", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to place order: {side} {order_type} {quantity} {symbol}", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )
        
    async def get_order_book(self, account_no: str) -> Optional[Dict]:
        url = f"{self.config.base_url}/order-service/v2/orders"
        params = {"accountNo": account_no}
        
        try:
            result = await self.make_request("GET", url, params=params)
            self.log_success(f"{LOGGER_PREFIX} Get order book successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to get order book", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )

    async def get_order_detail(self, order_id: str, account_no: str) -> Optional[Dict]:
        url = f"{self.config.base_url}/order-service/v2/orders/{order_id}"
        params = {"accountNo": account_no}
        
        try:
            result = await self.make_request("GET", url, params=params)
            self.log_success(f"{LOGGER_PREFIX} Get order detail successfully", result)
            return result
        except Exception as e:
            self.log_error("Failed to get order detail", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )

    async def cancel_order(self, order_id: str, account_no: str) -> Optional[Dict]:
        url = f"{self.config.base_url}/order-service/v2/orders/{order_id}"
        params = {"accountNo": account_no}
        
        try:
            result = await self.make_request("DELETE", url, params=params, include_trading_token=True)
            self.log_success(f"{LOGGER_PREFIX} Cancel order successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to cancel order", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )

    async def get_deals(self, account_no: str) -> Optional[Dict]:
        url = f"{self.config.base_url}/deal-service/deals"
        params = {"accountNo": account_no}
        
        try:
            result = await self.make_request("GET", url, params=params)
            self.log_success(f"{LOGGER_PREFIX} Get held deals successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to get held deals", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )

    async def batch_operations(self, operations: list) -> list:
        tasks = []
        for op in operations:
            method_name = op.pop('method')
            method = getattr(self, method_name)
            tasks.append(method(**op))
        
        return await asyncio.gather(*tasks, return_exceptions=True)

================
File: backend/modules/dnse/trading_api/users_client.py
================
from typing import Optional, Dict

from backend.common.consts import MessageConsts
from backend.common.responses.exceptions.base_exceptions import BaseExceptionResponse
from backend.modules.base_trading_client import BaseTradingClient


LOGGER_PREFIX = "[DNSE_API]"


class UsersClient(BaseTradingClient):
    async def get_users_info(self) -> Optional[Dict]:
        url = f"{self.config.base_url}/user-service/api/me"
        
        try:
            result = await self.make_request("GET", url)
            self.log_success(f"{LOGGER_PREFIX} Get user's info successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to get user's info", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )
        
    async def get_user_accounts(self) -> Optional[Dict]:
        url = f"{self.config.base_url}/order-service/accounts"
        
        try:
            result = await self.make_request("GET", url)
            self.log_success(f"{LOGGER_PREFIX} Get user's accounts successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to get user's accounts", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )

    async def get_buying_power(self, account_no: str, symbol: str, price: float, load_package_id: str) -> Optional[Dict]:
        url = f"{self.config.base_url}/order-service/accounts/{account_no}/ppse"
        params = {"symbol": symbol, "price": price, "loadPackageId": load_package_id}
        
        try:
            result = await self.make_request("GET", url, params=params)
            self.log_success(f"{LOGGER_PREFIX} Get buying power successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to get buying power", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )
        
    async def get_account_balance(self, account_no: str) -> Optional[Dict]:
        url = f"{self.config.base_url}/order-service/account-balances/{account_no}"
        
        try:
            result = await self.make_request("GET", url)
            self.log_success(f"{LOGGER_PREFIX} Get account balance successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to get account balance", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )
        
    async def get_account_deals(self, account_no: str) -> Optional[Dict]:
        url = f"{self.config.base_url}/deal-service/deals?accountNo={account_no}"
        
        try:
            result = await self.make_request("GET", url)
            self.log_success(f"{LOGGER_PREFIX} Get account deals successfully", result)
            return result
        except Exception as e:
            self.log_error(f"{LOGGER_PREFIX} Failed to get account deals", e)
            raise BaseExceptionResponse(
                http_code=502,
                status_code=502,
                message=MessageConsts.TRADING_API_ERROR,
                errors=str(e)
            )

================
File: backend/modules/dnse/trading_session.py
================
from contextlib import asynccontextmanager
from typing import Optional, AsyncGenerator
from backend.common.configs.dnse import TradingAPIConfig
from backend.modules.dnse.trading_api import AuthClient, OrdersClient, UsersClient


class TradingSession:
    def __init__(
        self, config: Optional[TradingAPIConfig] = None, account: Optional[str] = None
    ):
        self.config = config or TradingAPIConfig()
        self.auth_client: Optional[DNSE_API] = None
        self._jwt_token: Optional[str] = None
        self._trading_token: Optional[str] = None
        self.account = account

    async def __aenter__(self):
        self.auth_client = AuthClient(config=self.config)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.auth_client:
            await self.auth_client.cleanup()

    @property
    def jwt_token(self) -> Optional[str]:
        return self.auth_client.jwt_token if self.auth_client else self._jwt_token

    @property
    def trading_token(self) -> Optional[str]:
        return (
            self.auth_client.trading_token if self.auth_client else self._trading_token
        )

    async def authenticate(self, password: str) -> bool:
        if await self.auth_client.load_token(account=self.account):
            return True

        if await self.auth_client.login(account=self.account, password=password):
            return True
        return False

    async def send_otp(self) -> bool:
        return await self.auth_client.send_email_otp()

    async def complete_auth(self, otp: str, otp_type: str = "email") -> bool:
        return await self.auth_client.get_trading_token(otp=otp, otp_type=otp_type)

    def is_fully_authenticated(self) -> bool:
        return bool(self.jwt_token and self.trading_token)

    def is_jwt_authenticated(self) -> bool:
        return bool(self.jwt_token)

    def get_auth_status(self) -> dict:
        return {
            "has_jwt_token": bool(self.jwt_token),
            "has_trading_token": bool(self.trading_token),
            "fully_authenticated": self.is_fully_authenticated(),
            "can_trade": self.is_fully_authenticated(),
            "can_read_user_info": self.is_jwt_authenticated(),
        }

    @asynccontextmanager
    async def orders_client(self) -> AsyncGenerator[OrdersClient, None]:
        if not self.jwt_token:
            raise ValueError("JWT token is required for OrdersClient")
        if not self.trading_token:
            raise ValueError("Trading token is required for OrdersClient")

        client = OrdersClient(
            config=self.config,
            jwt_token=self.jwt_token,
            trading_token=self.trading_token,
        )
        try:
            yield client
        finally:
            await client.cleanup()

    @asynccontextmanager
    async def users_client(self) -> AsyncGenerator[UsersClient, None]:
        if not self.jwt_token:
            raise ValueError("JWT token is required for UsersClient")

        client = UsersClient(
            config=self.config,
            jwt_token=self.jwt_token,
            trading_token=self.trading_token,
        )
        try:
            yield client
        finally:
            await client.cleanup()

================
File: backend/modules/notifications/__init__.py
================
# Notifications module for trading bot
from .service import (
    notification_service,
    notification_context,
    notify_trade,
    notify_error,
    notify_success,
    notify_warning,
)
from .telegram import TelegramNotifier, MessageType, create_telegram_notifier

__all__ = [
    "notification_service",
    "notification_context",
    "notify_trade",
    "notify_error",
    "notify_success",
    "notify_warning",
    "TelegramNotifier",
    "MessageType",
    "create_telegram_notifier"
]

================
File: backend/modules/notifications/service.py
================
from typing import Optional, Dict, Any
import asyncio
from contextlib import asynccontextmanager

from backend.modules.notifications.telegram import TelegramNotifier, MessageType
from backend.common.configs.telegram import telegram_config
from backend.utils.logger import LOGGER


class NotificationService:
    def __init__(self):
        self.telegram: Optional[TelegramNotifier] = None
        self._initialized = False

    async def initialize(self) -> bool:
        if not telegram_config.is_enabled():
            LOGGER.warning(
                "Telegram notifications not configured - missing bot token or chat ID"
            )
            return False

        try:
            # Increase timeout for Docker environments
            import os

            docker_timeout = (
                90 if os.getenv("DOCKER_ENV", "false").lower() == "true" else 45
            )

            self.telegram = TelegramNotifier(
                bot_token=telegram_config.bot_token,
                chat_id=telegram_config.chat_id,
                max_retries=telegram_config.max_retries,
                retry_delay=telegram_config.retry_delay,
                timeout=docker_timeout,  # Dynamic timeout based on environment
            )

            if await self.telegram.test_connection():
                self._initialized = True
                LOGGER.info("Notification service initialized successfully")
                return True
            else:
                LOGGER.error("Failed to connect to Telegram")
                return False

        except Exception as e:
            LOGGER.error(f"Failed to initialize notification service: {e}")
            return False

    def is_ready(self) -> bool:
        return self._initialized and self.telegram is not None

    async def notify_daily_portfolio(self, portfolio_data: Dict[str, Any]) -> None:
        """Notify about daily portfolio weights."""
        if not self.is_ready():
            LOGGER.warning("Notification service not initialized")
            return

        try:
            await self.telegram.send_model_portfolio_update(
                portfolio_data=portfolio_data
            )
            LOGGER.info(f"Daily portfolio notification sent successfully")
        except Exception as e:
            LOGGER.error(f"Failed to send daily portfolio notification: {e}")

    # async def notify_trade_execution(
    #     self,
    #     symbol: str,
    #     side: str,
    #     quantity: int,
    #     price: float,
    #     account: str,
    #     order_id: Optional[str] = None,
    #     status: str = "EXECUTED",
    # ):
    #     if not self.is_ready() or not telegram_config.enable_trade_alerts:
    #         return

    #     try:
    #         await self.telegram.send_trade_alert(
    #             symbol=symbol,
    #             side=side,
    #             quantity=quantity,
    #             price=price,
    #             account=account,
    #             order_id=order_id,
    #             status=status,
    #         )
    #     except Exception as e:
    #         LOGGER.error(f"Failed to send trade notification: {e}")

    # async def notify_portfolio_update(
    #     self,
    #     total_value: float,
    #     cash: float,
    #     stocks_value: float,
    #     daily_pnl: float,
    #     daily_pnl_percent: float,
    # ):
    #     """Notify about portfolio updates."""
    #     if not self.is_ready() or not telegram_config.enable_portfolio_updates:
    #         return

    #     try:
    #         await self.telegram.send_portfolio_update(
    #             total_value=total_value,
    #             cash=cash,
    #             stocks_value=stocks_value,
    #             daily_pnl=daily_pnl,
    #             daily_pnl_percent=daily_pnl_percent,
    #         )
    #     except Exception as e:
    #         LOGGER.error(f"Failed to send portfolio notification: {e}")

    async def notify_system_event(
        self, title: str, description: str, alert_type: MessageType = MessageType.INFO
    ):
        """Notify about system events."""
        if not self.is_ready() or not telegram_config.enable_system_alerts:
            return

        try:
            # Use admin chat for critical system alerts if available
            chat_id = (
                telegram_config.admin_chat_id
                if alert_type == MessageType.ERROR
                else None
            )

            if chat_id and chat_id != telegram_config.chat_id:
                # Send to admin chat for critical alerts
                admin_notifier = TelegramNotifier(
                    bot_token=telegram_config.bot_token, chat_id=chat_id
                )
                await admin_notifier.send_system_alert(title, description, alert_type)

            # Also send to main chat
            await self.telegram.send_system_alert(title, description, alert_type)

        except Exception as e:
            LOGGER.error(f"Failed to send system notification: {e}")

    # async def notify_authentication(
    #     self, account: str, event: str, success: bool = True
    # ):
    #     """Notify about authentication events."""
    #     if not self.is_ready() or not telegram_config.enable_system_alerts:
    #         return

    #     try:
    #         await self.telegram.send_auth_alert(account, event, success)
    #     except Exception as e:
    #         LOGGER.error(f"Failed to send auth notification: {e}")

    # async def notify_market_alert(
    #     self,
    #     symbol: str,
    #     current_price: float,
    #     change: float,
    #     change_percent: float,
    #     volume: int,
    # ):
    #     """Notify about market data alerts."""
    #     if not self.is_ready() or not telegram_config.enable_market_alerts:
    #         return

    #     try:
    #         await self.telegram.send_market_data_alert(
    #             symbol=symbol,
    #             current_price=current_price,
    #             change=change,
    #             change_percent=change_percent,
    #             volume=volume,
    #         )
    #     except Exception as e:
    #         LOGGER.error(f"Failed to send market notification: {e}")

    # async def send_custom_message(
    #     self,
    #     message: str,
    #     message_type: MessageType = MessageType.INFO,
    #     disable_notification: bool = None,
    # ):
    #     """Send a custom message."""
    #     if not self.is_ready():
    #         return

    #     try:
    #         # Auto-detect if should be silent based on time
    #         if disable_notification is None:
    #             disable_notification = telegram_config.is_silent_hour()

    #         await self.telegram.send_message(
    #             text=message,
    #             message_type=message_type,
    #             disable_notification=disable_notification,
    #         )
    #     except Exception as e:
    #         LOGGER.error(f"Failed to send custom notification: {e}")


# Global notification service instance
notification_service = NotificationService()


@asynccontextmanager
async def notification_context():
    """
    Context manager for notification service.
    Automatically initializes and cleans up notification service.
    """
    try:
        await notification_service.initialize()
        yield notification_service
    finally:
        # Cleanup if needed
        pass


# Convenience functions for direct use
async def notify_trade(
    symbol: str, side: str, quantity: int, price: float, account: str, **kwargs
):
    """Quick function to notify about trades."""
    await notification_service.notify_trade_execution(
        symbol, side, quantity, price, account, **kwargs
    )


async def notify_error(title: str, description: str):
    """Quick function to notify about errors."""
    await notification_service.notify_system_event(
        title, description, MessageType.ERROR
    )


async def notify_success(title: str, description: str):
    """Quick function to notify about success events."""
    await notification_service.notify_system_event(
        title, description, MessageType.SUCCESS
    )


async def notify_warning(title: str, description: str):
    """Quick function to notify about warnings."""
    await notification_service.notify_system_event(
        title, description, MessageType.WARNING
    )

================
File: backend/modules/notifications/telegram.py
================
import asyncio
import aiohttp
import ssl
import os
from typing import Optional, Dict, Any, List
from enum import Enum
from datetime import datetime

from backend.common.consts import SQLServerConsts
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils


LOGGER_PREFIX = "[Telegram]"


class MessageType(Enum):
    INFO = ""
    SUCCESS = ""
    WARNING = ""
    ERROR = ""
    TRADE_BUY = ""
    TRADE_SELL = ""
    MONEY = ""
    CHART = ""
    ROBOT = ""


class TelegramNotifier:
    def __init__(
        self,
        bot_token: str,
        chat_id: str,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        timeout: int = 60,  # Increase default timeout for Docker
    ):
        self.bot_token = bot_token
        self.chat_id = chat_id
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.timeout = timeout
        self.base_url = f"https://api.telegram.org/bot{bot_token}"

        self._last_send_time = 0
        self._min_interval = 0.05

        # Create SSL context for better connection handling
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = True
        self.ssl_context.verify_mode = ssl.CERT_REQUIRED

        # For Docker environments, allow for more flexible SSL
        import os

        if os.getenv("DOCKER_ENV", "false").lower() == "true":
            LOGGER.info(f"{LOGGER_PREFIX} Running in Docker environment - adjusting SSL settings")
            self.ssl_context.check_hostname = False
            self.ssl_context.verify_mode = ssl.CERT_NONE

    async def _rate_limit(self):
        current_time = asyncio.get_event_loop().time()
        time_since_last = current_time - self._last_send_time

        if time_since_last < self._min_interval:
            await asyncio.sleep(self._min_interval - time_since_last)

        self._last_send_time = asyncio.get_event_loop().time()

    async def send_message(
        self,
        text: str,
        message_type: MessageType = MessageType.INFO,
        parse_mode: str = "HTML",
        disable_notification: bool = False,
    ) -> bool:
        await self._rate_limit()

        timestamp = TimeUtils.get_current_vn_time().strftime("%H:%M:%S")
        formatted_text = f"{message_type.value} <b>[{timestamp}]</b> {text}"

        url = f"{self.base_url}/sendMessage"
        payload = {
            "chat_id": self.chat_id,
            "text": formatted_text,
            "parse_mode": parse_mode,
            "disable_notification": disable_notification,
        }

        for attempt in range(self.max_retries):
            try:
                # Create connector with specific SSL and DNS settings optimized for Docker
                connector = aiohttp.TCPConnector(
                    ssl=self.ssl_context,
                    limit=10,
                    limit_per_host=5,
                    enable_cleanup_closed=True,
                    resolver=aiohttp.resolver.DefaultResolver(),
                    family=0,  # Allow both IPv4 and IPv6
                    local_addr=None,
                    ttl_dns_cache=300,
                    use_dns_cache=True,
                )

                # Increase timeouts for Docker environment
                connect_timeout = (
                    20 if os.getenv("DOCKER_ENV", "false").lower() == "true" else 10
                )
                sock_read_timeout = (
                    20 if os.getenv("DOCKER_ENV", "false").lower() == "true" else 10
                )

                timeout = aiohttp.ClientTimeout(
                    total=self.timeout,
                    connect=connect_timeout,
                    sock_read=sock_read_timeout,
                )

                async with aiohttp.ClientSession(
                    connector=connector, timeout=timeout
                ) as session:
                    async with session.post(url, json=payload) as response:
                        if response.status == 200:
                            LOGGER.info(
                                f"{LOGGER_PREFIX} Telegram message sent successfully (attempt {attempt + 1})"
                            )
                            return True
                        else:
                            error_text = await response.text()
                            LOGGER.warning(
                                f"{LOGGER_PREFIX} Telegram API error: {response.status} - {error_text}"
                            )

            except asyncio.TimeoutError:
                LOGGER.error(
                    f"{LOGGER_PREFIX} Telegram API timeout (attempt {attempt + 1}) - Consider checking network connectivity in Docker"
                )
            except aiohttp.ClientConnectorError as e:
                LOGGER.error(f"{LOGGER_PREFIX} Telegram connection error (attempt {attempt + 1}): {e}")
                # Log additional info for Docker debugging
                if os.getenv("DOCKER_ENV", "false").lower() == "true":
                    LOGGER.error(
                        f"{LOGGER_PREFIX} Docker environment detected. This might be a DNS/network issue. Check:"
                    )
                    LOGGER.error(f"{LOGGER_PREFIX} 1. Container has internet access")
                    LOGGER.error(f"{LOGGER_PREFIX} 2. DNS resolution is working")
                    LOGGER.error(f"{LOGGER_PREFIX} 3. Firewall rules allow outbound HTTPS")
            except ssl.SSLError as e:
                LOGGER.error(f"{LOGGER_PREFIX} Telegram SSL error (attempt {attempt + 1}): {e}")
            except Exception as e:
                LOGGER.error(
                    f"{LOGGER_PREFIX} Failed to send Telegram message (attempt {attempt + 1}): {e}"
                )

            if attempt < self.max_retries - 1:
                await asyncio.sleep(self.retry_delay * (attempt + 1))

        LOGGER.error(
            f"{LOGGER_PREFIX} Failed to send Telegram message after {self.max_retries} attempts"
        )
        return False

    async def send_model_portfolio_update(self, portfolio_data: Dict):
        date = portfolio_data["date"]
        long_only = portfolio_data["LongOnly"]
        market_neutral = portfolio_data["MarketNeutral"]

        date_display = datetime.strptime(date, SQLServerConsts.DATE_FORMAT).strftime(
            "%d/%m/%Y"
        )

        message = f"""
<b> BO CO DANH MC U T NGY GIAO DCH TIP THEO</b>
<b> Ngy giao dch: {date_display}</b>

<b> DANH MC LONG-ONLY ({len(long_only)} m)</b>
"""

        # Add long-only positions
        if long_only:
            total_long_weight = sum(pos["weight"] for pos in long_only)
            for i, pos in enumerate(long_only[:15], 1):  # Top 15 positions
                message += f"{i:2d}. <b>{pos['symbol']}</b>: {pos['weight']:.2f}%\n"

            if len(long_only) > 15:
                remaining = len(long_only) - 15
                message += f"    ... v {remaining} m khc\n"

            message += f"\n <b>Tng t trng:</b> {total_long_weight:.2f}%\n"
        else:
            message += "Khng c v th no\n"

        message += f"\n<b> DANH MC MARKET NEUTRAL ({len(market_neutral)} m)</b>\n"

        # Add market neutral positions
        if market_neutral:
            long_positions = [pos for pos in market_neutral if pos["weight"] > 0]
            short_positions = [pos for pos in market_neutral if pos["weight"] < 0]

            # Long positions
            if long_positions:
                message += f"\n <b>V TH LONG ({len(long_positions)} m):</b>\n"
                for i, pos in enumerate(long_positions[:20], 1):  # Top 10
                    message += f"{i:2d}. <b>{pos['symbol']}</b>: {pos['weight']:.2f}%\n"

            # Short positions
            if short_positions:
                message += f"\n <b>V TH SHORT ({len(short_positions)} m):</b>\n"
                for i, pos in enumerate(short_positions[:20], 1):  # Top 10
                    message += f"{i:2d}. <b>{pos['symbol']}</b>: {pos['weight']:.2f}%\n"

            # Summary
            # total_long = sum(pos["weight"] for pos in long_positions)
            # total_short = sum(pos["weight"] for pos in short_positions)
            # message += f"\n <b>Tng kt:</b> Long {total_long:+.2f}% | Short {total_short:+.2f}% | Net {(total_long+total_short):+.2f}%"
        else:
            message += "Khng c v th no\n"

        message += f"\n\n <i>c to t ng lc {TimeUtils.get_current_vn_time().strftime('%H:%M:%S %d/%m/%Y')}</i>"

        await self.send_message(message, MessageType.CHART, disable_notification=False)

    async def send_system_alert(
        self,
        title: str,
        description: str,
        alert_type: MessageType = MessageType.WARNING,
    ):
        message = f"""
            <b>{alert_type.value} {title}</b>

            {description}

             <i>Thi gian: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}</i>
        """

        await self.send_message(message, alert_type)

    async def test_connection(self) -> bool:
        try:
            url = f"{self.base_url}/getMe"

            # Create connector with Docker-optimized settings
            connector = aiohttp.TCPConnector(
                ssl=self.ssl_context,
                limit=10,
                limit_per_host=5,
                enable_cleanup_closed=True,
                resolver=aiohttp.resolver.DefaultResolver(),
                family=0,  # Allow both IPv4 and IPv6
                local_addr=None,
                ttl_dns_cache=300,
                use_dns_cache=True,
            )

            # Increase timeouts for Docker environment
            connect_timeout = (
                20 if os.getenv("DOCKER_ENV", "false").lower() == "true" else 10
            )
            sock_read_timeout = (
                20 if os.getenv("DOCKER_ENV", "false").lower() == "true" else 10
            )

            timeout = aiohttp.ClientTimeout(
                total=self.timeout, connect=connect_timeout, sock_read=sock_read_timeout
            )

            async with aiohttp.ClientSession(
                connector=connector, timeout=timeout
            ) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        bot_info = data.get("result", {})
                        LOGGER.info(
                            f"{LOGGER_PREFIX} Telegram bot connected successfully: @{bot_info.get('username', 'unknown')}"
                        )
                        # await self.send_message(
                        #     f" Bot kt ni thnh cng!\n"
                        #     f" Tn bot: {bot_info.get('first_name', 'Unknown')}\n"
                        #     f" Username: @{bot_info.get('username', 'unknown')}",
                        #     MessageType.SUCCESS,
                        # )
                        return True
                    else:
                        error_text = await response.text()
                        LOGGER.error(
                            f"{LOGGER_PREFIX} Telegram bot test failed: {response.status} - {error_text}"
                        )
                        return False
        except asyncio.TimeoutError:
            LOGGER.error(f"{LOGGER_PREFIX} Telegram connection test error: Connection timeout")
            return False
        except aiohttp.ClientConnectorError as e:
            LOGGER.error(
                f"{LOGGER_PREFIX} Telegram connection test error: Cannot connect to host api.telegram.org:443 ssl:default [{e}]"
            )
            return False
        except ssl.SSLError as e:
            LOGGER.error(f"{LOGGER_PREFIX} Telegram connection test error: SSL error [{e}]")
            return False
        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Telegram connection test error: {e}")
            return False

    # async def send_trade_alert(
    #     self,
    #     symbol: str,
    #     side: str,
    #     quantity: int,
    #     price: float,
    #     account: str,
    #     order_id: Optional[str] = None,
    #     status: str = "PLACED",
    # ):
    #     side_emoji = (
    #         MessageType.TRADE_BUY
    #         if side.upper() in ["BUY", "NB"]
    #         else MessageType.TRADE_SELL
    #     )
    #     side_text = "MUA" if side.upper() in ["BUY", "NB"] else "BN"

    #     message = f"""
    #         <b> LNH GIAO DCH - {status}</b>

    #          <b>M CK:</b> {symbol}
    #         {side_emoji.value} <b>Loi:</b> {side_text}
    #          <b>S lng:</b> {quantity:,} c phiu
    #          <b>Gi:</b> {price:,} VND
    #          <b>Ti khon:</b> {account}
    #     """

    #     if order_id:
    #         message += f" <b>M lnh:</b> {order_id}"

    #     await self.send_message(message, MessageType.ROBOT)

    # async def send_portfolio_update(
    #     self,
    #     total_value: float,
    #     cash: float,
    #     stocks_value: float,
    #     daily_pnl: float,
    #     daily_pnl_percent: float,
    # ):
    #     pnl_emoji = MessageType.SUCCESS if daily_pnl >= 0 else MessageType.ERROR
    #     pnl_sign = "+" if daily_pnl >= 0 else ""

    #     message = f"""
    #         <b> CP NHT DANH MC U T</b>

    #          <b>Tng ti sn:</b> {total_value:,.0f} VND
    #          <b>Tin mt:</b> {cash:,.0f} VND
    #          <b>Gi tr CP:</b> {stocks_value:,.0f} VND

    #         {pnl_emoji.value} <b>P&L hm nay:</b> {pnl_sign}{daily_pnl:,.0f} VND ({pnl_sign}{daily_pnl_percent:.2f}%)
    #     """

    #     await self.send_message(message, MessageType.CHART)

    # async def send_market_data_alert(
    #     self,
    #     symbol: str,
    #     current_price: float,
    #     change: float,
    #     change_percent: float,
    #     volume: int,
    # ):
    #     change_emoji = MessageType.SUCCESS if change >= 0 else MessageType.ERROR
    #     change_sign = "+" if change >= 0 else ""

    #     message = f"""
    #         <b> D LIU TH TRNG - {symbol}</b>

    #          <b>Gi hin ti:</b> {current_price:,.0f} VND
    #         {change_emoji.value} <b>Thay i:</b> {change_sign}{change:,.0f} VND ({change_sign}{change_percent:.2f}%)
    #          <b>Khi lng:</b> {volume:,} CP
    #     """

    #     await self.send_message(message, MessageType.CHART)

    # async def send_auth_alert(self, account: str, event: str, success: bool = True):
    #     status_emoji = MessageType.SUCCESS if success else MessageType.ERROR
    #     status_text = "THNH CNG" if success else "THT BI"

    #     message = f"""
    #         <b> XANH THC {event.upper()} - {status_text}</b>

    #          <b>Ti khon:</b> {account}
    #          <b>Thi gian:</b> {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
    #     """

    #     await self.send_message(message, status_emoji)


def create_telegram_notifier(
    bot_token: str, chat_id: str, **kwargs
) -> TelegramNotifier:
    """
    Create a TelegramNotifier instance with enhanced connection settings.

    Args:
        bot_token: Telegram bot token
        chat_id: Telegram chat ID
        **kwargs: Additional parameters (timeout, max_retries, retry_delay)
    """
    return TelegramNotifier(bot_token, chat_id, **kwargs)

================
File: backend/modules/portfolio/core/__init__.py
================
from .portfolio_optimizer import PortfolioOptimizer
from .value_objects import (
    Money,
    Weight,
    Position,
    TradeRecommendation
)

================
File: backend/modules/portfolio/core/portfolio_optimizer.py
================
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd
import cvxpy as cp
from scipy.linalg import pinv


class PortfolioOptimizer:
    success_count = 0
    fallback_count = 0

    @classmethod
    def optimize(cls, df_portfolio: pd.DataFrame):
        returns_df = df_portfolio.pct_change(periods=2).fillna(0)
        # expected_returns_df = returns_df.ewm(span=21, adjust=False).mean()

        # def normalize(series):
        #     min_val = np.min(series)
        #     max_val = np.max(series)
        #     norm_series = (series - min_val) / (max_val - min_val)
        #     return norm_series

        # df_portfolio_normalized = df_portfolio.apply(normalize, axis=0)

        Q = returns_df.cov().values
        mu = returns_df.mean().values
        lambda_ = 0.01

        # Q = np.nan_to_num(Q, nan=0.0, posinf=1e6, neginf=-1e6)
        # mu = np.nan_to_num(mu, nan=0.0, posinf=1e6, neginf=-1e6)

        # x_CEMV, cvxpy_success = cls.solve_portfolio_cvxpy(mu, Q, lambda_)

        # if cvxpy_success:
        #     cls.success_count += 1
        # else:
        #     x_CEMV = cls.solve_portfolio_analytical(mu, Q, lambda_)
        #     cls.fallback_count += 1

        x_CEMV = cls.solve_portfolio_analytical(mu, Q, lambda_)

        # x_CEMV = cls.normalize_weights_exact(x_CEMV)
        x_CEMV_neutralized = cls.neutralize_weights_exact(x_CEMV)
        # x_CEMV_limited = cls.normalize_weights_limit(x_CEMV, max_weight=0.1)
        # x_CEMV_neutralized_limit = cls.neutralize_weights_limit(x_CEMV_limited, max_weight=0.1)

        return x_CEMV, x_CEMV_neutralized, 0.0, 0.0

    @classmethod
    def solve_portfolio_cvxpy(cls, mu, Q, lambda_):
        n_assets = len(mu)

        try:
            Q_psd = cls.make_psd(Q)
            x = cp.Variable(n_assets)
            objective = cp.Maximize(mu.T @ x - lambda_ * cp.quad_form(x, Q_psd))

            constraints = [cp.sum(x) == 1, x >= 0]

            prob = cp.Problem(objective, constraints)
            prob.solve()

            weights = x.value
            if weights is None or np.any(np.isnan(weights)):
                weights = np.ones(n_assets) / n_assets  # fallback
                return weights, False
            return weights, True

            # if prob.status == cp.OPTIMAL and x.value is not None:
            #     weights = x.value
            #     weights = np.maximum(weights, 0)
            #     weights = weights / np.sum(weights)
            #     return weights, True
            # else:
            #     return None, False

        except Exception as e:
            return None, False

    @classmethod
    def solve_portfolio_analytical(cls, mu, Q, lambda_):
        n_assets = len(mu)

        try:
            # Q_reg = Q + np.eye(n_assets) * 1e-8
            # A = pinv(Q_reg)

            A = pinv(Q)
            B = np.sum(A)
            C = A @ mu
            C_sum = np.sum(C)

            if abs(B) < 1e-12:
                return np.ones(n_assets) / n_assets

            nu = (2 * lambda_ * (C_sum - 1)) / B
            x = (1 / (2 * lambda_)) * A @ (mu - nu * np.ones(n_assets))


            x = np.clip(x, 0, None)  # Remove negatives
            x_sum = np.sum(x)
            if x_sum > 0:
                return x / x_sum
            else:
                return np.ones(n_assets) / n_assets

            # x = np.maximum(x, 0)
            # x_sum = np.sum(x)

            # if x_sum > 1e-12:
            #     return x / x_sum
            # else:
            #     return np.ones(n_assets) / n_assets

        except Exception as e:
            return np.ones(n_assets) / n_assets

    @staticmethod
    def normalize_weights_exact(weights):
        weights = np.maximum(weights, 0)
        weights_sum = np.sum(weights)

        if weights_sum > 1e-15:
            weights = weights / weights_sum
            residual = 1.0 - np.sum(weights)
            if abs(residual) > 1e-15:
                max_idx = np.argmax(weights)
                weights[max_idx] += residual
        else:
            weights = np.ones(len(weights)) / len(weights)

        return weights

    @staticmethod
    def neutralize_weights_exact(weights):
        n_assets = len(weights)

        # Convert to market neutral: subtract mean to make sum = 0
        mean_weight = np.mean(weights)
        market_neutral_weights = weights - mean_weight

        # Clip to [-1, 1] range
        clipped_weights = np.clip(market_neutral_weights, -1, 1)

        # Ensure exact sum = 0 by adjusting the weight with largest absolute value
        current_sum = np.sum(clipped_weights)
        if abs(current_sum) > 1e-15:
            # Find index with largest absolute weight to adjust
            max_abs_idx = np.argmax(np.abs(clipped_weights))
            clipped_weights[max_abs_idx] -= current_sum

            # If adjustment pushes outside [-1, 1], redistribute the excess
            if clipped_weights[max_abs_idx] > 1:
                excess = clipped_weights[max_abs_idx] - 1
                clipped_weights[max_abs_idx] = 1
                # Distribute excess to other positions
                other_indices = np.arange(n_assets) != max_abs_idx
                clipped_weights[other_indices] -= excess / (n_assets - 1)
            elif clipped_weights[max_abs_idx] < -1:
                excess = clipped_weights[max_abs_idx] + 1
                clipped_weights[max_abs_idx] = -1
                # Distribute excess to other positions
                other_indices = np.arange(n_assets) != max_abs_idx
                clipped_weights[other_indices] -= excess / (n_assets - 1)

        return clipped_weights


    @staticmethod
    def normalize_weights_limit(weights, max_weight=0.15):
        weights = np.maximum(weights, 0)

        # Apply max weight constraint iteratively
        max_iterations = 100
        tolerance = 1e-10

        for iteration in range(max_iterations):
            # Normalize to sum = 1
            weights_sum = np.sum(weights)
            if weights_sum > 1e-15:
                weights = weights / weights_sum
            else:
                weights = np.ones(len(weights)) / len(weights)
                break

            # Check if any weight exceeds max_weight
            over_limit_mask = weights > max_weight

            if not np.any(over_limit_mask):
                break  # All weights are within limit

            # Calculate excess weight to redistribute
            excess_weight = np.sum(weights[over_limit_mask] - max_weight)

            # Cap weights at max_weight
            weights[over_limit_mask] = max_weight

            # Redistribute excess to remaining assets
            remaining_mask = ~over_limit_mask
            n_remaining = np.sum(remaining_mask)

            if n_remaining > 0:
                # Calculate available capacity for remaining assets
                remaining_weights = weights[remaining_mask]
                available_capacity = np.maximum(0, max_weight - remaining_weights)
                total_capacity = np.sum(available_capacity)

                if total_capacity > tolerance:
                    # Redistribute proportionally to available capacity
                    redistribution = excess_weight * (
                        available_capacity / total_capacity
                    )
                    weights[remaining_mask] += redistribution
                else:
                    # If no capacity, distribute equally among all remaining
                    weights[remaining_mask] += excess_weight / n_remaining
            else:
                # All assets are at max weight, can't redistribute
                break

        # Final normalization
        weights_sum = np.sum(weights)
        if weights_sum > 1e-15:
            weights = weights / weights_sum

            # Adjust for exact sum = 1
            residual = 1.0 - np.sum(weights)
            if abs(residual) > 1e-15:
                # Add residual to asset with most room (furthest from max_weight)
                room_available = max_weight - weights
                max_room_idx = np.argmax(room_available)
                weights[max_room_idx] += residual
        else:
            weights = np.ones(len(weights)) / len(weights)

        return weights

    @staticmethod
    def neutralize_weights_limit(weights, max_weight=0.15):
        n_assets = len(weights)
        max_iterations = 100
        tolerance = 1e-10

        # Convert to market neutral: subtract mean to make sum  0
        mean_weight = np.mean(weights)
        market_neutral_weights = weights - mean_weight

        for iteration in range(max_iterations):
            # Clip to [-max_weight, max_weight] range
            clipped_weights = np.clip(market_neutral_weights, -max_weight, max_weight)

            # Check if sum is close enough to zero
            current_sum = np.sum(clipped_weights)
            if abs(current_sum) <= tolerance:
                break

            # Find positions that are not at limits and can absorb the imbalance
            at_upper_limit = np.abs(clipped_weights - max_weight) < tolerance
            at_lower_limit = np.abs(clipped_weights + max_weight) < tolerance
            at_limit = at_upper_limit | at_lower_limit

            adjustable_mask = ~at_limit
            n_adjustable = np.sum(adjustable_mask)

            if n_adjustable == 0:
                # All positions are at limits, can't achieve perfect neutrality
                break

            # Calculate how much each adjustable position can absorb
            if current_sum > 0:  # Need to reduce sum (move towards negative)
                # Calculate available downward capacity
                available_capacity = clipped_weights[adjustable_mask] + max_weight
                available_capacity = np.maximum(0, available_capacity)
            else:  # Need to increase sum (move towards positive)
                # Calculate available upward capacity
                available_capacity = max_weight - clipped_weights[adjustable_mask]
                available_capacity = np.maximum(0, available_capacity)

            total_capacity = np.sum(available_capacity)

            if total_capacity > tolerance:
                # Distribute adjustment proportionally to available capacity
                adjustment_per_unit = min(abs(current_sum) / total_capacity, 1.0)
                if current_sum > 0:
                    adjustments = -available_capacity * adjustment_per_unit
                else:
                    adjustments = available_capacity * adjustment_per_unit

                clipped_weights[adjustable_mask] += adjustments
            else:
                # Distribute equally among adjustable positions
                adjustment_per_position = -current_sum / n_adjustable
                clipped_weights[adjustable_mask] += adjustment_per_position

            market_neutral_weights = clipped_weights

        # Final clipping and verification
        final_weights = np.clip(market_neutral_weights, -max_weight, max_weight)

        # If still not neutral enough, make a final adjustment to the position with most room
        final_sum = np.sum(final_weights)
        if abs(final_sum) > tolerance:
            # Find position with most room to absorb the remaining imbalance
            if final_sum > 0:
                # Need to reduce sum - find position furthest from lower limit
                room_to_decrease = final_weights + max_weight
                max_room_idx = np.argmax(room_to_decrease)
                adjustment = min(final_sum, room_to_decrease[max_room_idx])
                final_weights[max_room_idx] -= adjustment
            else:
                # Need to increase sum - find position furthest from upper limit
                room_to_increase = max_weight - final_weights
                max_room_idx = np.argmax(room_to_increase)
                adjustment = min(-final_sum, room_to_increase[max_room_idx])
                final_weights[max_room_idx] += adjustment

        return final_weights




    @staticmethod
    def make_psd(matrix, min_eigenvalue=1e-8):
        try:
            eigenvals, eigenvecs = np.linalg.eigh(matrix)
            eigenvals = np.maximum(eigenvals, min_eigenvalue)
            return eigenvecs @ np.diag(eigenvals) @ eigenvecs.T
        except:
            return matrix + np.eye(matrix.shape[0]) * min_eigenvalue

================
File: backend/modules/portfolio/core/strategies/__init__.py
================
from .base import BasePortfolioStrategy
from .factory import StrategyFactory
from .long_only import LongOnlyStrategy
from .market_neutral import MarketNeutralStrategy

================
File: backend/modules/portfolio/core/strategies/base.py
================
from abc import ABC, abstractmethod
from typing import Dict, List


class BasePortfolioStrategy(ABC):
    @abstractmethod
    def get_target_weights(self, portfolio_data: Dict) -> List[Dict]:
        pass

================
File: backend/modules/portfolio/core/strategies/factory.py
================
from backend.modules.portfolio.core.strategies.base import BasePortfolioStrategy
from backend.modules.portfolio.core.strategies.long_only import LongOnlyStrategy
from backend.modules.portfolio.core.strategies.market_neutral import MarketNeutralStrategy


class StrategyFactory:
    @staticmethod
    def create_strategy(strategy_type: str) -> BasePortfolioStrategy:
        strategies = {
            "LongOnly": LongOnlyStrategy(),
            "MarketNeutral": MarketNeutralStrategy(),
        }
        if strategy_type not in strategies:
            raise ValueError(f"Unknown strategy type: {strategy_type}")
        return strategies[strategy_type]

================
File: backend/modules/portfolio/core/strategies/long_only.py
================
from typing import Dict, List
from backend.modules.portfolio.core.strategies.base import BasePortfolioStrategy


class LongOnlyStrategy(BasePortfolioStrategy):
    def get_target_weights(self, portfolio_data: Dict) -> List[Dict]:
        return portfolio_data.get("LongOnly", [])

================
File: backend/modules/portfolio/core/strategies/market_neutral.py
================
from typing import Dict, List
from backend.modules.portfolio.core.strategies.base import BasePortfolioStrategy


class MarketNeutralStrategy(BasePortfolioStrategy):
    def get_target_weights(self, portfolio_data: Dict) -> List[Dict]:
        return portfolio_data.get("MarketNeutral", [])

================
File: backend/modules/portfolio/core/value_objects.py
================
from dataclasses import dataclass, asdict
from decimal import Decimal
from typing import Dict, Any


# Value Objects
@dataclass(frozen=True)
class Money:
    amount: Decimal
    currency: str = "VND"

    def __add__(self, other: "Money") -> "Money":
        if self.currency != other.currency:
            raise ValueError("Cannot add different currencies")
        return Money(self.amount + other.amount, self.currency)

    def __sub__(self, other: "Money") -> "Money":
        if self.currency != other.currency:
            raise ValueError("Cannot subtract different currencies")
        return Money(self.amount - other.amount, self.currency)

    def __mul__(self, factor: float) -> "Money":
        return Money(self.amount * Decimal(str(factor)), self.currency)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to JSON-serializable dict"""
        return {"amount": float(self.amount), "currency": self.currency}


@dataclass(frozen=True)
class Weight:
    percentage: Decimal

    def __post_init__(self):
        if not 0 <= self.percentage <= 100:
            raise ValueError("Weight must be between 0 and 100")

    def to_dict(self) -> Dict[str, Any]:
        """Convert to JSON-serializable dict"""
        return {"percentage": float(self.percentage)}


@dataclass(frozen=True)
class Position:
    symbol: str
    quantity: int
    market_price: Money
    cost_price: Money
    break_even_price: Money 
    weight: Weight
    weight_over_sv: Weight = None 
    realized_profit: Money = None
    unrealized_profit: Money = None

    @property
    def market_value(self) -> Money:
        return self.market_price * self.quantity

    def to_dict(self) -> Dict[str, Any]:
        """Convert to JSON-serializable dict"""
        return {
            "symbol": self.symbol,
            "quantity": self.quantity,
            "market_price": self.market_price.to_dict(),
            "cost_price": self.cost_price.to_dict(),
            "break_even_price": self.break_even_price.to_dict(),
            "weight": self.weight.to_dict(),
            "weight_over_sv": self.weight_over_sv.to_dict(),
            "market_value": self.market_value.to_dict(),
            "realized_profit": (
                self.realized_profit.to_dict() if self.realized_profit else None
            ),
            "unrealized_profit": (
                self.unrealized_profit.to_dict() if self.unrealized_profit else None
            ),
        }


@dataclass(frozen=True)
class TradeRecommendation:
    symbol: str
    action: str  # "BUY" or "SELL"
    current_weight: Weight
    target_weight: Weight
    amount: Money
    priority: str  # "HIGH", "MEDIUM", "LOW"
    reason: str
    action_price: Money = None  # Optional, if applicable
    action_quantity: int = None  # Optional, if applicable

    def to_dict(self) -> Dict[str, Any]:
        """Convert to JSON-serializable dict"""
        return {
            "symbol": self.symbol,
            "action": self.action,
            "current_weight": self.current_weight.to_dict(),
            "target_weight": self.target_weight.to_dict(),
            "amount": self.amount.to_dict(),
            "priority": self.priority,
            "action_price": self.action_price.to_dict() if self.action_price else None,
            "action_quantity": self.action_quantity,
            "reason": self.reason,
        }

================
File: backend/modules/portfolio/dtos/__init__.py
================
from .accounts import SetupDNSEAccountDTO, DefaultAccountResponseDTO
from .portfolio import CreateCustomPortfolioDTO, UpdatePortfolioDTO, AnalyzePortfolioDTO

================
File: backend/modules/portfolio/dtos/accounts.py
================
from typing import Optional
from backend.modules.base.dto import BaseDTO


class SetupDNSEAccountDTO(BaseDTO):
    username: str
    password: str


class DefaultAccountResponseDTO(BaseDTO):
    account_id: str
    name: str
    custody_code: str
    broker_account_id: str
    broker_name: str = "DNSE"
    broker_investor_id: str
    is_default: bool = True

================
File: backend/modules/portfolio/dtos/portfolio.py
================
from typing import List
from backend.modules.base.dto import BaseDTO


class CreateCustomPortfolioDTO(BaseDTO):
    portfolio_name: str
    portfolio_desc: str = ""
    symbols: List[str]


class UpdatePortfolioDTO(BaseDTO):
    portfolio_id: str
    symbols: List[str]


class AnalyzePortfolioDTO(BaseDTO):
    portfolio_id: str
    strategy_type: str = "MarketNeutral"

================
File: backend/modules/portfolio/entities/__init__.py
================
from .accounts import Accounts
from .orders import Orders
from .deals import Deals
from .balances import Balances
from .portfolio import Portfolios
from .portfolio_metadata import PortfolioMetadata
from .universe_top_monthly import StocksUniverse
from .process_tracking import ProcessTracking

================
File: backend/modules/portfolio/entities/accounts.py
================
from sqlalchemy import Column, Integer, String, ForeignKey
from sqlalchemy.orm import relationship

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class Accounts(Base):
    __tablename__ = "accounts"
    __table_args__ = ({"schema": SQLServerConsts.PORTFOLIO_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(String(36), primary_key=True, nullable=False, index=True)
    userId = Column(
        Integer, ForeignKey(f"{SQLServerConsts.AUTH_SCHEMA}.users.id"), nullable=False
    )
    name = Column(String(50), nullable=False)
    accountType = Column(String(20), nullable=True)
    identificationCode = Column(String(20), nullable=True)
    custodyCode = Column(String(20), nullable=False)
    password = Column(String(255), nullable=False)  
    brokerName = Column(String(20), nullable=True)
    brokerInvestorId = Column(String(20), nullable=False)
    brokerAccountId = Column(String(20), nullable=False)

    # Relationships
    user = relationship("Users", back_populates="accounts")
    balances = relationship("Balances", back_populates="account", cascade="all, delete-orphan")
    orders = relationship("Orders", back_populates="account", cascade="all, delete-orphan")
    deals = relationship("Deals", back_populates="account", cascade="all, delete-orphan")

================
File: backend/modules/portfolio/entities/balances.py
================
from sqlalchemy import Column, String, Integer, Float, ForeignKey
from sqlalchemy.orm import relationship

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class Balances(Base):
    __tablename__ = "balances"
    __table_args__ = ({"schema": SQLServerConsts.PORTFOLIO_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(String(36), primary_key=True, nullable=False, index=True)
    date = Column(String(10), nullable=False, index=True)  
    brokerAccountId = Column(
        String(20),
        ForeignKey(f"{SQLServerConsts.PORTFOLIO_SCHEMA}.accounts.brokerAccountId"),
        nullable=False,
    )
    totalCash = Column(Integer, nullable=True)
    availableCash = Column(Integer, nullable=True)
    termDeposit = Column(Integer, nullable=True)
    depositInterest = Column(Integer, nullable=True)
    stockValue = Column(Integer, nullable=True)
    marginableAmount = Column(Integer, nullable=True)
    nonMarginableAmount = Column(Integer, nullable=True)
    totalDebt = Column(Integer, nullable=True)
    netAssetValue = Column(Integer, nullable=True)
    receivingAmount = Column(Integer, nullable=True)
    secureAmount = Column(Integer, nullable=True)
    depositFeeAmount = Column(Integer, nullable=True)
    maxLoanLimit = Column(Integer, nullable=True)
    withdrawableCash = Column(Integer, nullable=True)
    collateralValue = Column(Integer, nullable=True)
    orderSecured = Column(Integer, nullable=True)
    purchasingPower = Column(Integer, nullable=True)
    cashDividendReceiving = Column(Integer, nullable=True)
    marginDebt = Column(Float, nullable=True)
    marginRate = Column(Float, nullable=True)
    ppWithdraw = Column(Integer, nullable=True)
    blockMoney = Column(Integer, nullable=True)
    totalRemainDebt = Column(Float, nullable=True)
    totalUnrealizedDebt = Column(Float, nullable=True)
    blockedAmount = Column(Float, nullable=True)
    advancedAmount = Column(Integer, nullable=True)
    advanceWithdrawnAmount = Column(Float, nullable=True)

    # Relationships
    account = relationship("Accounts", back_populates="balances")

================
File: backend/modules/portfolio/entities/deals.py
================
from sqlalchemy import Column, Integer, String, ForeignKey, Float
from sqlalchemy.orm import relationship

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class Deals(Base):
    __tablename__ = "deals"
    __table_args__ = ({"schema": SQLServerConsts.PORTFOLIO_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(String(36), primary_key=True, nullable=False, index=True)
    date = Column(String(10), nullable=False, index=True)
    brokerAccountId = Column(
        String(20),
        ForeignKey(f"{SQLServerConsts.PORTFOLIO_SCHEMA}.accounts.brokerAccountId"),
        nullable=False,
    )
    dealId = Column(String(10), nullable=False, index=True)
    symbol = Column(String(10), nullable=False, index=True)
    status = Column(String(10), nullable=True)
    side = Column(String(10), nullable=True)
    secure = Column(Float, nullable=True)
    accumulateQuantity = Column(Integer, nullable=False)
    tradeQuantity = Column(Integer, nullable=False)
    closedQuantity = Column(Integer, nullable=False)
    t0ReceivingQuantity = Column(Integer, nullable=True)
    t1ReceivingQuantity = Column(Integer, nullable=True)
    t2ReceivingQuantity = Column(Integer, nullable=True)
    costPrice = Column(Float, nullable=False)
    averageCostPrice = Column(Float, nullable=False)
    marketPrice = Column(Float, nullable=False)
    realizedProfit = Column(Float, nullable=False)
    unrealizedProfit = Column(Float, nullable=True)
    breakEvenPrice = Column(Float, nullable=True)
    dividendReceivingQuantity = Column(Integer, nullable=True)
    dividendQuantity = Column(Integer, nullable=True)
    cashReceiving = Column(Float, nullable=True)
    rightReceivingCash = Column(Integer, nullable=True)
    t0ReceivingCash = Column(Float, nullable=True)
    t1ReceivingCash = Column(Float, nullable=True)
    t2ReceivingCash = Column(Float, nullable=True)
    createdDate = Column(String(30), nullable=True)
    modifiedDate = Column(String(30), nullable=True)
    
    # Relationships
    account = relationship("Accounts", back_populates="deals")

================
File: backend/modules/portfolio/entities/orders.py
================
from sqlalchemy import Column, Integer, String, ForeignKey
from sqlalchemy.orm import relationship

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class Orders(Base):
    __tablename__ = "orders"
    __table_args__ = ({"schema": SQLServerConsts.PORTFOLIO_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(String(36), primary_key=True, nullable=False, index=True)
    accountId = Column(
        String(36),
        ForeignKey(f"{SQLServerConsts.PORTFOLIO_SCHEMA}.accounts.id"),
        nullable=False,
    )
    brokerAccountId = Column(String(20), nullable=True)
    side = Column(String(10), nullable=False)
    symbol = Column(String(10), nullable=False)
    price = Column(Integer, nullable=False)
    quantity = Column(Integer, nullable=False)
    orderType = Column(String(10), nullable=False)
    orderStatus = Column(String(10), nullable=False)
    fillQuantity = Column(Integer, nullable=True)
    lastQuantity = Column(Integer, nullable=True)
    lastPrice = Column(Integer, nullable=True)
    averagePrice = Column(Integer, nullable=True)
    transDate = Column(String(30), nullable=True)
    createdDate = Column(String(30), nullable=True)
    modifiedDate = Column(String(30), nullable=True)
    leaveQuantity = Column(Integer, nullable=True)
    canceledQuantity = Column(Integer, nullable=True)
    priceSecure = Column(Integer, nullable=True)
    error = Column(String(255), nullable=True)

    # Relationships
    account = relationship("Accounts", back_populates="orders")

================
File: backend/modules/portfolio/entities/portfolio_metadata.py
================
from sqlalchemy import Column, Integer, String, Boolean

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class PortfolioMetadata(Base):
    __tablename__ = "portfolioMetadata"
    __table_args__ = ({"schema": SQLServerConsts.PORTFOLIO_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(
        Integer, primary_key=True, nullable=False, autoincrement=True, index=True
    )
    portfolioId = Column(String, nullable=False)
    userId = Column(Integer, nullable=True)
    portfolioName = Column(String, nullable=False)
    portfolioType = Column(String)
    portfolioDesc = Column(String)
    algorithm = Column(String)
    isActive = Column(Boolean, default=True)

================
File: backend/modules/portfolio/entities/portfolio.py
================
from sqlalchemy import Column, Integer, String, Float

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class Portfolios(Base):
    __tablename__ = "portfolios"
    __table_args__ = ({"schema": SQLServerConsts.PORTFOLIO_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(Integer, primary_key=True, nullable=False, autoincrement=True, index=True)
    date = Column(String, nullable=False)
    portfolioId = Column(String, nullable=False)
    symbol = Column(String, nullable=False)
    marketPrice = Column(Float, nullable=False)
    initialWeight = Column(Float, nullable=False)
    neutralizedWeight = Column(Float, nullable=True)
    limitedWeight = Column(Float, nullable=True)
    neutralizedLimitedWeight = Column(Float, nullable=True)

================
File: backend/modules/portfolio/entities/process_tracking.py
================
from sqlalchemy import Column, Integer, String, Float

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class ProcessTracking(Base):
    __tablename__ = "__processTracking__"
    __table_args__ = (
        {"schema": SQLServerConsts.PORTFOLIO_SCHEMA},
    )
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(Integer, primary_key=True, nullable=False, autoincrement=True, index=True)
    schemaName = Column(String)
    tableName = Column(String)
    keyName = Column(String)
    keyValue = Column(String)

================
File: backend/modules/portfolio/entities/universe_top_monthly.py
================
from sqlalchemy import Column, Integer, String, Float

from backend.common.consts import SQLServerConsts
from backend.modules.base.entities import Base


class StocksUniverse(Base):
    __tablename__ = "stocksUniverse"
    __table_args__ = ({"schema": SQLServerConsts.PORTFOLIO_SCHEMA},)
    __sqlServerType__ = f"[{SQLServerConsts.PORTFOLIO_SCHEMA}].[{__tablename__}]"

    id = Column(
        Integer, primary_key=True, nullable=False, autoincrement=True, index=True
    )
    year = Column(Integer, nullable=False)
    month = Column(Integer, nullable=False)
    symbol = Column(String, nullable=False)
    exchangeCode = Column(String, nullable=True)
    sectorL2 = Column(String, nullable=True)
    cap = Column(Float, nullable=True)
    averageLiquidity21 = Column(Float, nullable=True)
    averageLiquidity63 = Column(Float, nullable=True)
    averageLiquidity252 = Column(Float, nullable=True)
    grossProfitQoQ = Column(Float, nullable=True)
    roe = Column(Float, nullable=True)
    eps = Column(Float, nullable=True)
    pe = Column(Float, nullable=True)
    pb = Column(Float, nullable=True)

================
File: backend/modules/portfolio/handlers/__init__.py
================
from .routers import portfolio_router, accounts_router
from .portfolio import (
    get_portfolio_analysis,
    create_custom_portfolio,
    send_portfolio_report_notification,
    get_portfolios_by_id,
    get_my_portfolios,
    get_available_symbols,
    update_portfolio,
    delete_portfolio,
)
from .accounts import setup_dnse_account, get_default_account

================
File: backend/modules/portfolio/handlers/accounts.py
================
from fastapi import Depends, Request
from starlette.responses import JSONResponse

from backend.common.consts import MessageConsts
from backend.common.responses import SuccessResponse
from backend.modules.auth.decorators import UserPayload
from backend.modules.auth.guards import auth_guard
from backend.modules.auth.types import JwtPayload
from backend.modules.portfolio.dtos import SetupDNSEAccountDTO
from backend.modules.portfolio.handlers import accounts_router
from backend.modules.portfolio.services import AccountsService


@accounts_router.get("/default", dependencies=[Depends(auth_guard)])
async def get_default_account(
    user: JwtPayload = Depends(UserPayload)
):
    account = await AccountsService.get_default_account(user=user)
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=account,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@accounts_router.post("/setup", dependencies=[Depends(auth_guard)])
async def setup_dnse_account(
    payload: SetupDNSEAccountDTO, user: JwtPayload = Depends(UserPayload)
):
    result = await AccountsService.setup_dnse_account(user=user, payload=payload)
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=result,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())

================
File: backend/modules/portfolio/handlers/portfolio.py
================
from typing import List
from fastapi import Depends, Query
from httpcore import request
from starlette.responses import JSONResponse

from backend.common.consts import MessageConsts
from backend.common.responses.base import BaseResponse
from backend.common.responses import SuccessResponse
from backend.modules.portfolio.handlers import portfolio_router
from backend.modules.portfolio.services import (
    PortfolioAnalysisService,
    PortfolioNotificationService,
    PortfoliosService,
)
from backend.modules.portfolio.dtos import (
    CreateCustomPortfolioDTO,
    UpdatePortfolioDTO,
    AnalyzePortfolioDTO,
)
from backend.modules.auth.decorators import UserPayload
from backend.modules.auth.guards import auth_guard
from backend.modules.auth.types import JwtPayload
from backend.modules.notifications.telegram import MessageType


@portfolio_router.get("/me", dependencies=[Depends(auth_guard)])
async def get_my_portfolios(user: JwtPayload = Depends(UserPayload)):
    portfolios = await PortfoliosService.get_portfolios_by_user_id(user=user)
    response = SuccessResponse(
        http_code=200, status_code=200, message=MessageConsts.SUCCESS, data=portfolios
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.get("/system", dependencies=[Depends(auth_guard)])
async def get_system_portfolios(user: JwtPayload = Depends(UserPayload)):
    portfolios = await PortfoliosService.get_system_portfolios()
    response = SuccessResponse(
        http_code=200, status_code=200, message=MessageConsts.SUCCESS, data=portfolios
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.get("/symbols", dependencies=[Depends(auth_guard)])
async def get_available_symbols(user: JwtPayload = Depends(UserPayload)):
    available_symbols = await PortfoliosService.get_available_symbols()
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=available_symbols,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.get("/pnl/{portfolio_id}", dependencies=[Depends(auth_guard)])
async def get_portfolio_pnl_by_id(
    portfolio_id: str,
    strategy: str = "LongOnly",
    user: JwtPayload = Depends(UserPayload),
):
    pnl = await PortfoliosService.get_portfolio_pnl(
        portfolio_id=portfolio_id, strategy=strategy
    )
    response = SuccessResponse(
        http_code=200, status_code=200, message=MessageConsts.SUCCESS, data=pnl
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.get("/{portfolio_id}", dependencies=[Depends(auth_guard)])
async def get_portfolios_by_id(
    portfolio_id: str, user: JwtPayload = Depends(UserPayload)
):
    portfolio = await PortfoliosService.get_portfolios_by_id(
        portfolio_id=portfolio_id, user=user
    )
    response = SuccessResponse(
        http_code=200, status_code=200, message=MessageConsts.SUCCESS, data=portfolio
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.post("/create", dependencies=[Depends(auth_guard)])
async def create_custom_portfolio(
    payload: CreateCustomPortfolioDTO, user: JwtPayload = Depends(UserPayload)
):
    create_result = await PortfoliosService.create_portfolio(
        user_id=user.userId,
        portfolio_name=payload.portfolio_name,
        portfolio_desc=payload.portfolio_desc,
        symbols=payload.symbols,
    )

    response = SuccessResponse(
        http_code=201,
        status_code=201,
        message=MessageConsts.CREATED,
        data=create_result,
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.put("/update", dependencies=[Depends(auth_guard)])
async def update_portfolio(
    payload: UpdatePortfolioDTO, user: JwtPayload = Depends(UserPayload)
):
    if not payload.symbols or len(payload.symbols) < 2:
        response = BaseResponse(
            http_code=400,
            status_code=400,
            message=MessageConsts.BAD_REQUEST,
            errors="Portfolio must contain at least 2 symbols",
        )
        return JSONResponse(status_code=response.http_code, content=response.to_dict())

    # Update portfolio symbols
    update_result = await PortfoliosService.update_portfolio(
        portfolio_id=payload.portfolio_id, symbols=payload.symbols, user=user
    )

    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=update_result,
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.delete("/{portfolio_id}", dependencies=[Depends(auth_guard)])
async def delete_portfolio(portfolio_id: str, user: JwtPayload = Depends(UserPayload)):
    delete_result = await PortfoliosService.delete_portfolio(portfolio_id=portfolio_id)

    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=delete_result,
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.post(
    "/analysis/{broker_account_id}", dependencies=[Depends(auth_guard)]
)
async def get_portfolio_analysis(
    broker_account_id: str,
    payload: AnalyzePortfolioDTO,
    user: JwtPayload = Depends(UserPayload),
):
    analysis_result = await PortfolioAnalysisService.analyze_portfolio(
        broker_account_id=broker_account_id,
        portfolio_id=payload.portfolio_id,
        strategy_type=payload.strategy_type,
    )
    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message=MessageConsts.SUCCESS,
        data=analysis_result,
    )
    return JSONResponse(status_code=response.http_code, content=response.to_dict())


@portfolio_router.post(
    "/notify/{broker_account_id}", dependencies=[Depends(auth_guard)]
)
async def send_portfolio_report_notification(
    broker_account_id: str,
    user: JwtPayload = Depends(UserPayload),
    strategy_type: str = Query(
        default="MarketNeutral", description="Portfolio strategy type"
    ),
    include_trade_plan: bool = Query(
        default=True, description="Include trade recommendations in report"
    ),
):
    notify_result = await PortfolioNotificationService.send_portfolio_analysis_report(
        broker_account_id=broker_account_id,
        strategy_type=strategy_type,
        include_trade_plan=include_trade_plan,
        message_type=MessageType.CHART,
    )

    response = SuccessResponse(
        http_code=200,
        status_code=200,
        message="Portfolio report sent to Telegram successfully",
        data=notify_result,
    )

    return JSONResponse(status_code=response.http_code, content=response.to_dict())

================
File: backend/modules/portfolio/handlers/routers.py
================
from fastapi import APIRouter

portfolio_router = APIRouter()
accounts_router = APIRouter()

================
File: backend/modules/portfolio/infrastructure/__init__.py
================
from .trade_calendar import TradingCalendarService
from .report_generator import PortfolioReportGenerator

================
File: backend/modules/portfolio/infrastructure/report_generator.py
================
from typing import List, Dict
from backend.utils.logger import LOGGER


class PortfolioReportGenerator:
    @staticmethod
    def generate_telegram_report(
        analysis_result: dict, include_trade_plan: bool = True
    ) -> str:
        """Generate a formatted Telegram report"""
        try:
            account_id = analysis_result.get("account_id", "N/A")
            strategy_type = analysis_result.get("strategy_type", "N/A")
            account_balance = analysis_result.get("account_balance", {})
            current_positions = analysis_result.get("current_positions", [])
            recommendations = analysis_result.get("recommendations", [])
            analysis_date = analysis_result.get("analysis_date", "N/A")

            # Start building report
            report_lines = []
            report_lines.append(" <b>BO CO DANH MC U T</b>")
            report_lines.append(f" Ngy giao dch: {analysis_date}")
            report_lines.append(f" Ti khon: <code>{account_id}</code>")
            report_lines.append(
                f" Chin lc: {strategy_type.replace('_', ' ').title()}"
            )
            report_lines.append("")

            # Account balance summary
            report_lines.append(" <b>TNH HNH TI KHON</b>")
            net_asset = account_balance.get("net_asset_value", 0)
            available_cash = account_balance.get("available_cash", 0)
            cash_ratio = account_balance.get("cash_ratio", 0)

            report_lines.append(f" Tng ti sn: <b>{net_asset:,.0f} VND</b>")
            report_lines.append(f" Tin mt: <b>{available_cash:,.0f} VND</b>")
            report_lines.append(f" T l tin mt: <b>{cash_ratio:.1f}%</b>")
            report_lines.append("")

            # Current portfolio (top 5)
            if current_positions:
                report_lines.append(
                    f" <b>DANH MC HIN TI</b> ({len(current_positions)} c phiu)"
                )
                for i, pos in enumerate(current_positions[:5], 1):
                    symbol = pos.get("symbol", "N/A")
                    weight_pct = pos.get("weight", {}).get("percentage", 0)
                    quantity = pos.get("quantity", 0)
                    market_price = pos.get("market_price", {}).get("amount", 0)

                    report_lines.append(
                        f"{i}. <b>{symbol}</b>: {weight_pct:.1f}% "
                        f"({quantity:,} c {market_price:,.0f} VND)"
                    )

                # if len(current_positions) > 5:
                #     report_lines.append(
                #         f"... v {len(current_positions) - 5} c phiu khc"
                #     )
                report_lines.append("")

            # Recommendations
            buy_recs = [r for r in recommendations if r.get("action") == "BUY"]
            sell_recs = [r for r in recommendations if r.get("action") == "SELL"]

            if recommendations and include_trade_plan:
                report_lines.append(
                    f" <b>KHUYN NGH GIAO DCH</b> ({len(recommendations)} giao dch)"
                )

                if buy_recs:
                    report_lines.append(
                        f" <b>MUA VO</b> ({len(buy_recs)} c phiu):"
                    )
                    for rec in buy_recs[:]:
                        symbol = rec.get("symbol", "N/A")
                        target_weight = rec.get("target_weight", {}).get(
                            "percentage", 0
                        )
                        amount = rec.get("amount", {}).get("amount", 0)
                        action_price = rec.get("action_price", {}).get("amount", 0)
                        action_quantity = rec.get("action_quantity", 0)
                        priority = rec.get("priority", "N/A")

                        report_lines.append(
                            f" <b>{symbol}</b>: {target_weight:.1f}% "
                            f"(KL: {action_quantity:,} - Gi {action_price:,.0f} VND) "
                            # f" Tng gi tr: {amount:,.0f} VND - {priority}"
                        )
                    # if len(buy_recs) > 3:
                    #     report_lines.append(f"... v {len(buy_recs) - 3} c phiu khc")
                    report_lines.append("")

                if sell_recs:
                    report_lines.append(
                        f" <b>BN RA</b> ({len(sell_recs)} c phiu):"
                    )
                    for rec in sell_recs[:]:
                        symbol = rec.get("symbol", "N/A")
                        current_weight = rec.get("current_weight", {}).get(
                            "percentage", 0
                        )
                        target_weight = rec.get("target_weight", {}).get(
                            "percentage", 0
                        )
                        action_price = rec.get("action_price", {}).get("amount", 0)
                        action_quantity = rec.get("action_quantity", 0)
                        amount = rec.get("amount", {}).get("amount", 0)
                        priority = rec.get("priority", "N/A")

                        report_lines.append(
                            f" <b>{symbol}</b>: {current_weight:.1f}%  {target_weight:.1f}% "
                            f"(KL: {action_quantity:,} - Gi: {action_price:,.0f} VND) "
                            # f" Thu v: {amount:,.0f} VND - {priority}"
                        )
                    # if len(sell_recs) > 3:
                    #     report_lines.append(
                    #         f"... v {len(sell_recs) - 3} c phiu khc"
                    #     )
            elif not recommendations:
                report_lines.append(
                    " <b>DANH MC  CN BNG</b> - Khng cn iu chnh"
                )

            return "\n".join(report_lines)

        except Exception as e:
            LOGGER.error(f"Error generating Telegram report: {e}")
            return (
                f" <b>LI TO BO CO</b>\nKhng th to bo co portfolio: {str(e)}"
            )

================
File: backend/modules/portfolio/infrastructure/trade_calendar.py
================
from datetime import datetime, timedelta
from backend.utils.time_utils import TimeUtils
from backend.utils.logger import LOGGER

class TradingCalendarService:
    @staticmethod
    def is_trading_day(date: datetime) -> bool:
        return date.weekday() < 5

    @staticmethod
    def get_last_trading_date(current_date: datetime) -> datetime:
        check_date = current_date - timedelta(days=1)
        while not TradingCalendarService.is_trading_day(check_date):
            check_date -= timedelta(days=1)
        return check_date

    @staticmethod
    def get_next_trading_date(current_date: datetime) -> datetime:
        check_date = current_date + timedelta(days=1)
        while not TradingCalendarService.is_trading_day(check_date):
            check_date += timedelta(days=1)
        return check_date
    
    @classmethod
    def get_last_next_trading_dates(cls) -> tuple[datetime, datetime]:

        current_date = TimeUtils.get_current_vn_time()

        if not cls.is_trading_day(current_date):
            last_trading_date = cls.get_last_trading_date(current_date)
            next_trading_date = cls.get_next_trading_date(current_date)
            return last_trading_date, next_trading_date

        current_hour = current_date.hour
        if current_hour >= 17:
            last_trading_date = current_date
            next_trading_date = cls.get_next_trading_date(current_date)
        else:
            last_trading_date = cls.get_last_trading_date(current_date)
            next_trading_date = current_date
    
        return last_trading_date, next_trading_date

================
File: backend/modules/portfolio/repositories/__init__.py
================
from .accounts import AccountsRepo
from .balances import BalancesRepo
from .orders import OrdersRepo
from .deals import DealsRepo
from .portfolio import PortfoliosRepo
from .portfolio_metadata import PortfolioMetadataRepo
from .universe_top_monthly import StocksUniverseRepo
from .process_tracking import ProcessTrackingRepo

================
File: backend/modules/portfolio/repositories/accounts.py
================
from typing import Dict, List, Optional

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import Accounts


class AccountsRepo(BaseRepo[Dict]):
    entity = Accounts
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

    @classmethod
    async def get_by_user_id(cls, user_id: int) -> List[Dict]:
        with cls.session_scope() as session:
            conditions = {cls.entity.userId.name: user_id}
            records = await cls.get_by_condition(conditions=conditions)
            session.commit()
        return records

    @classmethod
    async def get_accounts_by_username(cls, custody_code: str) -> Optional[Dict]:
        with cls.session_scope() as session:
            conditions = {cls.entity.custodyCode.name: custody_code}
            records = await cls.get_by_condition(conditions=conditions)
            session.commit()

        return records if records else None
    
    @classmethod
    async def get_by_broker_account_id(cls, broker_account_id: str) -> Optional[Dict]:
        with cls.session_scope() as session:
            conditions = {cls.entity.brokerAccountId.name: broker_account_id}
            records = await cls.get_by_condition(conditions=conditions)
            session.commit()

        return records if records else None

================
File: backend/modules/portfolio/repositories/balances.py
================
from typing import Dict, Optional

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import Balances


class BalancesRepo(BaseRepo[Dict]):
    entity = Balances
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

================
File: backend/modules/portfolio/repositories/deals.py
================
from typing import Dict, List, Optional

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import Deals


class DealsRepo(BaseRepo[Dict]):
    entity = Deals
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

    @classmethod
    async def get_deals_by_broker_account_id(
        cls, broker_account_id: str
    ) -> List[Dict]:
        with cls.session_scope() as session:
            conditions = {cls.entity.brokerAccountId.name: broker_account_id}
            results = await cls.get_by_condition(conditions=conditions)
            # Ensure the session is used for the query
            session.commit()
        return results

================
File: backend/modules/portfolio/repositories/orders.py
================
from typing import Dict, List, Optional

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import Orders


class OrdersRepo(BaseRepo[Dict]):
    entity = Orders
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

================
File: backend/modules/portfolio/repositories/portfolio_metadata.py
================
from typing import Dict

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import PortfolioMetadata

class PortfolioMetadataRepo(BaseRepo[Dict]):
    entity = PortfolioMetadata
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

    @classmethod
    async def get_by_portfolio_id(cls, portfolio_id: str) -> Dict:
        conditions = {cls.entity.portfolioId.name: portfolio_id}
        records = await cls.get_by_condition(conditions=conditions)
        return records
    
    @classmethod
    async def get_by_user_id(cls, user_id: int) -> Dict:
        conditions = {cls.entity.userId.name: user_id}
        records = await cls.get_by_condition(conditions=conditions)
        return records

    @classmethod
    async def delete_by_portfolio_id(cls, portfolio_id: str) -> None:
        conditions = {cls.entity.portfolioId.name: portfolio_id}
        await cls.delete(conditions=conditions)

================
File: backend/modules/portfolio/repositories/portfolio.py
================
from typing import Dict

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import Portfolios


class PortfoliosRepo(BaseRepo[Dict]):
    entity = Portfolios
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

    @classmethod
    async def get_by_portfolio_id(cls, portfolio_id: str) -> Dict:
        conditions = {cls.entity.portfolioId.name: portfolio_id}
        records = await cls.get_by_condition(conditions=conditions)
        return records
    
    @classmethod
    async def get_by_user_id(cls, user_id: int) -> Dict:
        conditions = {cls.entity.userId.name: user_id}
        records = await cls.get_by_condition(conditions=conditions)
        return records

    @classmethod
    async def delete_by_portfolio_id(cls, portfolio_id: str) -> None:
        conditions = {cls.entity.portfolioId.name: portfolio_id}
        await cls.delete(conditions=conditions)

================
File: backend/modules/portfolio/repositories/process_tracking.py
================
from typing import Dict

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import ProcessTracking


class ProcessTrackingRepo(BaseRepo[Dict]):
    entity = ProcessTracking
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

================
File: backend/modules/portfolio/repositories/universe_top_monthly.py
================
from typing import Dict

from backend.db.sessions import backend_session_scope
from backend.modules.base.query_builder import BaseQueryBuilder
from backend.modules.base.repositories import BaseRepo
from backend.modules.portfolio.entities import StocksUniverse


class StocksUniverseRepo(BaseRepo[Dict]):
    entity = StocksUniverse
    query_builder = BaseQueryBuilder(entity=entity)
    session_scope = backend_session_scope

    @classmethod
    async def get_asset_by_year_month(cls, year, month):
        with cls.session_scope() as session:
            conditions = {
                cls.entity.year.name: year,
                cls.entity.month.name: month,
            }
            records = await cls.get_by_condition(conditions=conditions)
            session.commit()
        return records

================
File: backend/modules/portfolio/services/__init__.py
================
from .portfolio_accounts_service import AccountsService
from .portfolio_balance_service import BalanceService
from .portfolio_deals_service import DealsService
from .portfolio_universe_service import StocksUniverseService
from .portfolio_service import PortfoliosService
from .portfolio_notification_service import PortfolioNotificationService
from .portfolio_analysis_service import PortfolioAnalysisService
from .portfolio_daily_pipeline_service import DailyDataPipelineService

================
File: backend/modules/portfolio/services/data_providers/__init__.py
================
from .account_data_provider import AccountDataProvider
from .portfolio_data_provider import PortfolioDataProvider
from .price_data_provider import PriceDataProvider

================
File: backend/modules/portfolio/services/data_providers/account_data_provider.py
================
from typing import Dict, Any, Optional, Union
from backend.modules.portfolio.repositories import AccountsRepo


class AccountDataProvider:
    repo = AccountsRepo

    @classmethod
    async def get_trading_account(cls, broker_account_id: str) -> Optional[Dict]:
        existing_accounts = await cls.repo.get_by_broker_account_id(
            broker_account_id=broker_account_id
        )

        if not existing_accounts:
            return None

        # Find trading account (type "0449")
        for account in existing_accounts:
            if account.get("accountType") == "0449":
                return account

        return None

================
File: backend/modules/portfolio/services/data_providers/portfolio_data_provider.py
================
from typing import Optional, Dict

from backend.common.consts import TradingConsts
from backend.modules.portfolio.repositories import PortfoliosRepo
from backend.modules.portfolio.entities import Portfolios
from backend.modules.portfolio.utils.portfolio_utils import PortfolioUtils
from backend.utils.logger import LOGGER


class PortfolioDataProvider:
    repo = PortfoliosRepo

    @classmethod
    async def get_portfolio_weights_by_id(cls, portfolio_id) -> Optional[Dict]:
        try:
            with cls.repo.session_scope() as session:
                conditions = {Portfolios.portfolioId.name: portfolio_id}
                records = await cls.repo.get_by_condition(conditions=conditions)

                if not records:
                    return None

                latest_date = max(record[Portfolios.date.name] for record in records)
                portfolios = [
                    record
                    for record in records
                    if (
                        record[Portfolios.portfolioId.name] == portfolio_id
                        and record[Portfolios.date.name] == latest_date
                    )
                ]

                long_only_positions = []
                market_neutral_positions = []

                for position in portfolios:
                    symbol = position[Portfolios.symbol.name]
                    initial_weight_pct = float(
                        position[Portfolios.initialWeight.name] * 100 or 0
                    )
                    neutralized_weight_pct = float(
                        position[Portfolios.neutralizedWeight.name] * 100 or 0
                    )

                    if initial_weight_pct >= 1:
                        long_only_positions.append(
                            {
                                "symbol": symbol,
                                "marketPrice": float(
                                    position[Portfolios.marketPrice.name]
                                ),
                                "weight": float(initial_weight_pct),
                            }
                        )

                    if neutralized_weight_pct >= 1:
                        market_neutral_positions.append(
                            {
                                "symbol": symbol,
                                "marketPrice": float(
                                    position[Portfolios.marketPrice.name]
                                ),
                                "weight": float(
                                    min(
                                        neutralized_weight_pct,
                                        TradingConsts.LIMIT_WEIGHT_PCT,
                                    )
                                ),
                            }
                        )

                long_only_positions.sort(key=lambda x: x["weight"], reverse=True)
                market_neutral_positions.sort(key=lambda x: x["weight"], reverse=True)
                session.commit()

            return {
                "date": latest_date,
                "LongOnly": long_only_positions,
                "MarketNeutral": market_neutral_positions,
            }

        except Exception as e:
            LOGGER.error(f"Error getting portfolio weights: {e}")
            return None

================
File: backend/modules/portfolio/services/data_providers/price_data_provider.py
================
import pandas as pd
import pickle
import warnings
import redis
from typing import Optional
from datetime import timedelta

from backend.common.consts import SQLServerConsts
from backend.redis.client import REDIS_CLIENT
from backend.db.sessions import mart_session_scope
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils


LOGGER_PREFIX = "[Redis]"


try:
    redis_conn = REDIS_CLIENT.get_conn()
    redis_binary_conn = redis.Redis(
        connection_pool=redis.ConnectionPool(
            host=redis_conn.connection_pool.connection_kwargs.get("host"),
            port=redis_conn.connection_pool.connection_kwargs.get("port"),
            db=redis_conn.connection_pool.connection_kwargs.get("db", 0),
            password=redis_conn.connection_pool.connection_kwargs.get("password"),
            decode_responses=False,  # Keep binary data as bytes
            socket_timeout=10.0,
            socket_connect_timeout=10.0,
        )
    )
    redis_binary_conn.ping()
    REDIS_AVAILABLE = True
except Exception as e:
    LOGGER.error(f"{LOGGER_PREFIX} Failed to connect to Redis: {e}")
    LOGGER.info(f"{LOGGER_PREFIX} Redis is not available, falling back to SQL Server")
    REDIS_AVAILABLE = False


class PriceDataProvider:
    def __init__(self, prefix: str = "STOCK"):
        self.prefix = prefix
        self.cache_ttl = 12 * 3600

    async def get_market_data(self, from_date: Optional[str] = None) -> pd.DataFrame:
        if from_date is None:
            current_date = TimeUtils.get_current_vn_time()
            from_date = (current_date - timedelta(days=63)).strftime(
                SQLServerConsts.DATE_FORMAT
            )

        cache_key = self.generate_cache_key(from_date)

        cached_data = await self.get_from_cache(cache_key)
        if cached_data is not None:
            LOGGER.info(f"{LOGGER_PREFIX} Cache HIT for price data: {cache_key}")
            return cached_data

        LOGGER.info(f"{LOGGER_PREFIX} Cache MISS for price data: {cache_key} - fetching from DB")

        df_pivoted = await self.fetch_price_data_from_database(from_date)
        await self.save_to_cache(cache_key, df_pivoted)
        return df_pivoted

    def generate_cache_key(self, from_date: str) -> str:
        key_components = [self.prefix, from_date]

        current_date = TimeUtils.get_current_vn_time()
        today = current_date.strftime(SQLServerConsts.DATE_FORMAT)
        key_components.append(f"cached-{today}")

        return ":".join(key_components)

    @classmethod
    async def get_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        try:
            # Use binary connection to avoid decoding issues
            cached_bytes = redis_binary_conn.get(cache_key)

            if cached_bytes:
                try:
                    df = pickle.loads(cached_bytes)

                    if isinstance(df, pd.DataFrame) and not df.empty:
                        LOGGER.debug(
                            f"{LOGGER_PREFIX} Successfully retrieved cached DataFrame: shape {df.shape}"
                        )
                        return df
                    else:
                        LOGGER.warning(
                            f"{LOGGER_PREFIX} Invalid cached DataFrame for key: {cache_key}"
                        )
                        redis_binary_conn.delete(cache_key)

                except (
                    pickle.UnpicklingError,
                    UnicodeDecodeError,
                    ValueError,
                ) as decode_error:
                    LOGGER.warning(
                        f"{LOGGER_PREFIX} Corrupted cache data for key {cache_key}: {str(decode_error)}"
                    )
                    redis_binary_conn.delete(cache_key)
                    return None

            return None

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Error retrieving from cache {cache_key}: {str(e)}")
            try:
                redis_binary_conn.delete(cache_key)
                LOGGER.info(f"{LOGGER_PREFIX} Cleaned up corrupted cache entry: {cache_key}")
            except:
                pass
            return None

    async def save_to_cache(self, cache_key: str, df: pd.DataFrame) -> bool:
        """Save DataFrame to Redis cache"""
        try:
            if df.empty:
                LOGGER.warning(f"{LOGGER_PREFIX} Attempted to cache empty DataFrame - skipping")
                return False

            df_bytes = pickle.dumps(df, protocol=4)

            size_mb = len(df_bytes) / (1024 * 1024)
            if size_mb > 100:  # 100MB limit
                LOGGER.warning(
                    f"{LOGGER_PREFIX} DataFrame too large to cache: {size_mb:.2f}MB - skipping"
                )
                return False

            success = redis_binary_conn.setex(cache_key, self.cache_ttl, df_bytes)

            if success:
                LOGGER.info(
                    f"{LOGGER_PREFIX} Cached DataFrame: key={cache_key}, shape={df.shape}, size={size_mb:.2f}MB, ttl={self.cache_ttl}s"
                )
                return True
            else:
                LOGGER.warning(f"{LOGGER_PREFIX} Failed to set cache entry: {cache_key}")
                return False

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Error saving to cache {cache_key}: {str(e)}")
            return False

    async def fetch_price_data_from_database(self, from_date: str) -> pd.DataFrame:
        try:
            with mart_session_scope() as mart_session:

                if self.prefix == "STOCK":
                    sql_query = f"""
                        SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                        WITH UncommittedData AS (
                            SELECT 
                                [ticker] 
                                ,[date] 
                                ,[closePriceAdjusted]
                                ,ROW_NUMBER() OVER (PARTITION BY [ticker], [date] ORDER BY __updatedAt__ DESC) AS rn
                            FROM [priceVolume].[priceVolume]
                            WHERE [date] >= '{from_date}'
                        )

                        SELECT [ticker], [date], [closePriceAdjusted]
                        FROM UncommittedData
                        WHERE rn = 1
                        ORDER BY [date], [ticker];
                    """
                elif self.prefix == "INDEX":
                    sql_query = f"""
                        SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                        WITH UncommittedData AS (
                            SELECT 
                                [icbCode] 
                                ,[date] 
                                ,[closeIndex]
                                ,ROW_NUMBER() OVER (PARTITION BY [icbCode], [date] ORDER BY __updatedAt__ DESC) AS rn
                            FROM [priceVolume].[isPriceVolume]
                            WHERE [icbCode] = 'VNINDEX' AND [date] >= '{from_date}'
                        )

                        SELECT [icbCode], [date], [closeIndex]
                        FROM UncommittedData
                        WHERE rn = 1
                        ORDER BY [date], [icbCode];
                    """

                with warnings.catch_warnings():
                    warnings.simplefilter("ignore", UserWarning)
                    conn = mart_session.connection().connection

                    # Fetch data
                    df = pd.read_sql_query(sql_query, conn)

                    if df.empty:
                        LOGGER.warning(
                            f"{LOGGER_PREFIX} No price data found for criteria: from_date={from_date}"
                        )
                        return pd.DataFrame()

                    # Pivot the data
                    if self.prefix == "STOCK":
                        df_pivoted = df.pivot(
                            index="date", columns="ticker", values="closePriceAdjusted"
                        )
                    elif self.prefix == "INDEX":
                        df_pivoted = df[["date", "closeIndex"]].copy()

                    LOGGER.info(
                        f"{LOGGER_PREFIX} Fetched price data from DB: shape={df_pivoted.shape}, date_range={from_date} to {df_pivoted.index.max()}"
                    )
                    return df_pivoted

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Error fetching price data from database: {str(e)}")
            raise e

================
File: backend/modules/portfolio/services/portfolio_accounts_service.py
================
import uuid
from typing import Dict, Optional

from backend.common.consts import MessageConsts, SQLServerConsts
from backend.common.responses.exceptions.base_exceptions import BaseExceptionResponse
from backend.modules.auth.types import JwtPayload
from backend.modules.base.query_builder import TextSQL
from backend.modules.dnse.trading_session import TradingSession
from backend.modules.auth.entities import Users
from backend.modules.auth.repositories import UsersRepo
from backend.modules.portfolio.dtos import SetupDNSEAccountDTO, DefaultAccountResponseDTO
from backend.modules.portfolio.entities import Accounts
from backend.modules.portfolio.repositories import AccountsRepo
from backend.utils.logger import LOGGER


class AccountsService:
    repo = AccountsRepo
    user_repo = UsersRepo

    @classmethod
    async def get_default_account(cls, user: JwtPayload):
        try:
            accounts = await cls.repo.get_by_user_id(user.userId)
            if not accounts:
                raise BaseExceptionResponse(
                    http_code=404,
                    status_code=404,
                    message=MessageConsts.NOT_FOUND,
                    errors="No accounts found for this user",
                )
            for account in accounts:
                if account[Accounts.accountType.name] == "0449":
                    return DefaultAccountResponseDTO(
                        account_id=account[Accounts.id.name],
                        name=account[Accounts.name.name],
                        custody_code=account[Accounts.custodyCode.name],
                        broker_name=account[Accounts.brokerName.name],
                        broker_account_id=account[Accounts.brokerAccountId.name],
                        broker_investor_id=account[Accounts.brokerInvestorId.name],
                        is_default=True,
                    ).model_dump()
            return None
        
        except Exception as e:
            LOGGER.error(f"Failed to fetch accounts for user {user.userId}: {e}")
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )


    @classmethod
    async def setup_dnse_account(cls, user: JwtPayload, payload: SetupDNSEAccountDTO):
        try:

            existing_account = await cls.repo.get_accounts_by_username(payload.username)
            if existing_account:
                for account in existing_account:
                    if account[Accounts.userId.name] != user.userId:
                        raise BaseExceptionResponse(
                            http_code=409,
                            status_code=409,
                            message=MessageConsts.CONFLICT,
                            errors="This broker account ID is already registered to another user",
                        )

            username = payload.username
            password = payload.password

            async with TradingSession(account=username) as session:
                if not await session.authenticate(password=password):
                    raise BaseExceptionResponse(
                        http_code=401,
                        status_code=401,
                        message=MessageConsts.UNAUTHORIZED,
                        errors="Authentication failed",
                    )
                LOGGER.info(
                    f"User {user.userId} authenticated successfully with DNSE account {username}"
                )

                if session.is_jwt_authenticated():
                    async with session.users_client() as users_client:
                        user_dict = await users_client.get_users_info()
                        accounts_dict = await users_client.get_user_accounts()

                        if not user_dict or not accounts_dict:
                            raise BaseExceptionResponse(
                                http_code=502,
                                status_code=502,
                                message=MessageConsts.TRADING_API_ERROR,
                                errors="Failed to fetch user or account information from DNSE API",
                            )

                        user_data = {
                            Users.id.name: user.userId,
                            Users.mobile.name: user_dict.get("mobile"),
                            Users.email.name: user_dict.get("email"),
                        }

                        accounts_data = []
                        accounts = accounts_dict.get("accounts", [])
                        for account in accounts:
                            accounts_data.append(
                                {
                                    Accounts.userId.name: user.userId,
                                    Accounts.name.name: account.get("name"),
                                    Accounts.accountType.name: account.get(
                                        "accountType"
                                    ),
                                    Accounts.identificationCode.name: user_dict.get(
                                        "identificationCode"
                                    ),
                                    Accounts.custodyCode.name: account.get(
                                        "custodyCode"
                                    ),
                                    Accounts.password.name: payload.password,
                                    Accounts.brokerName.name: "DNSE",
                                    Accounts.brokerAccountId.name: account.get("id"),
                                    Accounts.brokerInvestorId.name: account.get(
                                        "investorId"
                                    ),
                                }
                            )

                        with cls.repo.session_scope() as session:
                            await cls.user_repo.update(
                                record={
                                    Users.id.name: user.userId,
                                    Users.mobile.name: user_data.get("mobile"),
                                    Users.email.name: user_data.get("email"),
                                },
                                identity_columns=[Users.id.name],
                                returning=False,
                                text_clauses={
                                    "__updatedAt__": TextSQL(
                                        SQLServerConsts.GMT_7_NOW_VARCHAR
                                    )
                                },
                            )

                            temp_table = f"#{cls.repo.query_builder.table}"
                            await cls.repo.upsert(
                                temp_table=temp_table,
                                records=accounts_data,
                                identity_columns=["custodyCode", "brokerAccountId"],
                                text_clauses={
                                    "__updatedAt__": TextSQL(
                                        SQLServerConsts.GMT_7_NOW_VARCHAR
                                    )
                                },
                            )

                            session.commit()


            return {
                "message": "DNSE account setup successfully",
            }

        except BaseExceptionResponse:
            raise
        except Exception as e:
            LOGGER.error(f"Failed to setup DNSE account for user {user.userId}: {e}")
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

================
File: backend/modules/portfolio/services/portfolio_analysis_service.py
================
from typing import Dict, Optional
from decimal import Decimal

from backend.common.consts import SQLServerConsts, MessageConsts
from backend.common.responses.exceptions.base_exceptions import BaseExceptionResponse
from backend.modules.dnse.trading_session import TradingSession
from backend.modules.portfolio.core import Money
from backend.modules.portfolio.core.strategies import StrategyFactory
from backend.modules.portfolio.services.processors import (
    PortfolioProcessor,
    RecommendationEngine,
)
from backend.modules.portfolio.services.data_providers import (
    PortfolioDataProvider,
    AccountDataProvider,
)
from backend.modules.portfolio.infrastructure import TradingCalendarService
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils
from backend.utils.json_utils import JSONUtils


LOGGER_PREFIX = "[PortfolioAnalysis]"


class PortfolioAnalysisService:
    portfolio_data_provider = PortfolioDataProvider
    account_data_provider = AccountDataProvider
    recommendation_engine = RecommendationEngine()
    trading_calendar = TradingCalendarService

    @classmethod
    async def analyze_portfolio(
        cls,
        broker_account_id: str,
        portfolio_id: str,
        strategy_type: str = "MarketNeutral",
    ) -> Optional[Dict]:
        try:
            trade_account = await cls.account_data_provider.get_trading_account(
                broker_account_id
            )
            if not trade_account:
                LOGGER.warning(
                    f"{LOGGER_PREFIX} No trading account found for broker_account_id: {broker_account_id}"
                )
                return None

            custody_code = trade_account.get("custodyCode")
            password = trade_account.get("password")

            async with TradingSession(account=custody_code) as session:
                if not await session.authenticate(password=password):
                    raise BaseExceptionResponse(
                        http_code=401,
                        status_code=401,
                        message=MessageConsts.UNAUTHORIZED,
                        errors="Authentication failed",
                    )

                async with session.users_client() as users_client:
                    balance_dict = await users_client.get_account_balance(account_no=broker_account_id)
                    deals_dict = await users_client.get_account_deals(account_no=broker_account_id)

                    if not balance_dict:
                        LOGGER.warning(
                            f"{LOGGER_PREFIX} No balance data found for account {broker_account_id}"
                        )
                        return None

                    if not deals_dict:
                        LOGGER.warning(
                            f"{LOGGER_PREFIX} No deals data found for account {broker_account_id}"
                        )
                        return None

                    deals_list = deals_dict.get("deals", [])
                    available_cash = Money(Decimal(str(balance_dict.get("availableCash", 0))))
                    net_asset_value = Money(Decimal(str(balance_dict.get("netAssetValue", 0))))
                    stock_value = Money(Decimal(str(balance_dict.get("stockValue", 0))))

                    # Process current deals into portfolio positions
                    current_positions = PortfolioProcessor.process_deals_to_positions(
                        deals_list,
                        float(net_asset_value.amount),
                        float(stock_value.amount),
                    )

                    portfolio_data = (
                        await cls.portfolio_data_provider.get_portfolio_weights_by_id(
                            portfolio_id=portfolio_id
                        )
                    )

                    if not portfolio_data:
                        return None

                    # Get target weights based on strategy: long only or market neutral
                    strategy = StrategyFactory.create_strategy(strategy_type)
                    target_weights = strategy.get_target_weights(portfolio_data)

                    # Generate recommendations
                    recommendations = (
                        cls.recommendation_engine.generate_recommendations(
                            current_positions=current_positions,
                            target_weights=target_weights,
                            available_cash=available_cash,
                            net_asset_value=net_asset_value,
                        )
                    )

                    result = {
                        "account_id": broker_account_id,
                        "strategy_type": strategy_type,
                        "account_balance": {
                            "available_cash": float(available_cash.amount),
                            "net_asset_value": float(net_asset_value.amount),
                            "cash_ratio": float(
                                (available_cash.amount / net_asset_value.amount * 100)
                                if net_asset_value.amount > 0
                                else 0
                            ),
                        },
                        "current_positions": [
                            pos.to_dict() for pos in current_positions
                        ],
                        "target_weights": target_weights,
                        "recommendations": [rec.to_dict() for rec in recommendations],
                        "analysis_date": (
                                TimeUtils.get_current_vn_time().strftime(
                                SQLServerConsts.DATE_FORMAT
                            )
                        ),
                    }

                    serializable_result = JSONUtils.make_json_serializable(result)
                    return serializable_result

        except Exception as e:
            LOGGER.error(
                f"{LOGGER_PREFIX} Error in portfolio analysis for account {broker_account_id}: {str(e)}"
            )
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

================
File: backend/modules/portfolio/services/portfolio_balance_service.py
================
from typing import List, Optional, Dict, Any
from decimal import Decimal
from datetime import datetime

from backend.common.consts import SQLServerConsts, MessageConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.base.query_builder import TextSQL
from backend.modules.portfolio.entities import Balances
from backend.modules.portfolio.repositories import AccountsRepo, BalancesRepo
from backend.modules.portfolio.utils.balance_utils import BalanceUtils
from backend.modules.dnse.trading_session import TradingSession
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils


LOGGER_PREFIX = "[BalanceService]"


class BalanceService:
    repo = BalancesRepo
    accounts_repo = AccountsRepo

    @classmethod
    async def update_newest_balances_daily(cls) -> bool:
        try:

            existing_accounts = await cls.accounts_repo.get_all()
            existing_accounts = [
                account
                for account in existing_accounts
                if account.get("accountType") == "0449"
            ]

            if len(existing_accounts) == 0:
                LOGGER.warning(f"No account found")
                return False


            data = []
            for account in existing_accounts:
                custody_code = account.get("custodyCode")
                password = account.get("password")
                broker_account_id = account.get("brokerAccountId")

                # Get current balance from DNSE API
                async with TradingSession(account=custody_code) as session:
                    if not await session.authenticate(password=password):
                        LOGGER.error(f"{LOGGER_PREFIX} Authentication failed for account {custody_code}")
                        raise BaseExceptionResponse(
                            http_code=404,
                            status_code=404,
                            message=MessageConsts.VALIDATION_FAILED,
                            errors=str(e),
                        )
                    
                    async with session.users_client() as users_client:
                        balance_dict = await users_client.get_account_balance(account_no=broker_account_id)

                        if not balance_dict:
                            LOGGER.warning(f"{LOGGER_PREFIX} No balance data found for account {broker_account_id}")
                            continue

                        # Prepare balance data
                        current_time = TimeUtils.get_current_vn_time()
                        today_str = current_time.strftime(SQLServerConsts.DATE_FORMAT)
                        balance_data = BalanceUtils.extract_balance_data(
                            raw_data=balance_dict, date=today_str
                        )
                        data.append(balance_data)


            with cls.repo.session_scope() as session:
                if data:
                    # Upsert balances
                    await cls.repo.upsert(
                        temp_table=f"#{cls.repo.query_builder.table}",
                        records=data,
                        identity_columns=[Balances.brokerAccountId.name, Balances.date.name],
                        text_clauses={
                            "__updatedAt__": TextSQL(
                                SQLServerConsts.GMT_7_NOW_VARCHAR
                            )
                        },
                    )
                    session.commit()
                    LOGGER.info(f"{LOGGER_PREFIX} Balances updated successfully for {len(data)} accounts")
                else:
                    LOGGER.warning(f"{LOGGER_PREFIX} No balance data to update")

            return {"success": True}


        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

================
File: backend/modules/portfolio/services/portfolio_daily_pipeline_service.py
================
import asyncio
from datetime import datetime, time, timedelta
from typing import Dict, Any


from backend.common.consts import MessageConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.portfolio.services.portfolio_balance_service import BalanceService
from backend.modules.portfolio.services.portfolio_deals_service import DealsService
from backend.modules.portfolio.services import StocksUniverseService
from backend.modules.portfolio.services.portfolio_service import PortfoliosService
from backend.modules.portfolio.services.portfolio_notification_service import PortfolioNotificationService
from backend.modules.notifications.service import notify_error, notify_success
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils


LOGGER_PREFIX = "[Pipeline]"


class DailyDataPipelineService:
    """Pipeline to update all portfolio data daily at 7PM"""

    PIPELINE_TIME = time(0, 0)  # 7:00 PM
    TEST_INTERVAL_MINUTES = 5  # Run every 5 minutes in test mode

    @classmethod
    async def run_pipeline(cls) -> Dict[str, Any]:
        """Run the complete daily data update pipeline"""
        start_time = datetime.now()
        LOGGER.info(f"{LOGGER_PREFIX} Starting daily data pipeline")

        pipeline_results = {
            "start_time": start_time.isoformat(),
            "steps": {},
            "success": True,
            "total_duration": 0,
        }

        try:
            # Step 1: Update Balance Data
            LOGGER.info(f"{LOGGER_PREFIX} Phase 1: Updating balance data...")
            balance_start = datetime.now()
            balance_result = await BalanceService.update_newest_balances_daily()
            balance_duration = (datetime.now() - balance_start).total_seconds()

            pipeline_results["steps"]["balance_update"] = {
                # "result": balance_result,
                "duration_seconds": balance_duration,
                "success": balance_result.get("success", False),
            }

            if not balance_result.get("success", False):
                pipeline_results["success"] = False
                LOGGER.error("Balance update failed, continuing with pipeline...")

            # Step 2: Update Deals Data
            LOGGER.info(f"{LOGGER_PREFIX} Phase 2: Updating deals data...")
            deals_start = datetime.now()
            deals_result = await DealsService.update_newest_deals_daily()
            deals_duration = (datetime.now() - deals_start).total_seconds()

            pipeline_results["steps"]["deals_update"] = {
                # "result": deals_result,
                "duration_seconds": deals_duration,
                "success": deals_result.get("success", False),
            }

            if not deals_result.get("success", False):
                pipeline_results["success"] = False
                LOGGER.error(f"{LOGGER_PREFIX} Deals update failed, continuing with pipeline...")

            # Step 3: Update Universe Top Monthly
            LOGGER.info(f"{LOGGER_PREFIX} Phase 3: Updating universe top monthly...")
            universe_start = datetime.now()
            universe_result = (
                await StocksUniverseService.update_newest_data_all_monthly()
            )
            universe_duration = (datetime.now() - universe_start).total_seconds()

            pipeline_results["steps"]["universe_update"] = {
                # "result": universe_result,
                "duration_seconds": universe_duration,
                "success": universe_result,
            }

            if not universe_result:
                pipeline_results["success"] = False
                LOGGER.error(f"{LOGGER_PREFIX} Universe update failed, continuing with pipeline...")

            # Step 4: Update Optimized Weights
            LOGGER.info(f"{LOGGER_PREFIX} Phase 4: Updating qOptimized weights...")
            weights_start = datetime.now()
            weights_result = await PortfoliosService.update_newest_data_all_daily()
            weights_duration = (datetime.now() - weights_start).total_seconds()

            pipeline_results["steps"]["weights_update"] = {
                # "result": weights_result,
                "duration_seconds": weights_duration,
                "success": weights_result,
            }

            if not weights_result:
                pipeline_results["success"] = False
                LOGGER.error(f"{LOGGER_PREFIX} Weights update failed, continuing with pipeline...")

            # Step 5: Send Portfolio Notifications
            LOGGER.info(f"{LOGGER_PREFIX} Phase 5: Sending portfolio notifications...")
            notification_start = datetime.now()
            notification_result = (
                await PortfolioNotificationService.send_daily_system_portfolio()
            )
            notification_duration = (
                datetime.now() - notification_start
            ).total_seconds()

            pipeline_results["steps"]["notification_send"] = {
                # "result": notification_result,
                "duration_seconds": notification_duration,
                "success": notification_result,
            }

            if not notification_result:
                pipeline_results["success"] = False
                LOGGER.error(f"{LOGGER_PREFIX} Notification sending failed")

            # Calculate total duration
            end_time = datetime.now()
            pipeline_results["end_time"] = end_time.isoformat()
            pipeline_results["total_duration"] = (end_time - start_time).total_seconds()

            # Send summary notification
            await cls.send_pipeline_summary(pipeline_results)

            LOGGER.info(
                f"{LOGGER_PREFIX} Daily pipeline completed in {pipeline_results['total_duration']:.2f} seconds"
            )
            return pipeline_results

        except Exception as e:
            LOGGER.error(f"{LOGGER_PREFIX} Fatal error in daily pipeline: {e}")
            await notify_error(
                "DAILY PIPELINE FATAL ERROR",
                f"Daily data pipeline failed with fatal error: {str(e)}",
            )

            pipeline_results["success"] = False
            pipeline_results["error"] = str(e)
            pipeline_results["total_duration"] = (
                datetime.now() - start_time
            ).total_seconds()

            return pipeline_results

    @classmethod
    async def send_pipeline_summary(cls, results: Dict[str, Any]) -> None:
        """Send pipeline summary notification"""
        try:
            success = results.get("success", False)
            total_duration = results.get("total_duration", 0)
            steps = results.get("steps", {})

            # Count successful steps
            successful_steps = sum(
                1 for step in steps.values() if step.get("success", False)
            )
            total_steps = len(steps)

            # Create summary message
            status_emoji = "" if success else ""
            title = f"{status_emoji} Daily Pipeline Summary"

            message_lines = [
                f"<b>{title}</b>",
                f"",
                f" <b>Overall Status:</b> {'SUCCESS' if success else 'FAILED'}",
                f" <b>Total Duration:</b> {total_duration:.2f} seconds",
                f" <b>Successful Steps:</b> {successful_steps}/{total_steps}",
                f"",
            ]

            # Add step details
            for step_name, step_data in steps.items():
                step_success = step_data.get("success", False)
                step_duration = step_data.get("duration_seconds", 0)
                step_emoji = "" if step_success else ""

                step_title = step_name.replace("_", " ").title()
                message_lines.append(
                    f"{step_emoji} <b>{step_title}:</b> {step_duration:.2f}s"
                )

                # Add specific results for each step
                result = step_data.get("result", {})
                if step_name == "balance_update" and result:
                    updated = result.get("updated_accounts", 0)
                    failed = result.get("failed_accounts", 0)
                    message_lines.append(
                        f"    Updated: {updated} accounts, Failed: {failed}"
                    )

                elif step_name == "deals_update" and result:
                    updated = result.get("updated_accounts", 0)
                    total_deals = result.get("total_deals", 0)
                    message_lines.append(
                        f"    Updated: {updated} accounts, Total deals: {total_deals}"
                    )

                elif step_name == "universe_update" and result:
                    updated = result.get("updated_symbols", 0)
                    message_lines.append(f"    Updated symbols: {updated}")

                elif step_name == "weights_update" and result:
                    updated = result.get("updated_weights", 0)
                    message_lines.append(f"    Updated weights: {updated}")

                elif step_name == "notification_send" and result:
                    sent = result.get("sent_notifications", 0)
                    message_lines.append(f"    Notifications sent: {sent}")

            message_lines.append("")
            message_lines.append(
                f" <b>Pipeline Time:</b> {results.get('start_time', '')}"
            )

            message = "\n".join(message_lines)

            if success:
                await notify_success("Daily Pipeline Completed", message)
            else:
                await notify_error("Daily Pipeline Issues", message)

        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def schedule_daily_run(cls) -> None:
        """Schedule the pipeline to run daily at 7PM"""
        LOGGER.info(f"{LOGGER_PREFIX} Starting daily pipeline scheduler...")

        LOGGER.info(f"{LOGGER_PREFIX} PRODUCTION MODE: Pipeline will run daily at {cls.PIPELINE_TIME}")

        while True:
            try:
                current_time = TimeUtils.get_current_vn_time()

                # Production mode: run daily at specified time
                target_time = current_time.replace(
                    hour=cls.PIPELINE_TIME.hour,
                    minute=cls.PIPELINE_TIME.minute,
                    second=0,
                    microsecond=0,
                )

                # If target time has passed today, schedule for tomorrow
                if current_time >= target_time:
                    target_time = target_time + timedelta(days=1)

                # Calculate seconds until next run
                time_until_run = (target_time - current_time).total_seconds()

                LOGGER.info(
                    f"{LOGGER_PREFIX} Next pipeline run scheduled for: {target_time.isoformat()}"
                )
                LOGGER.info(f"{LOGGER_PREFIX} Waiting {time_until_run:.0f} seconds...")

                # Break long waits into smaller chunks with health checks
                if time_until_run > 3600:  # If waiting more than 1 hour
                    await cls.wait_with_health_checks(time_until_run)
                else:
                    await asyncio.sleep(time_until_run)

                # Run the pipeline
                LOGGER.info(f"{LOGGER_PREFIX} Executing scheduled daily pipeline...")
                await cls.run_pipeline()

                # Sleep for a minute to avoid running multiple times
                await asyncio.sleep(60)

            except Exception as e:
                LOGGER.error(f"Error in pipeline scheduler: {e}")
                await notify_error(
                    "PIPELINE SCHEDULER ERROR",
                    f"Daily pipeline scheduler encountered an error: {str(e)}",
                )
                # Wait 5 minutes before retrying
                await asyncio.sleep(300)

    @classmethod
    async def wait_with_health_checks(cls, total_seconds: float) -> None:
        """Wait for a long duration with periodic health checks"""
        check_interval = 3600  # Check every hour
        elapsed = 0

        while elapsed < total_seconds:
            LOGGER.info(f"{LOGGER_PREFIX} Running health checks every {check_interval} seconds...")
            remaining = total_seconds - elapsed
            sleep_time = min(check_interval, remaining)

            elapsed += sleep_time

            # Health check every hour
            if elapsed % check_interval == 0 and remaining > 0:
                hours_remaining = remaining / 3600
                LOGGER.info(f"{LOGGER_PREFIX} Pipeline scheduler health check: {hours_remaining:.1f} hours remaining")

                if hours_remaining <= 2:  # Only notify when close to execution time
                    await notify_success(
                        "Pipeline Scheduler Health Check",
                        f"Pipeline will run in {hours_remaining:.1f} hours",
                    )

            await asyncio.sleep(sleep_time)

    @classmethod
    async def run_manual(cls) -> Dict[str, Any]:
        """Run the pipeline manually (for testing or emergency updates)"""
        LOGGER.info(f"{LOGGER_PREFIX} Running daily pipeline manually...")
        return await cls.run_pipeline()

================
File: backend/modules/portfolio/services/portfolio_deals_service.py
================
from backend.common.consts import SQLServerConsts, MessageConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.base.query_builder import TextSQL
from backend.modules.portfolio.entities import Deals
from backend.modules.portfolio.repositories import AccountsRepo, DealsRepo
from backend.modules.portfolio.utils.deals_utils import DealsUtils
from backend.modules.dnse.trading_session import TradingSession
from backend.utils.logger import LOGGER
from backend.utils.time_utils import TimeUtils


LOGGER_PREFIX = "[DealsService]"


class DealsService:
    repo = DealsRepo
    accounts_repo = AccountsRepo

    @classmethod
    async def update_newest_deals_daily(cls) -> bool:
        """Update deals for all accounts"""
        try:
            existing_accounts = await cls.accounts_repo.get_all()
            existing_accounts = [
                account
                for account in existing_accounts
                if account.get("accountType") == "0449"
            ]

            if len(existing_accounts) == 0:
                LOGGER.warning(f"{LOGGER_PREFIX} No account found")
                return False


            data = []
            for account in existing_accounts:
                custody_code = account.get("custodyCode")
                password = account.get("password")
                broker_account_id = account.get("brokerAccountId")

                # Get current balance from DNSE API
                async with TradingSession(account=custody_code) as session:
                    if not await session.authenticate(password=password):
                        LOGGER.error(f"{LOGGER_PREFIX} Authentication failed for account {custody_code}")
                        return False

                    async with session.users_client() as users_client:
                        deals_raw = await users_client.get_account_deals(account_no=broker_account_id)

                        if not deals_raw:
                            continue

                        if len(deals_raw["deals"]) == 0:
                            continue

                        # Prepare deals data
                        current_time = TimeUtils.get_current_vn_time()
                        today_str = current_time.strftime(SQLServerConsts.DATE_FORMAT)

                        deals_list = [
                            DealsUtils.extract_deal_data(raw_data=deal, date=today_str)
                            for deal in deals_raw["deals"]
                        ]
                        if not deals_list:
                            continue

                        data.extend(deals_list)

            with cls.repo.session_scope() as session:
                if data:
                    # Upsert deals
                    await cls.repo.upsert(
                        temp_table=f"#{cls.repo.query_builder.table}",
                        records=data,
                        identity_columns=[Deals.brokerAccountId.name, Deals.date.name],
                        text_clauses={
                            "__updatedAt__": TextSQL(
                                SQLServerConsts.GMT_7_NOW_VARCHAR
                            )
                        },
                    )
                    session.commit()
                    LOGGER.info(f"{LOGGER_PREFIX} Deals updated successfully for {len(data)} accounts")
                else:
                    LOGGER.warning(f"{LOGGER_PREFIX} No deals data to update")

            return {"success": True}



        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

================
File: backend/modules/portfolio/services/portfolio_notification_service.py
================
from backend.common.consts import SQLServerConsts, MessageConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.portfolio.services.data_providers import PortfolioDataProvider
from backend.modules.portfolio.services.portfolio_analysis_service import (
    PortfolioAnalysisService,
)
from backend.modules.portfolio.infrastructure import (
    TradingCalendarService,
    PortfolioReportGenerator,
)
from backend.modules.notifications.service import notification_service
from backend.modules.notifications.service import notify_error
from backend.modules.notifications.telegram import MessageType
from backend.utils.logger import LOGGER


class PortfolioNotificationService:
    portfolio_data_provider = PortfolioDataProvider
    trading_calendar = TradingCalendarService

    @classmethod
    async def send_daily_system_portfolio(cls):
        try:
            last_trading_date, next_trading_date = (
                cls.trading_calendar.get_last_next_trading_dates()
            )
            if not last_trading_date or not next_trading_date:
                LOGGER.warning(
                    "No trading dates found. Cannot proceed with portfolio analysis."
                )
                raise BaseExceptionResponse(
                    http_code=400,
                    status_code=400,
                    message=MessageConsts.NOT_FOUND,
                    errors=None,
                )

            next_trading_date_str = next_trading_date.strftime(
                SQLServerConsts.DATE_FORMAT
            )
            last_trading_date_str = last_trading_date.strftime(
                SQLServerConsts.DATE_FORMAT
            )

            # Initialize notification service
            if not notification_service.is_ready():
                await notification_service.initialize()

            if not notification_service.is_ready():
                LOGGER.error("Notification service not available")
                raise BaseExceptionResponse(
                    http_code=400,
                    status_code=400,
                    message=MessageConsts.NOT_FOUND,
                    errors=None,
                )

            # Get portfolio data
            portfolio_data = (
                await cls.portfolio_data_provider.get_portfolio_weights_by_id(
                    last_trading_date_str, next_trading_date_str
                )
            )

            if not portfolio_data:
                raise BaseExceptionResponse(
                    http_code=404,
                    status_code=404,
                    message=MessageConsts.NOT_FOUND,
                    errors=None,
                )

            # Notify about daily portfolio weights
            await notification_service.notify_daily_portfolio(
                portfolio_data=portfolio_data
            )
            return {"message": "Daily portfolio notification sent successfully"}

        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def send_portfolio_analysis_report(
        cls,
        broker_account_id: str,
        strategy_type: str = "MarketNeutral",
        include_trade_plan: bool = True,
        message_type: MessageType = MessageType.INFO,
    ):
        try:
            LOGGER.info(
                f"Generating portfolio analysis report for account {broker_account_id}"
            )

            # Generate portfolio analysis
            analysis_result = await PortfolioAnalysisService.analyze_portfolio(
                broker_account_id=broker_account_id, strategy_type=strategy_type
            )

            if not analysis_result:
                error_msg = (
                    f"Khng th phn tch portfolio cho ti khon {broker_account_id}"
                )
                LOGGER.warning(error_msg)
                await notify_error("PORTFOLIO ANALYSIS ERROR", error_msg)
                return False

            # Generate text report
            report_text = PortfolioReportGenerator.generate_telegram_report(
                analysis_result=analysis_result, include_trade_plan=include_trade_plan
            )

            if not notification_service.is_ready():
                await notification_service.initialize()

            if not notification_service.is_ready():
                return False

            # Send report via Telegram
            success = await notification_service.telegram.send_message(
                text=report_text,
                message_type=message_type,
                parse_mode="HTML",
                disable_notification=False,
            )

            if success:
                LOGGER.info(
                    f"Portfolio analysis report sent successfully for account {broker_account_id}"
                )
                return {"message": "Portfolio analysis report sent successfully"}
            else:
                LOGGER.error(
                    f"Failed to send portfolio analysis report for account {broker_account_id}"
                )
                return {"message": "Failed to send portfolio analysis report"}

        except Exception as e:
            LOGGER.error(
                f"Error sending portfolio analysis report for account {broker_account_id}: {e}"
            )
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

================
File: backend/modules/portfolio/services/portfolio_scheduler_service.py
================
from datetime import datetime
import asyncio
import os
import schedule

from backend.modules.portfolio.services import PortfolioNotificationService
from backend.utils.logger import LOGGER

# Scheduler Service
class PortfolioSchedulerService:

    def schedule_daily_notifications(self):
        """Schedule daily notifications"""
        notification_time = "07:30" if os.getenv("TEST") == "1" else "00:00"
        schedule.every().day.at(notification_time).do(
            lambda: asyncio.create_task(
                PortfolioNotificationService.send_daily_portfolio_notification()
            )
        )
        LOGGER.info(f"Daily portfolio notifications scheduled for {notification_time}")

    async def run_scheduler(self):
        """Run the notification scheduler"""
        self.schedule_daily_notifications()
        LOGGER.info("Portfolio notification scheduler started")

        while True:
            schedule.run_pending()
            LOGGER.info(
                f"Scheduler running at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )
            await asyncio.sleep(60)  # Check every minute

================
File: backend/modules/portfolio/services/portfolio_service.py
================
import datetime
import warnings
import pandas as pd
from typing import List, Dict
from pydantic import ValidationError
from dateutil.relativedelta import relativedelta

from backend.common.consts import SQLServerConsts, MessageConsts
from backend.common.responses.exceptions import BaseExceptionResponse
from backend.modules.auth.types.auth import JwtPayload
from backend.modules.base.query_builder import TextSQL
from backend.modules.base_daily import BaseDailyService
from backend.modules.portfolio.entities import (
    StocksUniverse,
    Portfolios,
    PortfolioMetadata,
)
from backend.modules.portfolio.repositories import (
    PortfoliosRepo,
    PortfolioMetadataRepo,
    StocksUniverseRepo,
)
from backend.modules.portfolio.core import PortfolioOptimizer
from backend.modules.portfolio.services.data_providers import PriceDataProvider
from backend.modules.portfolio.services.processors import (
    PortfolioPnLCalculator,
    PortfolioRiskCalculator,
)
from backend.modules.portfolio.utils.portfolio_utils import PortfolioUtils
from backend.modules.portfolio.infrastructure import TradingCalendarService
from backend.utils.logger import LOGGER


LOGGER_PREFIX = "[PortfoliosService]"


class PortfoliosService(BaseDailyService):
    repo = PortfoliosRepo
    metadata_repo = PortfolioMetadataRepo
    trading_calendar = TradingCalendarService
    portfolio_optimizer = PortfolioOptimizer
    portfolio_pnl_calculator = PortfolioPnLCalculator

    @classmethod
    async def update_data(cls, from_date: str) -> pd.DataFrame:
        days_back = 21

        df_stock_pivoted = await PriceDataProvider(prefix="STOCK").get_market_data(
            from_date=from_date
        )
        df_stock_pivoted = df_stock_pivoted.dropna(axis=1, how="all")

        T = df_stock_pivoted.shape[0]
        tmp_year = pd.to_datetime(from_date).year
        tmp_month = pd.to_datetime(from_date).month
        assets = []
        portfolio_weights = []
        portfolio_id = PortfolioUtils.generate_general_portfolio_id(date=from_date)

        with cls.metadata_repo.session_scope() as metadata_session:
            for i in range(days_back - 1, T):
                window = df_stock_pivoted.iloc[i - days_back + 1 : i + 1]
                end_of_period = window.index[-1]

                # Check if need to update the assets for the current month
                if (
                    pd.to_datetime(end_of_period).year != tmp_year
                    or pd.to_datetime(end_of_period).month != tmp_month
                    or not assets
                ):
                    tmp_year = pd.to_datetime(end_of_period).year
                    tmp_month = pd.to_datetime(end_of_period).month
                    portfolio_id = PortfolioUtils.generate_general_portfolio_id(
                        date=end_of_period
                    )

                    records = await StocksUniverseRepo.get_asset_by_year_month(
                        year=tmp_year, month=tmp_month
                    )
                    assets = [record[StocksUniverse.symbol.name] for record in records]

                # Filter columns (tickers) that are in the current month's universe
                available_assets = [
                    asset for asset in assets if asset in window.columns
                ]
                df_portfolio = window[available_assets].copy()
                x_CEMV, x_CEMV_neutralized, _, _ = cls.portfolio_optimizer.optimize(
                    df_portfolio=df_portfolio
                )

                for j in range(len(x_CEMV)):
                    record = {
                        Portfolios.date.name: end_of_period,
                        Portfolios.symbol.name: df_portfolio.columns[j],
                        Portfolios.portfolioId.name: portfolio_id,
                        Portfolios.marketPrice.name: df_portfolio.iloc[-1, j],
                        Portfolios.initialWeight.name: x_CEMV[j],
                        Portfolios.neutralizedWeight.name: max(
                            x_CEMV_neutralized[j], 0.0
                        ),
                        Portfolios.limitedWeight.name: float(0),
                        Portfolios.neutralizedLimitedWeight.name: float(0),
                    }
                    portfolio_weights.append(record)

                portfolio_year = pd.to_datetime(end_of_period).year
                portfolio_month = pd.to_datetime(end_of_period).month
                portfolio_name = f"SysPrtfUnv20S{portfolio_year}{portfolio_month}"
                temp_table = f"#{cls.metadata_repo.query_builder.table}"
                await cls.metadata_repo.upsert(
                    temp_table=temp_table,
                    records=[
                        {
                            PortfolioMetadata.portfolioId.name: portfolio_id,
                            PortfolioMetadata.portfolioName.name: portfolio_name,
                            PortfolioMetadata.portfolioDesc.name: "UniverseTop20Portfolio",
                        }
                    ],
                    identity_columns=[PortfolioMetadata.portfolioId.name],
                    text_clauses={
                        "__updatedAt__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)
                    },
                )
            metadata_session.commit()

        portfolio_weights_df = pd.DataFrame(portfolio_weights)
        return portfolio_weights_df

    @classmethod
    async def create_portfolio(
        cls,
        user_id: int,
        portfolio_name: str,
        portfolio_desc: str,
        symbols: List[str],
        max_positions: int = 30,
        days_back: int = 21,
    ) -> str:
        try:
            if len(symbols) > max_positions:
                raise BaseExceptionResponse(
                    http_code=400,
                    status_code=400,
                    message=MessageConsts.BAD_REQUEST,
                    errors=f"Maximum {max_positions} symbols allowed",
                )
            if len(symbols) < 2:
                raise BaseExceptionResponse(
                    http_code=400,
                    status_code=400,
                    message=MessageConsts.BAD_REQUEST,
                    errors="At least 2 symbols are required to create a portfolio",
                )

            df_stock_pivoted = await PriceDataProvider(prefix="STOCK").get_market_data()

            # Validate symbols exist
            await cls.validate_symbols(
                symbols=symbols, df_stock_pivoted=df_stock_pivoted
            )

            df_portfolio = df_stock_pivoted[symbols].copy()
            df_stock_pivoted = df_stock_pivoted.dropna(axis=1, how="all")
            # filter data with rows 21 days from the latest date
            if df_portfolio.empty:
                raise BaseExceptionResponse(
                    http_code=404,
                    status_code=404,
                    message=MessageConsts.NOT_FOUND,
                    errors="No stock data available",
                )
            latest_date = df_portfolio.index[-1]
            start_date = pd.to_datetime(latest_date) - pd.Timedelta(days=days_back)
            last_date = pd.to_datetime(latest_date)
            df_portfolio = df_portfolio.loc[
                start_date.strftime(SQLServerConsts.DATE_FORMAT) : last_date.strftime(
                    SQLServerConsts.DATE_FORMAT
                )
            ]

            x_CEMV, x_CEMV_neutralized, x_CEMV_limited, x_CEMV_neutralized_limit = (
                cls.portfolio_optimizer.optimize(df_portfolio=df_portfolio)
            )
            portfolio_id = PortfolioUtils.generate_custom_portfolio_id(
                user_id=user_id, portfolio_name=portfolio_name
            )

            # Get portfolio weights
            portfolio_weights = []
            for j in range(len(x_CEMV)):
                record = {
                    Portfolios.date.name: latest_date,
                    Portfolios.symbol.name: df_portfolio.columns[j],
                    Portfolios.portfolioId.name: portfolio_id,
                    Portfolios.marketPrice.name: df_portfolio.iloc[-1, j],
                    Portfolios.initialWeight.name: x_CEMV[j],
                    Portfolios.neutralizedWeight.name: max(x_CEMV_neutralized[j], 0.0),
                    Portfolios.limitedWeight.name: float(0),
                    Portfolios.neutralizedLimitedWeight.name: float(0),
                }
                portfolio_weights.append(record)
            portfolio_weights_df = pd.DataFrame(portfolio_weights)

            # Save portfolio weights to db
            with cls.repo.session_scope() as session:
                await cls.metadata_repo.insert(
                    record={
                        PortfolioMetadata.portfolioId.name: portfolio_id,
                        PortfolioMetadata.userId.name: user_id,
                        PortfolioMetadata.portfolioName.name: portfolio_name,
                        PortfolioMetadata.portfolioType.name: "CUSTOM",
                        PortfolioMetadata.portfolioDesc.name: portfolio_desc,
                        PortfolioMetadata.algorithm.name: "CMV",
                    },
                    returning=False,
                )
                temp_table = f"#{cls.repo.query_builder.table}"
                await cls.repo.upsert(
                    temp_table=temp_table,
                    records=portfolio_weights_df,
                    identity_columns=["date", "symbol"],
                    text_clauses={
                        "__updatedAt__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)
                    },
                )
                session.commit()

            return {"message": f"Portfolio {portfolio_id} created successfully"}
        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def update_portfolio(
        cls, portfolio_id: str, symbols: List[str], user: JwtPayload
    ) -> None:
        try:
            # First validate that the portfolio exists
            with cls.repo.session_scope() as session:
                records = await cls.metadata_repo.get_by_portfolio_id(
                    portfolio_id=portfolio_id
                )
                if len(records) == 0:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.NOT_FOUND,
                        errors=f"Portfolio with ID {portfolio_id} not found",
                    )

                existing_portfolio = records[0]
                await cls.repo.delete_by_portfolio_id(portfolio_id=portfolio_id)

                # Get price data for optimization
                df_stock_pivoted = await PriceDataProvider(
                    prefix="STOCK"
                ).get_market_data()
                await cls.validate_symbols(
                    symbols=symbols, df_stock_pivoted=df_stock_pivoted
                )
                df_portfolio = df_stock_pivoted[symbols].copy()
                df_stock_pivoted = df_stock_pivoted.dropna(axis=1, how="all")
                # filter data with rows 21 days from the latest date
                if df_portfolio.empty:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.NOT_FOUND,
                        errors="No stock data available",
                    )
                latest_date = df_portfolio.index[-1]
                days_back = 21  # temporary
                start_date = pd.to_datetime(latest_date) - pd.Timedelta(days=days_back)
                last_date = pd.to_datetime(latest_date)
                df_portfolio = df_portfolio.loc[
                    start_date.strftime(
                        SQLServerConsts.DATE_FORMAT
                    ) : last_date.strftime(SQLServerConsts.DATE_FORMAT)
                ]

                x_CEMV, x_CEMV_neutralized, x_CEMV_limited, x_CEMV_neutralized_limit = (
                    cls.portfolio_optimizer.optimize(df_portfolio=df_portfolio)
                )
                portfolio_id = existing_portfolio[PortfolioMetadata.portfolioId.name]

                # Get portfolio weights
                portfolio_weights = []
                for j in range(len(x_CEMV)):
                    record = {
                        Portfolios.date.name: latest_date,
                        Portfolios.symbol.name: df_portfolio.columns[j],
                        Portfolios.portfolioId.name: portfolio_id,
                        Portfolios.marketPrice.name: df_portfolio.iloc[-1, j],
                        Portfolios.initialWeight.name: x_CEMV[j],
                        Portfolios.neutralizedWeight.name: max(
                            x_CEMV_neutralized[j], 0.0
                        ),
                        Portfolios.limitedWeight.name: float(0),
                        Portfolios.neutralizedLimitedWeight.name: float(0),
                    }
                    portfolio_weights.append(record)

                if len(portfolio_weights) == 0:
                    raise BaseExceptionResponse(
                        http_code=400,
                        status_code=400,
                        message=MessageConsts.BAD_REQUEST,
                        errors="No valid portfolio weights calculated for the given symbols",
                    )

                # Insert new records using insert_many
                await cls.repo.insert_many(portfolio_weights, returning=False)

                # Update portfolio metadata
                await cls.metadata_repo.update(
                    record={PortfolioMetadata.portfolioId.name: portfolio_id},
                    identity_columns=[PortfolioMetadata.portfolioId.name],
                    returning=False,
                    text_clauses={
                        "__updatedAt__": TextSQL(SQLServerConsts.GMT_7_NOW_VARCHAR)
                    },
                )
                session.commit()

                return {"message": f"Portfolio {portfolio_id} updated successfully"}

        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=f"Failed to update portfolio: {str(e)}",
            )

    @classmethod
    async def get_portfolios_by_id(
        cls, portfolio_id: str, user: JwtPayload
    ) -> pd.DataFrame:
        try:
            with cls.repo.session_scope() as session:
                portfolio = await cls.repo.get_by_portfolio_id(
                    portfolio_id=portfolio_id
                )
                if not portfolio:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.NOT_FOUND,
                        errors=f"Portfolio {portfolio_id} not found",
                    )

                portfolios_metadata = await cls.metadata_repo.get_by_portfolio_id(
                    portfolio_id=portfolio_id
                )
                if not portfolios_metadata:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.NOT_FOUND,
                        errors=f"No portfolios found for user {user.userId}",
                    )
                session.commit()

            user_id = portfolios_metadata[0][PortfolioMetadata.userId.name]
            if user_id != user.userId:
                raise BaseExceptionResponse(
                    http_code=403,
                    status_code=403,
                    message=MessageConsts.FORBIDDEN,
                    errors=f"User {user.userId} is not allowed to access portfolio {portfolio_id}",
                )

            return {"records": portfolio}
        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def get_portfolios_by_user_id(cls, user: JwtPayload) -> pd.DataFrame:
        try:
            with cls.repo.session_scope() as session:
                portfolios_metadata = await cls.metadata_repo.get_by_user_id(
                    user_id=user.userId
                )
                if not portfolios_metadata:
                    LOGGER.info(
                        f"{LOGGER_PREFIX} No portfolio metadata found for user {user.userId}"
                    )
                    return {"portfolios": []}
                unique_portfolio_ids = set(
                    metadata[PortfolioMetadata.portfolioId.name]
                    for metadata in portfolios_metadata
                )

                condition_str = "'" + "','".join(unique_portfolio_ids) + "'"
                sql_query = f"""
                    SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                    WITH UncommittedData AS (
                        SELECT *
                        FROM [BotPortfolio].[portfolios]
                        WHERE [portfolioId] IN ({condition_str})
                    )

                    SELECT *
                    FROM UncommittedData
                """

                with warnings.catch_warnings():
                    warnings.simplefilter("ignore", UserWarning)
                    conn = session.connection().connection
                    df_portfolios = pd.read_sql_query(sql_query, conn)

                session.commit()

            # convert to list of portfolios
            portfolios_records = df_portfolios.to_dict(orient="records")
            if not portfolios_records:
                return {"portfolios": []}

            portfolios = []
            for portfolio_id in unique_portfolio_ids:
                portfolio_metadata = next(
                    (
                        metadata
                        for metadata in portfolios_metadata
                        if metadata[Portfolios.portfolioId.name] == portfolio_id
                    ),
                    None,
                )
                records = [
                    port
                    for port in portfolios_records
                    if port[Portfolios.portfolioId.name] == portfolio_id
                ]
                if portfolio_metadata and len(records) > 0:
                    portfolios.append(
                        {
                            "portfolioId": portfolio_id,
                            "metadata": portfolio_metadata,
                            "records": records,
                        }
                    )
            return {"portfolios": portfolios}

        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def get_system_portfolios(cls) -> Dict:
        try:
            with cls.repo.session_scope() as session:
                system_portfolios_metadata = await cls.metadata_repo.get_by_condition(
                    conditions={PortfolioMetadata.portfolioType.name: "SYSTEM"}
                )
                if not system_portfolios_metadata:
                    return {"portfolios": []}

                unique_portfolio_ids = set(
                    metadata[PortfolioMetadata.portfolioId.name]
                    for metadata in system_portfolios_metadata
                )

                condition_str = "'" + "','".join(unique_portfolio_ids) + "'"
                sql_query = f"""
                    SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                    WITH UncommittedData AS (
                        SELECT *
                        FROM [BotPortfolio].[portfolios]
                        WHERE [portfolioId] IN ({condition_str})
                    )

                    SELECT *
                    FROM UncommittedData
                """

                with warnings.catch_warnings():
                    warnings.simplefilter("ignore", UserWarning)
                    conn = session.connection().connection
                    df_portfolios = pd.read_sql_query(sql_query, conn)

                session.commit()

            # convert to list of portfolios
            portfolios_records = df_portfolios.to_dict(orient="records")
            if not portfolios_records:
                return {"portfolios": []}

            portfolios = []
            for portfolio_id in unique_portfolio_ids:
                portfolio_metadata = next(
                    (
                        meta
                        for meta in system_portfolios_metadata
                        if meta[PortfolioMetadata.portfolioId.name] == portfolio_id
                    ),
                    None,
                )

                latest_date = max(
                    record[Portfolios.date.name] for record in portfolios_records
                )
                records = [
                    record
                    for record in portfolios_records
                    if (
                        record[Portfolios.portfolioId.name] == portfolio_id
                        and record[Portfolios.date.name] == latest_date
                    )
                ]
                if portfolio_metadata and len(records) > 0:
                    portfolios.append(
                        {
                            "portfolioId": portfolio_id,
                            "metadata": portfolio_metadata,
                            "records": records,
                        }
                    )
            return {"portfolios": portfolios}

        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def delete_portfolio(cls, portfolio_id: str) -> None:
        try:
            with cls.repo.session_scope() as session:
                existing_portfolio = await cls.metadata_repo.get_by_portfolio_id(
                    portfolio_id=portfolio_id
                )
                if not existing_portfolio:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.NOT_FOUND,
                        errors=f"No portfolio found with id {portfolio_id}",
                    )

                await cls.repo.delete_by_portfolio_id(portfolio_id=portfolio_id)
                await cls.metadata_repo.delete_by_portfolio_id(
                    portfolio_id=portfolio_id
                )
                session.commit()
            return {"message": f"Portfolio {portfolio_id} deleted successfully"}
        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def get_available_symbols(cls) -> pd.DataFrame:
        try:
            df_stock_pivoted = await PriceDataProvider(prefix="STOCK").get_market_data()
            # df_stock_pivoted = df_stock_pivoted.dropna(axis=1, how="all")
            if df_stock_pivoted.empty:
                raise BaseExceptionResponse(
                    http_code=404,
                    status_code=404,
                    message=MessageConsts.NOT_FOUND,
                    errors="No stock data available",
                )

            available_symbols = df_stock_pivoted.columns.tolist()

            return {"records": available_symbols}
        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

    @classmethod
    async def validate_symbols(
        cls, symbols: List[str], df_stock_pivoted: pd.DataFrame
    ) -> None:
        """Validate that all symbols exist in the database."""
        if df_stock_pivoted.empty:
            raise BaseExceptionResponse(
                http_code=404,
                status_code=404,
                message=MessageConsts.NOT_FOUND,
                errors="No stock data available",
            )

        available_symbols = df_stock_pivoted.columns.tolist()

        try:
            for symbol in symbols:
                if symbol not in available_symbols:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.BAD_REQUEST,
                        errors=f"Symbol {symbol} not found in available stock data",
                    )
        except ValidationError as e:
            raise BaseExceptionResponse(
                http_code=400,
                status_code=400,
                message=MessageConsts.BAD_REQUEST,
                errors=str(e),
            )

    @classmethod
    async def get_portfolio_pnl(
        cls, portfolio_id: str, strategy: str = "LongOnly"
    ) -> Dict:
        try:
            with cls.repo.session_scope() as session:
                portfolio_metadata = await cls.metadata_repo.get_by_portfolio_id(
                    portfolio_id=portfolio_id
                )
                if not portfolio_metadata:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.NOT_FOUND,
                        errors=f"No portfolio metadata found with id {portfolio_id}",
                    )

                days_back = 21
                last_trading_date, _ = (
                    cls.trading_calendar.get_last_next_trading_dates()
                )
                from_date = last_trading_date - relativedelta(months=15)
                from_date_str = from_date.strftime(SQLServerConsts.DATE_FORMAT)

                portfolios = await cls.repo.get_by_condition(
                    conditions={Portfolios.portfolioId.name: portfolio_id}
                )
                if not portfolios:
                    raise BaseExceptionResponse(
                        http_code=404,
                        status_code=404,
                        message=MessageConsts.NOT_FOUND,
                        errors=f"No portfolio found with id {portfolio_id}",
                    )

                # Find the actual latest date from all records
                latest_date = max(record[Portfolios.date.name] for record in portfolios)
                # Filter to get only records with the latest date
                portfolios = [
                    record
                    for record in portfolios
                    if record[Portfolios.date.name] == latest_date
                ]

                session.commit()

                symbols = list(
                    set([record[Portfolios.symbol.name] for record in portfolios])
                )
                df_stock_pivoted = await PriceDataProvider(
                    prefix="STOCK"
                ).get_market_data(from_date=from_date_str)
                df_index = await PriceDataProvider(prefix="INDEX").get_market_data(
                    from_date=from_date_str
                )
                df_portfolio = df_stock_pivoted[symbols].copy()

                portfolio_weights_df = []
                T = df_portfolio.shape[0]
                n_assets = df_portfolio.shape[1]
                for i in range(days_back - 1, T):
                    window = df_portfolio.iloc[i - days_back + 1 : i + 1, :]
                    end_of_period = window.index[-1]
                    prices = window.iloc[-1].values
                    x_CEMV, x_CEMV_neutralized, _, _ = cls.portfolio_optimizer.optimize(
                        df_portfolio=window
                    )
                    weights = x_CEMV if strategy == "LongOnly" else x_CEMV_neutralized

                    for j in range(n_assets):
                        record = {
                            "date": end_of_period,
                            "symbol": symbols[j],
                            "weight": weights[j],
                            "price": prices[j],
                        }
                        portfolio_weights_df.append(record)

                portfolio_weights_df = pd.DataFrame(portfolio_weights_df)
                portfolio_pnl_df = cls.portfolio_pnl_calculator.process_portfolio_pnl(
                    portfolio_weights_df=portfolio_weights_df
                )
                index_pnl_df = cls.portfolio_pnl_calculator.process_index_pnl(
                    df_index=df_index
                )

                # Check and synchronize dates between portfolio and index PnL
                # Ensure both dataframes have exactly the same 252 trading days (latest dates)
                portfolio_dates = set(portfolio_pnl_df["date"].tolist())
                index_dates = set(index_pnl_df["date"].tolist())

                # Get all available dates and sort them
                all_dates = sorted(list(portfolio_dates.union(index_dates)))

                # Take only the latest 252 trading days
                TRADING_DAYS = 252
                if len(all_dates) > TRADING_DAYS:
                    latest_dates = all_dates[-TRADING_DAYS:]
                else:
                    latest_dates = all_dates

                # Create complete date range and forward fill missing values
                if latest_dates:
                    # Convert date columns to datetime for proper sorting
                    portfolio_pnl_df["date"] = pd.to_datetime(portfolio_pnl_df["date"])
                    index_pnl_df["date"] = pd.to_datetime(index_pnl_df["date"])

                    # Create complete date range DataFrame with only the latest 252 dates
                    complete_dates_df = pd.DataFrame(
                        {"date": pd.to_datetime(latest_dates)}
                    )

                    # Merge and forward fill portfolio data
                    portfolio_complete = complete_dates_df.merge(
                        portfolio_pnl_df, on="date", how="left"
                    )
                    # Forward fill missing pnl_pct values
                    portfolio_complete["pnl_pct"] = portfolio_complete["pnl_pct"].ffill()
                    portfolio_complete["pnl_pct"] = portfolio_complete["pnl_pct"].fillna(0)
                    portfolio_complete["pnl"] = portfolio_complete["pnl"].ffill()
                    portfolio_complete["pnl"] = portfolio_complete["pnl"].fillna(0)
                    portfolio_complete["daily_profit_pct"] = portfolio_complete["daily_profit_pct"].ffill()
                    portfolio_complete["daily_profit_pct"] = portfolio_complete["daily_profit_pct"].fillna(0)
                    portfolio_complete["drawdown_pct"] = portfolio_complete["drawdown_pct"].ffill()
                    portfolio_complete["drawdown_pct"] = portfolio_complete["drawdown_pct"].fillna(0)

                    # Merge and forward fill index data
                    index_complete = complete_dates_df.merge(
                        index_pnl_df, on="date", how="left"
                    )
                    # Forward fill missing pnl_pct values
                    index_complete["pnl_pct"] = index_complete["pnl_pct"].ffill()
                    index_complete["pnl_pct"] = index_complete["pnl_pct"].fillna(0)
                    index_complete["pnl"] = index_complete["pnl"].ffill()
                    index_complete["pnl"] = index_complete["pnl"].fillna(0)
                    index_complete["daily_profit_pct"] = index_complete["daily_profit_pct"].ffill()
                    index_complete["daily_profit_pct"] = index_complete["daily_profit_pct"].fillna(0)
                    index_complete["drawdown_pct"] = index_complete["drawdown_pct"].ffill()
                    index_complete["drawdown_pct"] = index_complete["drawdown_pct"].fillna(0)

                    # Convert back to string format for response
                    portfolio_complete["date"] = portfolio_complete["date"].dt.strftime(
                        SQLServerConsts.DATE_FORMAT
                    )
                    index_complete["date"] = index_complete["date"].dt.strftime(
                        SQLServerConsts.DATE_FORMAT
                    )

                    portfolio_pnl_df = portfolio_complete
                    index_pnl_df = index_complete

                else:
                    # If no dates available, create empty dataframes with proper columns
                    portfolio_pnl_df = pd.DataFrame(columns=["date", "pnl_pct", "pnl", "daily_profit_pct", "drawdown_pct"])
                    index_pnl_df = pd.DataFrame(columns=["date", "pnl_pct", "pnl", "daily_profit_pct", "drawdown_pct"])
                    latest_dates = []

                portfolio_risk_metrics = PortfolioRiskCalculator(
                    portfolio_pnl_df
                ).calculate_all_metrics()
                index_risk_metrics = PortfolioRiskCalculator(
                    index_pnl_df
                ).calculate_all_metrics()

                # Format response for easy frontend chart plotting
                pnl_data = {
                    "metadata": {
                        "portfolio_id": portfolio_id,
                        "strategy": strategy,
                        "from_date": from_date_str,
                        "to_date": last_trading_date.strftime(
                            SQLServerConsts.DATE_FORMAT
                        ),
                        "total_portfolio_days": len(portfolio_pnl_df),
                        "total_index_days": len(index_pnl_df),
                        "total_dates": len(latest_dates),
                        "trading_days_limit": TRADING_DAYS,
                        "symbols": symbols,
                    },
                    "risk_metrics": {
                        "portfolio": portfolio_risk_metrics,
                        "vnindex": index_risk_metrics,
                    },
                    "portfolio": portfolio_pnl_df[["date", "pnl_pct"]].to_dict(
                        "records"
                    ),
                    "vnindex": index_pnl_df[["date", "pnl_pct"]].to_dict("records"),
                }

                return pnl_data
        except Exception as e:
            raise BaseExceptionResponse(
                http_code=500,
                status_code=500,
                message=MessageConsts.INTERNAL_SERVER_ERROR,
                errors=str(e),
            )

================
File: backend/modules/portfolio/services/portfolio_universe_service.py
================
import time
import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings(
    "ignore",
    message=".*Downcasting behavior in `replace` is deprecated.*",
    category=FutureWarning,
)
from scipy.stats import rankdata

from backend.common.consts import SQLServerConsts
from backend.modules.base_monthly import BaseMonthlyService
from backend.modules.portfolio.repositories import StocksUniverseRepo
from backend.db.sessions import mart_session_scope, lake_session_scope
from backend.utils.data_utils import DataUtils
from backend.utils.logger import LOGGER


LOGGER_PREFIX = "[StocksUniverseService]"
UNIVERSE_SIZE = 20


class StocksUniverseService(BaseMonthlyService):
    repo = StocksUniverseRepo

    @classmethod
    async def update_data(cls, from_date):
        start_time = time.time()
        with lake_session_scope() as lake_session:
            sql_query_0 = """
                SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                WITH UncommittedData AS (
                    SELECT
                        [ticker]
                        ,[exchangeCode]
                    FROM [LakeEod].[appFiinxCommon].[stocks]
                )
                SELECT * FROM UncommittedData
                ORDER BY [ticker];
            """
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            conn = lake_session.connection().connection
            df_stock_0 = pd.read_sql_query(sql_query_0, conn)
        with mart_session_scope() as mart_session:
            sql_query_1 = f"""
                SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                WITH UncommittedData AS (
                SELECT ticker, date, closePrice,  referencePriceAdjusted, openPriceAdjusted, highPriceAdjusted, lowPriceAdjusted, closePriceAdjusted, averagePriceAdjusted, beta2Y, beta6M, 
                totalMatchValue, totalMatchVolume, totalValue, totalVolume, adjustedRatioCumProd,
                ROW_NUMBER() OVER (PARTITION BY [ticker], [date] ORDER BY __updatedAt__ DESC) AS rn
                FROM [priceVolume].[priceVolume]
                )

                SELECT * FROM UncommittedData
                where rn = 1 and [date] >= '{from_date}'
                ORDER BY [date], ticker;
            """

            sql_query_2 = f"""
                SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                WITH UncommittedData AS (
                SELECT ticker, date, 
                f0141 as cap,
                f0114 as roe,
                f0127 as eps,
                f0143 as pb,
                f0145 as pe,
                f0069 as grossProfitQoQ,
                f0073 as netIncomeQoQ,

                ROW_NUMBER() OVER (PARTITION BY [ticker], [date] ORDER BY __updatedAt__ DESC) AS rn

                FROM [fiin].[fiinFinancialRatio]
                )

                SELECT * FROM UncommittedData
                where rn = 1 and [date] >= '{from_date}'
                ORDER BY [date], ticker;
            """

            sql_query_3 = f"""
                SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

                WITH UncommittedData AS (
                SELECT ticker, sectorL2,
                ROW_NUMBER() OVER (PARTITION BY [ticker] ORDER BY [__updatedAt__] DESC) AS rn

                FROM [company].[organization]
                )

                SELECT * FROM UncommittedData
                where rn = 1
                ORDER BY ticker;
            """
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", UserWarning)
                conn = mart_session.connection().connection
                df_stock_1 = pd.read_sql_query(sql_query_1, conn)
                df_stock_2 = pd.read_sql_query(sql_query_2, conn)
                df_sector = pd.read_sql_query(sql_query_3, conn)

                df_stock_1 = df_stock_1.drop(columns=["rn"], errors="ignore")
                df_stock_2 = df_stock_2.drop(columns=["rn"], errors="ignore")
                df_sector = df_sector.drop(columns=["rn"], errors="ignore")

            df_stock_2["cap"] = df_stock_2.groupby(["ticker"], group_keys=False)[
                "cap"
            ].ffill()
            df_stock_2["roe"] = df_stock_2.groupby(["ticker"], group_keys=False)[
                "roe"
            ].ffill()
            df_stock_2["eps"] = df_stock_2.groupby(["ticker"], group_keys=False)[
                "eps"
            ].ffill()
            df_stock_2["pb"] = df_stock_2.groupby(["ticker"], group_keys=False)[
                "pb"
            ].ffill()
            df_stock_2["pe"] = df_stock_2.groupby(["ticker"], group_keys=False)[
                "pe"
            ].ffill()
            df_stock_2["grossProfitQoQ"] = df_stock_2.groupby(
                ["ticker"], group_keys=False
            )["grossProfitQoQ"].ffill()
            df_stock_2["netIncomeQoQ"] = df_stock_2.groupby(
                ["ticker"], group_keys=False
            )["netIncomeQoQ"].ffill()

        end_time = time.time()
        LOGGER.info(f"{LOGGER_PREFIX} DATA FETCH TIME: {end_time - start_time:.2f}s")

        df_stock = pd.merge(df_stock_1, df_stock_2, on=["date", "ticker"], how="left")
        df_stock = pd.merge(df_stock, df_stock_0, on=["ticker"], how="left")
        df_stock = pd.merge(df_stock, df_sector, on=["ticker"], how="left")

        df_stock["averageLiquidity21"] = df_stock.groupby("ticker")[
            "totalMatchVolume"
        ].transform(lambda x: x.rolling(21).mean())
        df_stock["averageLiquidity63"] = df_stock.groupby("ticker")[
            "totalMatchVolume"
        ].transform(lambda x: x.rolling(63).mean())
        df_stock["averageLiquidity252"] = df_stock.groupby("ticker")[
            "totalMatchVolume"
        ].transform(lambda x: x.rolling(252).mean())

        df_stock["date"] = pd.to_datetime(df_stock["date"])
        df_stock = df_stock.sort_values(["ticker", "date"])
        df_stock["year_month"] = df_stock["date"].dt.to_period("M")

        # Ly ngy giao dch cui cng trong thng cho tng m
        monthly_last_day = (
            df_stock.groupby(["ticker", "year_month"])["date"].max().reset_index()
        )
        monthly_last_day.columns = ["ticker", "year_month", "last_trading_date"]

        # Merge  ch gi li d liu ca ngy cui thng
        df_monthly = pd.merge(
            df_stock,
            monthly_last_day,
            left_on=["ticker", "date"],
            right_on=["ticker", "last_trading_date"],
            how="inner",
        )

        # To thng tin thng k tip  d bo
        df_monthly["next_month"] = df_monthly["date"].dt.to_period("M") + 1
        df_monthly["year"] = df_monthly["next_month"].dt.year
        df_monthly["month"] = df_monthly["next_month"].dt.month

        # Lc v x l d liu cn dng
        df_filtered = df_monthly[
            [
                "date",
                "year",
                "month",
                "ticker",
                "exchangeCode",
                "sectorL2",
                "cap",
                "averageLiquidity21",
                "averageLiquidity63",
                "averageLiquidity252",
                "netIncomeQoQ",
                "grossProfitQoQ",
                "roe",
                "eps",
                "pe",
                "pb",
            ]
        ].copy()

        current_period = pd.Period.now(freq="M")
        df_filtered = df_filtered[
            pd.to_datetime(
                df_filtered["year"].astype(str) + "-" + df_filtered["month"].astype(str)
            ).dt.to_period("M")
            <= current_period
        ]
        df_filtered = df_filtered.rename(columns={"ticker": "symbol"})
        df_filtered = df_filtered.drop_duplicates()
        df_filtered = df_filtered.apply(
            lambda col: DataUtils.round_and_fix_near_zero_column(col)
        )

        # Fix for pandas FutureWarning: explicitly handle inf values replacement
        df_filtered = df_filtered.replace(
            {np.inf: np.nan, -np.inf: np.nan}
        ).infer_objects(copy=False)

        df_filtered = df_filtered.sort_values(
            by=["year", "month", "symbol"]
        ).reset_index(drop=True)
        df_filtered["date"] = df_filtered["date"].dt.strftime(
            SQLServerConsts.DATE_FORMAT
        )

        """
        FILTER 1: Lc c bn v thanh khon, vn ha, ngnh v sn giao dch.
        """

        df_filtered = df_filtered.dropna(subset=["cap", "grossProfitQoQ"])

        min_market_cap = 1e12
        df_filtered = df_filtered[df_filtered["cap"] >= min_market_cap]

        min_avg_liquidity_63 = 1e4
        df_filtered = df_filtered[
            df_filtered["averageLiquidity63"] >= min_avg_liquidity_63
        ]

        exchanges = ["HOSE", "HNX"]
        df_filtered = df_filtered[df_filtered["exchangeCode"].isin(exchanges)]

        available_sectors = [
            "Financial Services",
            "Construction & Materials",
            "Real Estate",
            "Banks",
            "Basic Resources",
            "Retail",
        ]
        df_filtered = df_filtered[df_filtered["sectorL2"].isin(available_sectors)]

        """
        GROUP RANKING: Xp hng cc c phiu theo ROE v thanh khon trung bnh 21 ngy.
        """
        start_time = time.time()
        df_filtered["roe_group_rank"] = cls.apply_rank_by_date(
            df_filtered, "roe", "date", "sectorL2"
        )
        df_filtered["averageLiquidity21_group_rank"] = cls.apply_rank_by_date(
            df_filtered, "averageLiquidity21", "date", "sectorL2"
        )
        df_filtered["cap_group_rank"] = cls.apply_rank_by_date(
            df_filtered, "cap", "date", "sectorL2"
        )

        end_time = time.time()
        LOGGER.info(f"{LOGGER_PREFIX} GROUP RANKING TIME: {end_time - start_time:.2f}s")

        """
        FILTER 2: Lc theo tiu ch Cht lng.
        """

        # Loi b cc m thiu d liu cho lp lc ny
        df_filtered = df_filtered.dropna(subset=["roe", "pe"])

        # Lc theo ROE v D/E
        min_roe = 0.01
        min_gross_profit_qoq = 0.01
        df_filtered = df_filtered[
            (df_filtered["roe"] >= min_roe)
            & (df_filtered["grossProfitQoQ"] >= min_gross_profit_qoq)
        ]

        """
        FILTER 3: Xp hng cc c phiu cn li v chn ra 20 m tt nht.
        """
        df_ranked = df_filtered.copy()
        df_ranked["CompositeScore"] = (
            df_ranked["roe_group_rank"] + df_ranked["averageLiquidity21_group_rank"]
        )

        final_df = pd.DataFrame()
        for year_month, group in df_ranked.groupby(["year", "month"]):
            top_stocks = group.nlargest(UNIVERSE_SIZE, "CompositeScore", keep="first")
            final_df = pd.concat([final_df, top_stocks], ignore_index=True)

        universe_df = final_df[
            [
                "date",
                "year",
                "month",
                "symbol",
                "exchangeCode",
                "sectorL2",
                "cap",
                "averageLiquidity21",
                "averageLiquidity63",
                "averageLiquidity252",
                "grossProfitQoQ",
                "roe",
                "eps",
                "pe",
                "pb",
            ]
        ]

        LOGGER.info(f"{LOGGER_PREFIX} DONE UPDATING UNIVERSE TOP {UNIVERSE_SIZE}\n")
        return universe_df

    @staticmethod
    def rank(x, rate=2):
        x = np.array(x)

        valid_mask = ~np.isnan(x)
        if not np.any(valid_mask):
            return np.full_like(x, np.nan, dtype=float)

        if rate == 0:
            ranks = rankdata(x[valid_mask], method="average")
        else:
            ranks = rankdata(x[valid_mask], method="average")

        n = np.sum(valid_mask)
        if n == 1:
            normalized_ranks = np.array([0.5])
        else:
            normalized_ranks = (ranks - 1) / (n - 1)

        result = np.full_like(x, np.nan, dtype=float)
        result[valid_mask] = normalized_ranks
        return result

    @classmethod
    def group_rank(cls, x, group):
        x = np.array(x)
        group = np.array(group)

        result = np.full_like(x, np.nan, dtype=float)
        valid_mask = ~np.isnan(x)
        valid_groups = group[valid_mask]
        unique_groups = np.unique(valid_groups[~pd.isna(valid_groups)])

        for g in unique_groups:
            group_mask = (group == g) & valid_mask
            if not np.any(group_mask):
                continue

            group_values = x[group_mask]
            group_ranks = cls.rank(group_values, rate=2)
            result[group_mask] = group_ranks

        return result

    @classmethod
    def apply_rank_by_date(cls, df, value_col, date_col="date", group_col=None):
        result = pd.Series(index=df.index, dtype=float)

        for date, group_data in df.groupby(date_col):
            values = group_data[value_col].values

            if group_col is not None:
                groups = group_data[group_col].values
                ranks = cls.group_rank(values, groups)
            else:
                ranks = cls.rank(values)

            result.loc[group_data.index] = ranks

        return result

================
File: backend/modules/portfolio/services/processors/__init__.py
================
from .portfolio_processor import PortfolioProcessor
from .recommendation_engine import RecommendationEngine
from .portfolio_pnl_calculator import PortfolioPnLCalculator
from .portfolio_risk_calculator import PortfolioRiskCalculator

================
File: backend/modules/portfolio/services/processors/portfolio_pnl_calculator.py
================
import pandas as pd
import numpy as np
from decimal import Decimal
from typing import List, Dict, Optional

from backend.common.consts import SQLServerConsts


class PortfolioPnLCalculator:
    @staticmethod
    def process_portfolio_pnl(portfolio_weights_df: pd.DataFrame) -> pd.DataFrame:
        BOOK_SIZE = 1e9  # 1 t VND
        portfolio_data = portfolio_weights_df.copy()

        if portfolio_data.empty:
            return pd.DataFrame(
                columns=["date", "pnl_pct", "pnl", "drawdown_pct", "drawdown"]
            )

        # Sort by date to ensure proper order
        portfolio_data["date"] = pd.to_datetime(portfolio_data["date"])
        portfolio_data = portfolio_data.sort_values(["date", "symbol"])

        # Get unique dates and symbols
        dates = portfolio_data["date"].unique()
        symbols = portfolio_data["symbol"].unique()

        # Create pivot tables for easier calculation
        weights_pivot = portfolio_data.pivot(
            index="date", columns="symbol", values="weight"
        ).fillna(0)
        prices_pivot = portfolio_data.pivot(
            index="date", columns="symbol", values="price"
        ).ffill()

        # Initialize portfolio value tracking
        portfolio_values = []
        current_portfolio_value = BOOK_SIZE

        for i, date in enumerate(dates):
            if i == 0 or i == 1:
                portfolio_values.append(BOOK_SIZE)
            else:
                prev_date = dates[i - 2]
                prev_weights = weights_pivot.loc[prev_date].values
                prev_prices = prices_pivot.loc[prev_date].values
                curr_prices = prices_pivot.loc[date].values

                price_returns = (curr_prices - prev_prices) / prev_prices
                price_returns = np.nan_to_num(
                    price_returns, nan=0.0, posinf=0.0, neginf=0.0
                )

                portfolio_return = np.sum(prev_weights * price_returns)

                current_portfolio_value *= 1 + portfolio_return
                portfolio_values.append(current_portfolio_value)

        result_df = pd.DataFrame({"date": dates, "portfolio_value": portfolio_values})

        # Calculate PnL percentage and absolute PnL
        result_df["pnl_pct"] = (result_df["portfolio_value"] / BOOK_SIZE - 1) * 100
        result_df["pnl"] = result_df["portfolio_value"] - BOOK_SIZE

        # Tnh daily profit = PnL t - Pnl at (t-1): ct G, ngy u ko c t-1 th gn bng 0
        result_df["daily_profit"] = result_df["pnl"].diff().fillna(0)
        result_df["daily_profit_pct"] = result_df["daily_profit"] / BOOK_SIZE

        # Calculate drawdown
        running_max = result_df["portfolio_value"].expanding().max()
        result_df["drawdown_pct"] = (
            (running_max - result_df["portfolio_value"]) / BOOK_SIZE
        )
        result_df["drawdown"] = (running_max - result_df["portfolio_value"])

        # Format date
        result_df["date"] = result_df["date"].dt.strftime(SQLServerConsts.DATE_FORMAT)

        return result_df[["date", "pnl_pct", "pnl", "drawdown_pct", "drawdown", "daily_profit_pct"]]

    @staticmethod
    def process_index_pnl(df_index: pd.DataFrame) -> pd.DataFrame:
        BOOK_SIZE = 1e9
        index_data = df_index.copy()

        if index_data.empty:
            return pd.DataFrame(
                columns=["date", "pnl_pct", "pnl", "drawdown_pct", "drawdown", "daily_profit_pct"]
            )

        # Sort by date to ensure proper order
        index_data["date"] = pd.to_datetime(index_data["date"])
        index_data = index_data.sort_values("date")

        # Initialize index value tracking
        index_values = []
        current_index_value = BOOK_SIZE

        for i in range(len(index_data)):
            if i == 0 or i == 1:
                # First day - just use base value
                index_values.append(BOOK_SIZE)
            else:
                # Get previous and current index prices
                prev_price = index_data["closeIndex"].iloc[i - 2]
                curr_price = index_data["closeIndex"].iloc[i]

                # Calculate price return
                if prev_price > 0:
                    price_return = (curr_price - prev_price) / prev_price
                else:
                    price_return = 0.0

                # Handle NaN/inf
                price_return = np.nan_to_num(
                    price_return, nan=0.0, posinf=0.0, neginf=0.0
                )

                # Update index value
                current_index_value *= 1 + price_return
                index_values.append(current_index_value)

        # Create result DataFrame
        result_df = pd.DataFrame(
            {"date": index_data["date"], "index_value": index_values}
        )

        # Calculate PnL percentage and absolute PnL
        result_df["pnl_pct"] = (result_df["index_value"] / BOOK_SIZE - 1) * 100
        result_df["pnl"] = result_df["index_value"] - BOOK_SIZE

        result_df["daily_profit"] = result_df["pnl"].diff().fillna(0)
        result_df["daily_profit_pct"] = result_df["daily_profit"] / BOOK_SIZE

        # Calculate drawdown
        running_max = result_df["index_value"].expanding().max()
        result_df["drawdown_pct"] = (
            (running_max - result_df["index_value"]) / BOOK_SIZE
        )
        result_df["drawdown"] = (running_max - result_df["index_value"])

        # Format date
        result_df["date"] = result_df["date"].dt.strftime(SQLServerConsts.DATE_FORMAT)

        return result_df[["date", "pnl_pct", "pnl", "drawdown_pct", "drawdown", "daily_profit_pct"]]

================
File: backend/modules/portfolio/services/processors/portfolio_processor.py
================
import pandas as pd
from decimal import Decimal
from typing import List, Dict, Optional

from backend.modules.portfolio.core import Position, Money, Weight


class PortfolioProcessor:
    @staticmethod
    def process_deals_to_positions(
        deals_list: List[Dict], net_asset_value: float, stock_value: float
    ) -> List[Position]:
        """Convert raw deals data into Position objects"""
        positions = []

        if not deals_list or net_asset_value <= 0:
            return positions

        for deal in deals_list:
            symbol = deal.get("symbol", "")
            if not symbol:
                continue

            accumulate_quantity = deal.get("accumulateQuantity", 0)
            market_price_value = deal.get("marketPrice", 0)
            cost_price_value = deal.get("averageCostPrice", 0)
            break_even_price = deal.get("breakEvenPrice", 0)
            realized_profit_value = deal.get("realizedProfit", 0)
            unrealized_profit_value = deal.get("unrealizedProfit", 0)

            if accumulate_quantity > 0 and market_price_value > 0:
                market_price = Money(Decimal(str(market_price_value)))
                cost_price = Money(Decimal(str(cost_price_value)))
                break_even_price = Money(Decimal(str(break_even_price)))
                realized_profit = Money(Decimal(str(realized_profit_value)))
                unrealized_profit = Money(Decimal(str(unrealized_profit_value)))

                market_value = market_price.amount * accumulate_quantity
                weight_pct = (market_value / Decimal(str(net_asset_value))) * 100
                weight_over_sv_pct = (market_value / Decimal(str(stock_value))) * 100
                weight = Weight(weight_pct)
                weight_over_sv = Weight(weight_over_sv_pct)

                position = Position(
                    symbol=symbol,
                    quantity=accumulate_quantity,
                    market_price=market_price,
                    cost_price=cost_price,
                    break_even_price=break_even_price,
                    weight=weight,
                    weight_over_sv=weight_over_sv,
                    realized_profit=realized_profit,
                    unrealized_profit=unrealized_profit,
                )
                positions.append(position)

        positions.sort(key=lambda x: x.weight.percentage, reverse=True)
        return positions


    @staticmethod
    def process_portfolio_pnl(weights_df: pd.DataFrame) -> float:
        """Calculate the total portfolio PnL based on the weights DataFrame"""
        if weights_df is None or weights_df.empty:
            return 0.0

        # Calculate the total portfolio PnL
        total_value = (weights_df["target_weight"] * weights_df["market_price"]).sum()
        return total_value

================
File: backend/modules/portfolio/services/processors/portfolio_risk_calculator.py
================
import pandas as pd
import numpy as np
from scipy import stats


class PortfolioRiskCalculator:
    def __init__(self, df_pnl):
        self.trading_days = 252
        self.risk_free_rate = 0
        self.df_pnl = df_pnl.iloc[-self.trading_days:].copy()
        self.daily_profit_pct = df_pnl["daily_profit_pct"].iloc[-self.trading_days:].copy()
        

    def calculate_basic_metrics(self):
        total_return = self.df_pnl["pnl_pct"].iloc[-1]
        max_return = self.df_pnl["pnl_pct"].max()
        min_return = self.df_pnl["pnl_pct"].min()

        return {
            "total_return_pct": round(total_return, 2),
            "max_return_pct": round(max_return, 2),
            "min_return_pct": round(min_return, 2),
        }

    def calculate_volatility_metrics(self):
        if len(self.daily_profit_pct) < 2:
            return {
                "daily_volatility_pct": 0.0,
                "annualized_volatility_pct": 0.0,
                "downside_volatility_pct": 0.0,
            }

        daily_vol = self.daily_profit_pct.std()

        # Handle NaN case
        if pd.isna(daily_vol) or np.isinf(daily_vol):
            daily_vol = 0.0

        annualized_vol = daily_vol * np.sqrt(self.trading_days)

        downside_returns = self.daily_profit_pct[self.daily_profit_pct < 0]
        downside_vol = 0.0
        if len(downside_returns) > 0:
            downside_std = downside_returns.std()
            if not pd.isna(downside_std) and not np.isinf(downside_std):
                downside_vol = downside_std * np.sqrt(self.trading_days)

        return {
            "daily_volatility_pct": round(daily_vol * 100, 2),
            "annualized_volatility_pct": round(annualized_vol * 100, 2),
            "downside_volatility_pct": round(downside_vol * 100, 2),
        }

    def calculate_risk_adjusted_ratios(self):
        daily_rf_rate = self.risk_free_rate / self.trading_days

        excess_returns = self.daily_profit_pct - daily_rf_rate
        daily_std = self.daily_profit_pct.std()

        sharpe_ratio = 0.0
        if daily_std > 0 and not pd.isna(daily_std) and not np.isinf(daily_std):
            sharpe_calc = excess_returns.mean() / daily_std * np.sqrt(self.trading_days)
            if not pd.isna(sharpe_calc) and not np.isinf(sharpe_calc):
                sharpe_ratio = sharpe_calc

        downside_returns = self.daily_profit_pct[self.daily_profit_pct < daily_rf_rate]
        sortino_ratio = 0.0

        if len(downside_returns) > 0:
            downside_std = downside_returns.std()
            if (
                downside_std > 0
                and not pd.isna(downside_std)
                and not np.isinf(downside_std)
            ):
                sortino_calc = (
                    (self.daily_profit_pct.mean() - daily_rf_rate)
                    / downside_std
                    * np.sqrt(self.trading_days)
                )
                if not pd.isna(sortino_calc) and not np.isinf(sortino_calc):
                    sortino_ratio = sortino_calc

        return {
            "sharpe_ratio": round(sharpe_ratio, 2),
            "sortino_ratio": round(sortino_ratio, 2),
        }

    def calculate_drawdown_metrics(self):        
        max_drawdown = self.df_pnl["drawdown_pct"].max() * 100

        df_pnl = self.df_pnl.copy()

        total_return = self.df_pnl["pnl_pct"].iloc[-1]
        calmar_ratio = total_return / abs(max_drawdown) if max_drawdown != 0 else 0

        return {
            "max_dd_pct": round(max_drawdown, 2),
            "calmar_ratio": round(calmar_ratio, 2),
        }


    def calculate_var_cvar(self, confidence_level=0.05):
        if len(self.daily_profit_pct) == 0:
            return {
                f"var_{int((1-confidence_level)*100)}_daily_pct": 0.0,
                f"cvar_{int((1-confidence_level)*100)}_daily_pct": 0.0,
            }

        var = np.percentile(self.daily_profit_pct, confidence_level * 100)
        if pd.isna(var) or np.isinf(var):
            var = 0.0

        tail_returns = self.daily_profit_pct[self.daily_profit_pct <= var]
        cvar = tail_returns.mean() if len(tail_returns) > 0 else 0.0
        if pd.isna(cvar) or np.isinf(cvar):
            cvar = 0.0

        return {
            f"var_{int((1-confidence_level)*100)}_daily_pct": round(var * 100, 4),
            f"cvar_{int((1-confidence_level)*100)}_daily_pct": round(cvar * 100, 4),
        }

    def calculate_performance_metrics(self):
        if len(self.daily_profit_pct) == 0:
            return {
                "win_rate_pct": 0.0,
                "best_day_pct": 0.0,
                "worst_day_pct": 0.0,
                "skewness": 0.0,
                "kurtosis": 0.0,
            }

        win_rate = (self.daily_profit_pct > 0).mean() * 100
        best_day = self.daily_profit_pct.max()
        worst_day = self.daily_profit_pct.min()

        best_day_pct = min(best_day * 100, 1000)
        worst_day_pct = max(worst_day * 100, -1000)

        skewness = 0.0
        kurtosis = 0.0
        try:
            if len(self.daily_profit_pct) > 2:
                skew_calc = stats.skew(self.daily_profit_pct)
                if not pd.isna(skew_calc) and not np.isinf(skew_calc):
                    skewness = skew_calc

                kurt_calc = stats.kurtosis(self.daily_profit_pct)
                if not pd.isna(kurt_calc) and not np.isinf(kurt_calc):
                    kurtosis = kurt_calc
        except Exception as e:
            print(f"Error calculating skewness/kurtosis: {e}")

        return {
            "win_rate_pct": round(win_rate, 2),
            "best_day_pct": round(best_day_pct, 2),
            "worst_day_pct": round(worst_day_pct, 2),
            "skewness": round(skewness, 2),
            "kurtosis": round(kurtosis, 2),
        }

    def calculate_rolling_metrics(self, window=21):
        daily_rf_rate = self.risk_free_rate / self.trading_days
        rolling_excess = self.daily_profit_pct.rolling(window).mean() - daily_rf_rate
        rolling_vol = self.daily_profit_pct.rolling(window).std()
        rolling_sharpe = (rolling_excess / rolling_vol * np.sqrt(self.trading_days)).dropna()
        rolling_volatility = (self.daily_profit_pct.rolling(window).std() * np.sqrt(self.trading_days) * 100).dropna()
        return rolling_sharpe, rolling_volatility

    def calculate_fitness_score(self, portfolio_metrics, benchmark_metrics):
        portfolio_return = portfolio_metrics["total_return_pct"]
        benchmark_return = benchmark_metrics["total_return_pct"]
        portfolio_vol = portfolio_metrics.get("annualized_volatility_pct", 0)
        benchmark_vol = benchmark_metrics.get("annualized_volatility_pct", 0)
        portfolio_drawdown = abs(portfolio_metrics.get("max_dd_pct", 0))
        benchmark_drawdown = abs(benchmark_metrics.get("max_dd_pct", 0))
        excess_return = portfolio_return - benchmark_return
        vol_penalty = (portfolio_vol - benchmark_vol) * 0.5
        drawdown_penalty = (portfolio_drawdown - benchmark_drawdown) * 0.3
        fitness_score = excess_return - vol_penalty - drawdown_penalty
        return round(fitness_score, 2)

    def calculate_all_metrics(self):
        risk_metrics = {}
        risk_metrics.update(self.calculate_basic_metrics())
        risk_metrics.update(self.calculate_volatility_metrics())
        risk_metrics.update(self.calculate_risk_adjusted_ratios())
        risk_metrics.update(self.calculate_drawdown_metrics())
        risk_metrics.update(self.calculate_var_cvar())
        risk_metrics.update(self.calculate_performance_metrics())

        return risk_metrics

================
File: backend/modules/portfolio/services/processors/recommendation_engine.py
================
import math
from decimal import Decimal
from typing import List, Dict, Optional

from backend.modules.portfolio.core import Position, TradeRecommendation, Money, Weight


class RecommendationEngine:
    def __init__(self, weight_tolerance: float = 1.0):
        self.weight_tolerance = weight_tolerance

    def generate_recommendations(
        self,
        current_positions: List[Position],
        target_weights: List[Dict],
        available_cash: Money,
        net_asset_value: Money,
    ) -> List[TradeRecommendation]:
        """Generate trade recommendations based on current vs target portfolio"""
        recommendations = []

        # Create lookup dictionaries
        current_dict = {pos.symbol: pos for pos in current_positions}
        target_dict = {weight["symbol"]: weight for weight in target_weights}

        all_symbols = set(current_dict.keys()) | set(target_dict.keys())

        for symbol in all_symbols:
            current_weight = current_dict.get(
                symbol,
                Position(
                    symbol=symbol,
                    quantity=0,
                    market_price=Money(Decimal("0")),
                    cost_price=Money(Decimal("0")),
                    break_even_price=Money(Decimal("0")),
                    weight=Weight(Decimal("0")),
                ),
            ).weight.percentage

            target_weight = Decimal(str(target_dict.get(symbol, {}).get("weight", 0)))
            weight_diff = target_weight - current_weight

            if abs(weight_diff) < self.weight_tolerance:
                continue

            priority = self.calculate_priority(abs(weight_diff))

            if weight_diff > self.weight_tolerance:  # Need to buy
                required_value = (target_weight / 100) * net_asset_value.amount
                current_value = current_dict.get(
                    symbol,
                    Position(
                        symbol=symbol,
                        quantity=0,
                        market_price=Money(Decimal("0")),
                        cost_price=Money(Decimal("0")),
                        break_even_price=Money(Decimal("0")),
                        weight=Weight(Decimal("0")),
                    ),
                ).market_value.amount

                cash_needed = Money(required_value - current_value)
                action_price = target_dict[symbol]["marketPrice"]
                action_quantity = math.floor(float(cash_needed.amount) / action_price)

                if cash_needed.amount > 0:
                    recommendation = TradeRecommendation(
                        symbol=symbol,
                        action="BUY",
                        current_weight=Weight(current_weight),
                        target_weight=Weight(target_weight),
                        amount=cash_needed,
                        priority=priority,
                        reason=f"Increase weight from {current_weight:.1f}% to {target_weight:.1f}%",
                        action_price=Money(action_price),
                        action_quantity=action_quantity,
                    )
                    recommendations.append(recommendation)

            elif weight_diff < -self.weight_tolerance:  # Need to sell
                current_value = (
                    current_dict.get(symbol).market_value.amount
                    if symbol in current_dict
                    else Decimal("0")
                )
                target_value = (target_weight / 100) * net_asset_value.amount
                cash_to_raise = Money(current_value - target_value)

                recommendation = TradeRecommendation(
                    symbol=symbol,
                    action="SELL",
                    current_weight=Weight(current_weight),
                    target_weight=Weight(target_weight),
                    amount=cash_to_raise,
                    priority=priority,
                    reason=f"Reduce weight from {current_weight:.1f}% to {target_weight:.1f}%",
                    action_price=Money(current_value),
                    action_quantity=int(cash_to_raise.amount / current_value),
                )
                recommendations.append(recommendation)

        recommendations.sort(
            key=lambda x: (
                x.priority == "HIGH",
                abs(x.target_weight.percentage - x.current_weight.percentage),
            ),
            reverse=True,
        )
        return recommendations

    def calculate_priority(self, weight_diff: Decimal) -> str:
        if weight_diff > 3:
            return "HIGH"
        elif weight_diff > 1.5:
            return "MEDIUM"
        else:
            return "LOW"

================
File: backend/modules/portfolio/utils/balance_utils.py
================
from typing import Dict, Any
from backend.modules.portfolio.entities.balances import Balances


class BalanceUtils:
    @classmethod
    def extract_balance_data(cls, raw_data: Dict, date: str) -> Dict:
        field_mapping = {
            Balances.date.name: date,
            Balances.brokerAccountId.name: raw_data.get("investorAccountId"),
            Balances.totalCash.name: raw_data.get("totalCash"),
            Balances.availableCash.name: raw_data.get("availableCash"),
            Balances.termDeposit.name: raw_data.get("termDeposit"),
            Balances.depositInterest.name: raw_data.get("depositInterest"),
            Balances.stockValue.name: raw_data.get("stockValue"),
            Balances.marginableAmount.name: raw_data.get("marginableAmount"),
            Balances.nonMarginableAmount.name: raw_data.get("nonMarginableAmount"),
            Balances.totalDebt.name: raw_data.get("totalDebt"),
            Balances.netAssetValue.name: raw_data.get("netAssetValue"),
            Balances.receivingAmount.name: raw_data.get("receivingAmount"),
            Balances.secureAmount.name: raw_data.get("secureAmount"),
            Balances.depositFeeAmount.name: raw_data.get("depositFeeAmount"),
            Balances.maxLoanLimit.name: raw_data.get("maxLoanLimit"),
            Balances.withdrawableCash.name: raw_data.get("withdrawableCash"),
            Balances.collateralValue.name: raw_data.get("collateralValue"),
            Balances.orderSecured.name: raw_data.get("orderSecured"),
            Balances.purchasingPower.name: raw_data.get("purchasingPower"),
            Balances.cashDividendReceiving.name: raw_data.get("cashDividendReceiving"),
            Balances.marginDebt.name: raw_data.get("marginDebt"),
            Balances.marginRate.name: raw_data.get("marginRate"),
            Balances.ppWithdraw.name: raw_data.get("ppWithdraw"),
            Balances.blockMoney.name: raw_data.get("blockMoney"),
            Balances.totalRemainDebt.name: raw_data.get("totalRemainDebt"),
            Balances.totalUnrealizedDebt.name: raw_data.get("totalUnrealizedDebt"),
            Balances.blockedAmount.name: raw_data.get("blockedAmount"),
            Balances.advancedAmount.name: raw_data.get("advancedAmount"),
            Balances.advanceWithdrawnAmount.name: raw_data.get(
                "advanceWithdrawnAmount"
            ),
        }

        return {k: v for k, v in field_mapping.items() if v is not None}

================
File: backend/modules/portfolio/utils/deals_utils.py
================
from typing import Dict
from backend.modules.portfolio.entities.deals import Deals


class DealsUtils:
    @classmethod
    def extract_deal_data(cls, raw_data: Dict, date: str) -> Dict:
        field_mapping = {
            Deals.date.name: date,
            Deals.brokerAccountId.name: raw_data.get("accountNo"),
            Deals.dealId.name: raw_data.get("id"),
            Deals.symbol.name: raw_data.get("symbol"),
            Deals.status.name: raw_data.get("status"),
            Deals.side.name: raw_data.get("side"),
            Deals.secure.name: raw_data.get("secure"),
            Deals.accumulateQuantity.name: raw_data.get("accumulateQuantity"),
            Deals.tradeQuantity.name: raw_data.get("tradeQuantity"),
            Deals.closedQuantity.name: raw_data.get("closedQuantity"),
            Deals.t0ReceivingQuantity.name: raw_data.get("t0ReceivingQuantity"),
            Deals.t1ReceivingQuantity.name: raw_data.get("t1ReceivingQuantity"),
            Deals.t2ReceivingQuantity.name: raw_data.get("t2ReceivingQuantity"),
            Deals.costPrice.name: raw_data.get("costPrice"),
            Deals.averageCostPrice.name: raw_data.get("averageCostPrice"),
            Deals.marketPrice.name: raw_data.get("marketPrice"),
            Deals.realizedProfit.name: raw_data.get("realizedProfit"),
            Deals.unrealizedProfit.name: raw_data.get("unrealizedProfit"),
            Deals.breakEvenPrice.name: raw_data.get("breakEvenPrice"),
            Deals.dividendReceivingQuantity.name: raw_data.get("dividendReceivingQuantity"),
            Deals.dividendQuantity.name: raw_data.get("dividendQuantity"),
            Deals.cashReceiving.name: raw_data.get("cashReceiving"),
            Deals.rightReceivingCash.name: raw_data.get("rightReceivingCash"),
            Deals.t0ReceivingCash.name: raw_data.get("t0ReceivingCash"),
            Deals.t1ReceivingCash.name: raw_data.get("t1ReceivingCash"),
            Deals.t2ReceivingCash.name: raw_data.get("t2ReceivingCash"),
            Deals.createdDate.name: raw_data.get("createdDate"),
            Deals.modifiedDate.name: raw_data.get("modifiedDate"),
        }

        return {k: v for k, v in field_mapping.items() if v is not None}

================
File: backend/modules/portfolio/utils/portfolio_utils.py
================
from typing import Dict, Any, Optional, Union
import re


class PortfolioUtils:
    @staticmethod
    def generate_general_portfolio_id(date: str) -> str:
        year_month = date[:7]  # "2024-12"
        return f"SYSTEM-{year_month}"
    
    @staticmethod 
    def generate_custom_portfolio_id(user_id: int, portfolio_name: str) -> str:
        import uuid
        unique_id = str(uuid.uuid4())[:8].upper()
        safe_name = re.sub(r'[^a-zA-Z0-9]', '', portfolio_name)[:10].upper()
        return f"CUSTOM-{user_id}-{safe_name}-{unique_id}"
    
    @staticmethod
    def parse_portfolio_id(portfolio_id: str) -> Dict[str, Any]:
        parts = portfolio_id.split('_')
        
        if parts[0] == 'SYSTEM':
            return {
                'type': 'SYSTEM',
                'year_month': parts[1] if len(parts) > 1 else None
            }
        elif parts[0] == 'CUSTOM':
            return {
                'type': 'USER-CUSTOM', 
                'user_id': int(parts[1]) if len(parts) > 1 else None,
                'name_part': parts[2] if len(parts) > 2 else None,
                'unique_id': parts[3] if len(parts) > 3 else None
            }
        
        return {'type': 'UNKNOWN'}

================
File: backend/redis/client.py
================
import os
import redis

from backend.utils.logger import LOGGER


class RedisConnectionPool:
    def __init__(
        self,
        host,
        port=6379,
        password=None,
        db=0,
        decode_responses=True,
        socket_timeout=10.0,
        socket_connect_timeout=10.0,
    ):
        self.pool = redis.ConnectionPool(
            host=host,
            port=port,
            db=db,
            password=password,
            decode_responses=decode_responses,
            socket_timeout=socket_timeout,
            socket_connect_timeout=socket_connect_timeout,
        )

    def get_conn(self):
        """Ly mt kt ni Redis t pool."""
        try:
            conn = redis.Redis(connection_pool=self.pool)
            conn.ping()
            return conn
        except redis.ConnectionError as e:
            LOGGER.error(f"Failed to connect to Redis server: {e}")
            return None


host = os.getenv("REDIS_HOST", "localhost")
port = int(os.getenv("REDIS_PORT", 6379))
password = None if os.getenv("TEST_REDIS") == "1" else os.getenv("REDIS_PASS", None)

REDIS_CLIENT = RedisConnectionPool(
    host=host,
    port=port,
    password=password,
    socket_timeout=10.0,
    socket_connect_timeout=10.0,
)

================
File: backend/utils/data_utils.py
================
import numpy as np


class DataUtils:
    @classmethod
    def round_and_fix_near_zero_column(cls, col, decimal=4, eps=1e-8):
        if np.issubdtype(col.dtype, np.number):
            col = col.round(decimal)
            col = col.mask(col.abs() < eps, 0.0)  # thay gi tr gn 0 thnh 0.0
        return col

================
File: backend/utils/json_utils.py
================
"""
JSON serialization utilities for handling Decimal and other non-serializable objects
"""

import json
from decimal import Decimal
from typing import Any, Dict, List, Union

class JSONUtils:
    @staticmethod
    def convert_decimals_to_float(obj: Any) -> Any:
        if isinstance(obj, Decimal):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: JSONUtils.convert_decimals_to_float(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [JSONUtils.convert_decimals_to_float(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(JSONUtils.convert_decimals_to_float(item) for item in obj)
        else:
            return obj
        
    @staticmethod
    def safe_json_dumps(obj: Any, **kwargs) -> str:
        converted = JSONUtils.convert_decimals_to_float(obj)
        return json.dumps(converted, **kwargs)
    
    @staticmethod
    def make_json_serializable(data: Dict[str, Any]) -> Dict[str, Any]:
        return JSONUtils.convert_decimals_to_float(data)



def find_decimals_in_object(obj: Any, path: str = "root") -> List[str]:
    decimal_paths = []

    if isinstance(obj, Decimal):
        decimal_paths.append(f"{path} = {obj} (Decimal)")
    elif isinstance(obj, dict):
        for key, value in obj.items():
            decimal_paths.extend(find_decimals_in_object(value, f"{path}.{key}"))
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            decimal_paths.extend(find_decimals_in_object(item, f"{path}[{i}]"))
    elif isinstance(obj, tuple):
        for i, item in enumerate(obj):
            decimal_paths.extend(find_decimals_in_object(item, f"{path}({i})"))

    return decimal_paths


def has_decimals(obj: Any) -> bool:
    return len(find_decimals_in_object(obj)) > 0


def validate_json_serializable(obj: Any, raise_on_error: bool = True) -> bool:
    try:
        json.dumps(obj)
        return True
    except (TypeError, ValueError) as e:
        if raise_on_error:
            # Find and report specific problematic values
            decimals = find_decimals_in_object(obj)
            if decimals:
                error_msg = f"JSON serialization failed due to Decimal objects:\n"
                error_msg += "\n".join(decimals[:10])  # Show first 10
                if len(decimals) > 10:
                    error_msg += f"\n... and {len(decimals) - 10} more"
                raise TypeError(error_msg) from e
            else:
                raise e
        return False


def debug_json_serialization(obj: Any) -> Dict[str, Any]:
    result = {
        "is_serializable": False,
        "has_decimals": False,
        "decimal_count": 0,
        "decimal_paths": [],
        "error_message": None,
    }

    # Check for Decimals
    decimal_paths = find_decimals_in_object(obj)
    result["has_decimals"] = len(decimal_paths) > 0
    result["decimal_count"] = len(decimal_paths)
    result["decimal_paths"] = decimal_paths

    # Test JSON serialization
    try:
        json.dumps(obj)
        result["is_serializable"] = True
    except Exception as e:
        result["error_message"] = str(e)

    return result

================
File: backend/utils/jwt_utils.py
================
import jwt
import datetime

from backend.common.consts import CommonConsts
from backend.modules.auth.types import JwtPayload, RefreshPayload
from backend.utils.time_utils import TimeUtils


class JWTUtils:
    @staticmethod
    def create_access_token(payload: JwtPayload) -> str:
        current_time = datetime.datetime.now(datetime.timezone.utc)
        expire = current_time + datetime.timedelta(seconds=CommonConsts.ACCESS_TOKEN_EXPIRES_IN)
        payload_dict = payload.model_dump()
        payload_dict.update({"exp": int(expire.timestamp()), "iat": int(current_time.timestamp())})
        return jwt.encode(payload_dict, CommonConsts.AT_SECRET_KEY, algorithm="HS256")

    @staticmethod
    def create_refresh_token(payload: RefreshPayload) -> str:
        current_time = datetime.datetime.now(datetime.timezone.utc)
        expire = current_time + datetime.timedelta(seconds=CommonConsts.REFRESH_TOKEN_EXPIRES_IN)
        payload_dict = payload.model_dump()
        payload_dict.update({"exp": int(expire.timestamp()), "iat": int(current_time.timestamp())})
        return jwt.encode(payload_dict, CommonConsts.RT_SECRET_KEY, algorithm="HS256")

    @staticmethod
    def decode_token(token: str, secret_key: str) -> dict:
        return jwt.decode(token, secret_key, algorithms=["HS256"])

================
File: backend/utils/logger.py
================
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s:[%(levelname)s]:%(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

LOGGER = logging.getLogger(__name__)

================
File: backend/utils/time_utils.py
================
import datetime
import pytz
import os


class TimeUtils:
    @classmethod
    def get_current_vn_time(cls):
        utcnow = datetime.datetime.now()
        local_machine_time = utcnow # Timestamp in UTC for development
        docker_time = utcnow + pytz.timezone('Asia/Ho_Chi_Minh').utcoffset(utcnow) 
        return docker_time if os.getenv("DOCKER_TIME") == "1" else local_machine_time

================
File: frontend.py
================
import streamlit as st

from frontend.utils.config import STREAMLIT_CONFIG

st.set_page_config(**STREAMLIT_CONFIG)

from frontend.styles.main import MAIN_CSS
from frontend.utils.helpers import init_session_state
from frontend.components.login import render_login_page
from frontend.components.footer import render_footer
from frontend.components.dashboard import render_dashboard


st.markdown(MAIN_CSS, unsafe_allow_html=True)


def main():
    init_session_state()

    if not st.session_state.authenticated:
        render_login_page()
    else:
        render_dashboard()

    render_footer()

if __name__ == "__main__":
    main()

================
File: frontend/__init__.py
================
from dotenv import load_dotenv
load_dotenv(".env")

================
File: frontend/components/__init__.py
================
"""
Frontend components for CMV Trading Bot
"""

================
File: frontend/components/analysis/__init__.py
================
from .account_comparision import render_portfolio_vs_account_comparison
from .analysis import render_portfolio_analysis_page

================
File: frontend/components/analysis/account_comparision.py
================
import streamlit as st

from frontend.components.analysis.portfolio_selector import render_portfolio_selector_for_comparison
from frontend.components.analysis.account_summary import display_account_summary
from frontend.components.analysis.portfolio_summary_comparision import display_portfolio_summary_comparison
from frontend.components.analysis.portfolio_detailed_comparision import render_detailed_comparison
from frontend.services.portfolio import PortfolioService



def render_portfolio_vs_account_comparison():
    """Render comparison between user's real account and selected portfolio"""
    st.subheader(" Portfolio vs Real Account Comparison")

    # Check if user has broker account
    if not st.session_state.get("broker_account_id"):
        st.warning(
            " No broker account information available. Please check the Account Information section in the sidebar."
        )
        st.info(
            " If you just logged in, try refreshing the account information from the sidebar."
        )
        return

    # Portfolio selector for comparison
    st.markdown("####  Select Portfolio for Comparison")
    selected_portfolio_id = render_portfolio_selector_for_comparison()

    if not selected_portfolio_id:
        st.info(" Select your own portfolio to compare with your real account.")
        return

    # Strategy selector
    strategy = st.radio(
        "Select Strategy for Portfolio Analysis",
        options=["LongOnly", "MarketNeutral"],
        index=0,
        horizontal=True,
        key="comparison_strategy_selector",
    )

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("###  Your Real Account")
        with st.spinner(" Loading account data..."):
            account_data = PortfolioService.get_portfolio_analysis(
                broker_account_id=st.session_state.broker_account_id,
                portfolio_id=selected_portfolio_id,
                strategy_type=strategy,
            )

        if account_data:
            display_account_summary(account_data)
        else:
            st.error(" Failed to load account data.")

    with col2:
        st.markdown("###  Selected Portfolio")
        with st.spinner(" Loading portfolio data..."):
            portfolio_data = PortfolioService.get_portfolio_pnl(
                selected_portfolio_id, strategy
            )

        if portfolio_data:
            display_portfolio_summary_comparison(portfolio_data, selected_portfolio_id)
        else:
            st.error(" Failed to load portfolio data.")

    if account_data and portfolio_data:
        st.markdown("---")
        render_detailed_comparison(account_data, portfolio_data, strategy)

================
File: frontend/components/analysis/account_positions.py
================
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List
from frontend.utils.helpers import format_currency, format_percentage, safe_numeric_value



def display_current_positions(positions: List[Dict]):
    """Display current portfolio positions"""
    if not positions:
        st.info(" No current positions found")
        return

    st.subheader(" Current Portfolio Positions")

    # Create DataFrame
    df_positions = pd.DataFrame(positions)

    if "market_price" in df_positions.columns:
        df_positions = df_positions.drop(columns=["market_price"])

    df_positions["cost_price"] = df_positions["cost_price"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["break_even_price"] = df_positions["break_even_price"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["market_value"] = df_positions["market_value"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["realized_profit"] = df_positions["realized_profit"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["unrealized_profit"] = df_positions["unrealized_profit"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["weight"] = df_positions["weight"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["weight_over_sv"] = df_positions["weight_over_sv"].apply(lambda x: safe_numeric_value(x, default=0))


    # Convert numeric columns to proper dtypes
    numeric_columns = [
        "quantity",
        "cost_price",
        "break_even_price",
        "market_value",
        "realized_profit",
        "unrealized_profit",
    ]
    weight_columns = ["weight_percentage", "weight"]

    for col in numeric_columns:
        if col in df_positions.columns:
            df_positions[col] = pd.to_numeric(df_positions[col], errors="coerce")

    for col in weight_columns:
        if col in df_positions.columns:
            df_positions[col] = pd.to_numeric(df_positions[col], errors="coerce")

    # Portfolio weight distribution chart
    col1, col2 = st.columns([1, 1])

    with col1:
        if "weight_percentage" in df_positions.columns:
            weight_col = "weight_percentage"
        elif "weight" in df_positions.columns:
            weight_col = "weight"
        else:
            weight_col = None

        if weight_col:
            fig_pie = px.pie(
                df_positions,
                values=weight_col,
                names="symbol",
                title="Portfolio Weight Distribution",
                color_discrete_sequence=px.colors.qualitative.Set3,
            )
            fig_pie.update_traces(textposition="inside", textinfo="percent+label")
            fig_pie.update_layout(showlegend=True, height=400)
            st.plotly_chart(fig_pie, use_container_width=True)

    with col2:
        if (
            "market_value" in df_positions.columns
            and not df_positions["market_value"].isna().all()
        ):
            try:
                top_positions = df_positions.nlargest(5, "market_value")
                fig_bar = px.bar(
                    top_positions,
                    x="symbol",
                    y="market_value",
                    title="Top 5 Positions by Value",
                    color="market_value",
                    color_continuous_scale="Blues",
                )
                fig_bar.update_layout(height=400)
                st.plotly_chart(fig_bar, use_container_width=True)
            except (TypeError, ValueError) as e:
                st.warning(
                    "Unable to display top positions chart due to data format issues"
                )

    st.markdown("####  Detailed Positions")

    display_df = df_positions.copy()

    column_config = {
        "symbol": st.column_config.TextColumn("M", width="small"),
        "quantity": st.column_config.NumberColumn("Khi lng", format="%d"),
        "cost_price": st.column_config.NumberColumn("Gi vn TB", format="%.2f"),
        "break_even_price": st.column_config.NumberColumn("Gi ha vn", format="%.2f"),
        "weight": st.column_config.NumberColumn("Trng s (%)/ Ti sn rng", format="%.2f%%"),
        "weight_over_sv": st.column_config.NumberColumn("Trng s (%)/ Gi tr c phiu", format="%.2f%%"),
        "market_value": st.column_config.NumberColumn("Gi tr th trng", format="%.2f"),
        "realized_profit": st.column_config.NumberColumn("Li thc hin", format="%.2f"),
        "unrealized_profit": st.column_config.NumberColumn("Li cha thc hin", format="%.2f")
    }

    if "weight_percentage" in display_df.columns:
        column_config["weight_percentage"] = st.column_config.NumberColumn(
            "Weight %", format="%.2f%%"
        )
    elif "weight" in display_df.columns:
        column_config["weight"] = st.column_config.NumberColumn(
            "Weight %", format="%.2f%%"
        )

    st.dataframe(
        display_df,
        use_container_width=True,
        hide_index=True,
        column_config=column_config,
    )

================
File: frontend/components/analysis/account_summary.py
================
import streamlit as st
from frontend.utils.helpers import format_currency


def display_account_summary(account_data):
    """Display real account summary"""
    account_balance = account_data.get("account_balance", {})

    # Account metrics
    st.metric("Net Asset Value", format_currency(account_balance.get("net_asset_value", 0)))
    st.metric("Available Cash", format_currency(account_balance.get("available_cash", 0)))
    st.metric("Cash Ratio", f"{account_balance.get('cash_ratio', 0):.2%}")

    # Current positions
    current_positions = account_data.get("current_positions", [])
    if current_positions:
        st.markdown("**Current Holdings:**")
        for pos in current_positions[:5]:  # Show top 5
            symbol = pos.get("symbol", "")
            quantity = pos.get("quantity", 0)
            weight = pos.get("weight", {})
            weight_pct = (
                weight.get("percentage", 0) if isinstance(weight, dict) else weight
            )
            st.write(f" {symbol}: {quantity:,} shares ({weight_pct:.1f}%)")

        if len(current_positions) > 5:
            st.write(f"... and {len(current_positions) - 5} more positions")

================
File: frontend/components/analysis/analysis.py
================
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from typing import Dict, Optional
from datetime import datetime

from frontend.services.portfolio import PortfolioService


def render_portfolio_analysis_page():
    st.subheader(" Portfolio Analysis")

    selected_portfolio_id = render_portfolio_selector_for_analysis()

    if not selected_portfolio_id:
        st.info("Please select a portfolio to analyze.")
        return

    strategy = st.radio(
        "Select Strategy",
        options=["LongOnly", "MarketNeutral"],
        index=0,
        horizontal=True,
        key="global_strategy_selector",
    )

    portfolio_data = load_portfolio_data_cached(selected_portfolio_id, strategy)

    if not portfolio_data:
        st.error("Failed to load portfolio data.")
        return

    col1, col2 = st.columns([4, 1])
    with col2:
        if st.button(" Clear Cache", help="Clear cached data to force refresh"):
            clear_portfolio_cache(selected_portfolio_id)
            st.success("Cache cleared!")
            st.rerun()

    tab1, tab2 = st.tabs(
        [" Performance Chart", " Risk Metrics"]
    )

    with tab1:
        render_performance_comparison_chart_with_data(portfolio_data, strategy)

    with tab2:
        render_risk_metrics_real_with_data(portfolio_data, strategy)

    # with tab3:
    #     render_detailed_risk_analysis_tab_with_data(portfolio_data, strategy)


def load_portfolio_data_cached(portfolio_id: str, strategy: str) -> Optional[Dict]:
    """Central function to load portfolio data with caching"""
    cache_key = f"portfolio_data_{portfolio_id}_{strategy}"
    cache_time_key = f"portfolio_data_time_{portfolio_id}_{strategy}"

    # Show cache status if exists
    if cache_key in st.session_state and cache_time_key in st.session_state:
        cache_time = st.session_state[cache_time_key]
        st.info(f" Using cached data from: {cache_time.strftime('%H:%M:%S')}")

    # Load data if not cached or force refresh
    if cache_key not in st.session_state or st.button(
        " Refresh Data", key=f"refresh_data_{portfolio_id}_{strategy}"
    ):
        with st.spinner("Loading portfolio data..."):
            try:
                # Get PnL data from API - this includes all data we need
                pnl_data = PortfolioService.get_portfolio_pnl(portfolio_id, strategy)

                if not pnl_data:
                    st.error("Failed to load portfolio data.")
                    return None

                # Cache the complete data
                st.session_state[cache_key] = pnl_data
                st.session_state[cache_time_key] = datetime.now()
                st.success(" Data loaded successfully!")

            except Exception as e:
                st.error(f"Error loading portfolio data: {str(e)}")
                return None

    return st.session_state.get(cache_key)


def clear_portfolio_cache(portfolio_id: str):
    """Clear all cached data for a specific portfolio"""
    keys_to_remove = []
    for key in st.session_state.keys():
        if portfolio_id in key and any(
            prefix in key
            for prefix in ["portfolio_data", "risk_metrics", "performance_data"]
        ):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        del st.session_state[key]


def render_portfolio_selector_for_analysis() -> Optional[str]:
    """Render portfolio selector for analysis"""
    personal_portfolios = PortfolioService.get_my_portfolios().get("portfolios")
    system_portfolios = PortfolioService.get_system_portfolios().get("portfolios")
    portfolios = personal_portfolios + system_portfolios

    if not personal_portfolios:
        st.sidebar.info(
            " You can analyze the System Portfolio or create your first custom portfolio!"
        )

    # Create portfolio options
    portfolio_options = {}
    for portfolio in portfolios:
        try:
            name = portfolio["metadata"].get("portfolioName", "Unknown")
            portfolio_id = portfolio["portfolioId"]
            num_stocks = len(portfolio.get("records", []))
            display_name = f"{name} ({num_stocks} stocks)"
            portfolio_options[display_name] = portfolio_id
        except Exception as e:
            st.error(f"Error processing portfolio: {e}")

    selected_display_name = st.selectbox(
        " Select Portfolio to Analyze",
        options=list(portfolio_options.keys()),
        key="analysis_portfolio_selector",
    )

    return portfolio_options[selected_display_name] if selected_display_name else None


def render_performance_comparison_chart_with_data(portfolio_data: Dict, strategy: str):
    """Render performance comparison chart using cached data"""
    st.subheader(" Portfolio vs VN-Index Performance")

    if not portfolio_data:
        st.warning("No performance data available.")
        return

    # Extract data from cached portfolio_data
    portfolio_pnl = portfolio_data.get("portfolio", [])
    vnindex_pnl = portfolio_data.get("vnindex", [])
    metadata = portfolio_data.get("metadata", {})

    if not portfolio_pnl or not vnindex_pnl:
        st.warning("No PnL data available for the selected portfolio.")
        return

    # Create comparison chart
    fig = create_comparison_chart(portfolio_pnl, vnindex_pnl, metadata)
    st.plotly_chart(fig, use_container_width=True)

    # Display metadata
    render_performance_metadata(metadata)

    # Display risk metrics summary if available
    if "risk_metrics" in portfolio_data:
        render_risk_metrics_summary(portfolio_data["risk_metrics"])


def render_risk_metrics_real_with_data(portfolio_data: Dict, strategy: str):
    """Render actual risk metrics using cached data"""
    st.subheader(" Risk Metrics Analysis")

    if not portfolio_data or "risk_metrics" not in portfolio_data:
        st.warning("No risk metrics data available.")
        return

    risk_metrics = portfolio_data["risk_metrics"]
    portfolio_metrics = risk_metrics.get("portfolio", {})
    vnindex_metrics = risk_metrics.get("vnindex", {})

    # Risk Metrics Comparison
    render_risk_metrics_comparison(portfolio_metrics, vnindex_metrics)

    # Detailed Risk Analysis
    render_detailed_risk_analysis(portfolio_metrics, vnindex_metrics)

    # Risk vs Return Chart
    render_risk_return_chart(portfolio_metrics, vnindex_metrics)


def render_detailed_risk_analysis_tab_with_data(portfolio_data: Dict, strategy: str):
    """Render detailed risk analysis tab using cached data"""
    st.subheader(" Detailed Analysis")

    if not portfolio_data or "risk_metrics" not in portfolio_data:
        st.warning("No risk metrics data available for detailed analysis.")
        return

    risk_metrics = portfolio_data["risk_metrics"]
    portfolio_metrics = risk_metrics.get("portfolio", {})
    vnindex_metrics = risk_metrics.get("vnindex", {})

    # Show data freshness
    cache_time = st.session_state.get(
        f"portfolio_data_time_{portfolio_data.get('metadata', {}).get('portfolio_id', 'unknown')}_{strategy}"
    )
    if cache_time:
        st.caption(f" Data from: {cache_time.strftime('%H:%M:%S')}")

    # Call the existing detailed analysis function
    render_detailed_risk_analysis(portfolio_metrics, vnindex_metrics)

    # Additional detailed charts
    render_additional_detailed_analysis(portfolio_data)


def render_additional_detailed_analysis(portfolio_data: Dict):
    """Render additional detailed analysis charts"""
    st.subheader(" Additional Analysis")

    metadata = portfolio_data.get("metadata", {})

    # Portfolio composition
    symbols = metadata.get("symbols", [])
    if symbols:
        st.markdown("####  Portfolio Composition Details")

        col1, col2 = st.columns(2)

        with col1:
            st.metric("Total Stocks", len(symbols))
            st.metric("Trading Days Analyzed", metadata.get("total_dates", 0))

        with col2:
            st.metric(
                "Data Period",
                f"{metadata.get('from_date', 'N/A')} to {metadata.get('to_date', 'N/A')}",
            )
            st.metric("Strategy Used", metadata.get("strategy", "N/A"))

        # Show all symbols
        st.markdown("**Symbols in Portfolio:**")
        symbols_text = ", ".join(symbols)
        st.text_area(
            "", symbols_text, height=80, disabled=True, label_visibility="collapsed"
        )


def render_performance_comparison_chart(portfolio_id: str):
    """Render performance comparison chart between portfolio and VN-Index"""
    st.subheader(" Portfolio vs VN-Index Performance")

    # Strategy selector
    strategy = st.radio(
        "Select Strategy",
        options=["LongOnly", "MarketNeutral"],
        index=0,
        horizontal=True,
        key=f"strategy_selector_{portfolio_id}",
    )

    # Cache key for performance data
    cache_key = f"performance_data_{portfolio_id}_{strategy}"

    # Load data with caching
    if cache_key not in st.session_state or st.button(
        " Refresh Data", key=f"refresh_perf_{portfolio_id}"
    ):
        with st.spinner("Loading portfolio performance data..."):
            try:
                # Get PnL data from API
                pnl_data = PortfolioService.get_portfolio_pnl(portfolio_id, strategy)

                if not pnl_data:
                    st.error("Failed to load portfolio PnL data.")
                    return

                # Cache the data
                st.session_state[cache_key] = pnl_data

            except Exception as e:
                st.error(f"Error loading portfolio performance: {str(e)}")
                return

    # Get cached data
    pnl_data = st.session_state.get(cache_key)
    if not pnl_data:
        st.warning("No performance data available.")
        return

    # Extract data
    portfolio_pnl = pnl_data.get("portfolio", [])
    vnindex_pnl = pnl_data.get("vnindex", [])
    metadata = pnl_data.get("metadata", {})

    if not portfolio_pnl or not vnindex_pnl:
        st.warning("No PnL data available for the selected portfolio.")
        return

    # Create comparison chart
    fig = create_comparison_chart(portfolio_pnl, vnindex_pnl, metadata)
    st.plotly_chart(fig, use_container_width=True)

    # Display metadata
    render_performance_metadata(metadata)

    # Display risk metrics summary if available
    if "risk_metrics" in pnl_data:
        render_risk_metrics_summary(pnl_data["risk_metrics"])


def render_risk_metrics_summary(risk_metrics: dict):
    """Render a summary of key risk metrics on performance tab"""
    st.subheader(" Quick Risk Metrics Summary")

    portfolio_metrics = risk_metrics.get("portfolio", {})
    vnindex_metrics = risk_metrics.get("vnindex", {})

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        portfolio_sharpe = portfolio_metrics.get("sharpe_ratio", 0)
        vnindex_sharpe = vnindex_metrics.get("sharpe_ratio", 0)
        delta_sharpe = portfolio_sharpe - vnindex_sharpe
        st.metric(
            "Sharpe Ratio", f"{portfolio_sharpe:.2f}", delta=f"{delta_sharpe:+.2f}"
        )

    with col2:
        portfolio_mdd = portfolio_metrics.get("max_dd_pct", 0)
        vnindex_mdd = vnindex_metrics.get("max_dd_pct", 0)
        delta_mdd = portfolio_mdd - vnindex_mdd
        st.metric("Max Drawdown", f"{portfolio_mdd:.1f}%", delta=f"{delta_mdd:+.1f}%")

    with col3:
        portfolio_winrate = portfolio_metrics.get("win_rate_pct", 0)
        vnindex_winrate = vnindex_metrics.get("win_rate_pct", 0)
        delta_winrate = portfolio_winrate - vnindex_winrate
        st.metric(
            "Win Rate", f"{portfolio_winrate:.1f}%", delta=f"{delta_winrate:+.1f}%"
        )

    with col4:
        portfolio_vol = portfolio_metrics.get("annualized_volatility_pct", 0)
        vnindex_vol = vnindex_metrics.get("annualized_volatility_pct", 0)
        delta_vol = portfolio_vol - vnindex_vol
        st.metric("Volatility", f"{portfolio_vol:.1f}%", delta=f"{delta_vol:+.1f}%")


def create_comparison_chart(
    portfolio_pnl: list, vnindex_pnl: list, metadata: dict
) -> go.Figure:
    """Create PnL comparison chart"""

    # Convert to DataFrames
    portfolio_df = pd.DataFrame(portfolio_pnl)
    vnindex_df = pd.DataFrame(vnindex_pnl)

    # Convert date columns
    portfolio_df["date"] = pd.to_datetime(portfolio_df["date"])
    vnindex_df["date"] = pd.to_datetime(vnindex_df["date"])

    fig = go.Figure()

    # Portfolio PnL line
    fig.add_trace(
        go.Scatter(
            x=portfolio_df["date"],
            y=portfolio_df["pnl_pct"],
            mode="lines",
            name="Portfolio PnL (%)",
            line=dict(color="#1f77b4", width=2.5),
            hovertemplate="<b>Portfolio</b><br>Date: %{x}<br>PnL: %{y:.2f}%<extra></extra>",
        )
    )

    # VN-Index PnL line
    fig.add_trace(
        go.Scatter(
            x=vnindex_df["date"],
            y=vnindex_df["pnl_pct"],
            mode="lines",
            name="VN-Index PnL (%)",
            line=dict(color="#ff7f0e", width=2.5),
            hovertemplate="<b>VN-Index</b><br>Date: %{x}<br>PnL: %{y:.2f}%<extra></extra>",
        )
    )

    # Zero line
    fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)

    # Get date range for title
    from_date = metadata.get("from_date", "N/A")
    to_date = metadata.get("to_date", "N/A")
    portfolio_name = metadata.get("portfolio_id", "Portfolio")
    strategy = metadata.get("strategy", "LongOnly")

    fig.update_layout(
        title=f"Performance Comparison: {portfolio_name} vs VN-Index<br>"
        f"<sub>Strategy: {strategy} | Period: {from_date} to {to_date}</sub>",
        xaxis_title="Date",
        yaxis_title="PnL (%)",
        hovermode="x unified",
        legend=dict(x=0.02, y=0.98),
        height=600,
        showlegend=True,
        template="plotly_white",
        xaxis=dict(showgrid=True, gridwidth=1, gridcolor="rgba(128,128,128,0.2)"),
        yaxis=dict(
            showgrid=True,
            gridwidth=1,
            gridcolor="rgba(128,128,128,0.2)",
            ticksuffix="%",
        ),
    )

    return fig


def render_performance_metadata(metadata: dict):
    """Render performance metadata"""
    st.subheader(" Performance Summary")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        symbols = metadata.get("symbols", [])
        st.metric("Portfolio Stocks", len(symbols))

    with col2:
        st.metric("Trading Days", metadata.get("total_dates", 0))

    with col3:
        st.metric("From Date", metadata.get("from_date", "N/A"))

    with col4:
        st.metric("To Date", metadata.get("to_date", "N/A"))

    if symbols:
        st.subheader(" Portfolio Composition")
        symbols_text = ", ".join(symbols)
        st.text_area("Stocks in Portfolio", symbols_text, height=100, disabled=True)


def render_risk_metrics_real(portfolio_id: str, strategy: str = "LongOnly"):
    """Render actual risk metrics from API response"""
    st.subheader(" Risk Metrics Analysis")

    # Check if we have cached data for this portfolio and strategy
    cache_key = f"risk_metrics_{portfolio_id}_{strategy}"
    cache_time_key = f"risk_metrics_time_{portfolio_id}_{strategy}"

    # Show cache status
    if cache_key in st.session_state and cache_time_key in st.session_state:
        cache_time = st.session_state[cache_time_key]
        st.info(f" Data cached at: {cache_time.strftime('%Y-%m-%d %H:%M:%S')}")

    if cache_key not in st.session_state or st.button(
        " Refresh Risk Metrics", key=f"refresh_risk_{portfolio_id}"
    ):
        with st.spinner("Loading risk metrics..."):
            try:
                # Get PnL data which includes risk_metrics
                pnl_data = PortfolioService.get_portfolio_pnl(portfolio_id, strategy)

                if pnl_data and "risk_metrics" in pnl_data:
                    st.session_state[cache_key] = pnl_data["risk_metrics"]
                    st.session_state[cache_time_key] = datetime.now()
                else:
                    st.error("Failed to load risk metrics data.")
                    return
            except Exception as e:
                st.error(f"Error loading risk metrics: {str(e)}")
                return

    risk_metrics = st.session_state.get(cache_key)
    if not risk_metrics:
        st.warning("No risk metrics data available.")
        return

    portfolio_metrics = risk_metrics.get("portfolio", {})
    vnindex_metrics = risk_metrics.get("vnindex", {})

    # Risk Metrics Comparison
    render_risk_metrics_comparison(portfolio_metrics, vnindex_metrics)

    # Detailed Risk Analysis
    render_detailed_risk_analysis(portfolio_metrics, vnindex_metrics)

    # Risk vs Return Chart
    render_risk_return_chart(portfolio_metrics, vnindex_metrics)


def render_risk_metrics_comparison(portfolio_metrics: dict, vnindex_metrics: dict):
    """Render key risk metrics comparison"""
    st.subheader(" Portfolio vs VN-Index Risk Comparison")

    # Key metrics row 1
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        portfolio_sharpe = portfolio_metrics.get("sharpe_ratio", 0)
        vnindex_sharpe = vnindex_metrics.get("sharpe_ratio", 0)
        delta_sharpe = portfolio_sharpe - vnindex_sharpe
        st.metric(
            "Sharpe Ratio",
            f"{portfolio_sharpe:.2f}",
            delta=f"{delta_sharpe:+.2f} vs VN-Index",
            help="Risk-adjusted return (higher is better)",
        )

    with col2:
        portfolio_sortino = portfolio_metrics.get("sortino_ratio", 0)
        vnindex_sortino = vnindex_metrics.get("sortino_ratio", 0)
        delta_sortino = portfolio_sortino - vnindex_sortino
        st.metric(
            "Sortino Ratio",
            f"{portfolio_sortino:.2f}",
            delta=f"{delta_sortino:+.2f} vs VN-Index",
            help="Downside risk-adjusted return",
        )

    with col3:
        portfolio_mdd = portfolio_metrics.get("max_dd_pct", 0)
        vnindex_mdd = vnindex_metrics.get("max_dd_pct", 0)
        delta_mdd = portfolio_mdd - vnindex_mdd
        st.metric(
            "Max Drawdown",
            f"{portfolio_mdd:.2f}%",
            delta=f"{delta_mdd:+.2f}% vs VN-Index",
            help="Maximum peak-to-trough decline",
        )

    with col4:
        portfolio_winrate = portfolio_metrics.get("win_rate_pct", 0)
        vnindex_winrate = vnindex_metrics.get("win_rate_pct", 0)
        delta_winrate = portfolio_winrate - vnindex_winrate
        st.metric(
            "Win Rate",
            f"{portfolio_winrate:.1f}%",
            delta=f"{delta_winrate:+.1f}% vs VN-Index",
            help="Percentage of profitable days",
        )

    # Key metrics row 2
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        portfolio_vol = portfolio_metrics.get("annualized_volatility_pct", 0)
        vnindex_vol = vnindex_metrics.get("annualized_volatility_pct", 0)
        delta_vol = portfolio_vol - vnindex_vol
        st.metric(
            "Volatility (Annual)",
            f"{portfolio_vol:.1f}%",
            delta=f"{delta_vol:+.1f}% vs VN-Index",
            help="Annualized price volatility",
        )

    with col2:
        portfolio_var = portfolio_metrics.get("var_95_daily_pct", 0)
        vnindex_var = vnindex_metrics.get("var_95_daily_pct", 0)
        delta_var = portfolio_var - vnindex_var
        st.metric(
            "VaR (95%)",
            f"{portfolio_var:.2f}%",
            delta=f"{delta_var:+.2f}% vs VN-Index",
            help="Value at Risk - 95% confidence daily loss",
        )

    with col3:
        portfolio_return = portfolio_metrics.get("total_return_pct", 0)
        vnindex_return = vnindex_metrics.get("total_return_pct", 0)
        delta_return = portfolio_return - vnindex_return
        st.metric(
            "Total Return",
            f"{portfolio_return:.1f}%",
            delta=f"{delta_return:+.1f}% vs VN-Index",
            help="Total cumulative return",
        )

    with col4:
        portfolio_calmar = portfolio_metrics.get("calmar_ratio", 0)
        vnindex_calmar = vnindex_metrics.get("calmar_ratio", 0)
        delta_calmar = portfolio_calmar - vnindex_calmar
        st.metric(
            "Calmar Ratio",
            f"{portfolio_calmar:.2f}",
            delta=f"{delta_calmar:+.2f} vs VN-Index",
            help="Return / Max Drawdown ratio",
        )


def render_detailed_risk_analysis(portfolio_metrics: dict, vnindex_metrics: dict):
    """Render detailed risk analysis tables"""
    st.subheader(" Detailed Risk Analysis")

    # Create comparison table
    risk_comparison_data = []

    metrics_config = [
        ("Total Return", "total_return_pct", "%", "Total cumulative return"),
        ("Max Return", "max_return_pct", "%", "Maximum return reached"),
        ("Min Return", "min_return_pct", "%", "Minimum return (maximum loss)"),
        ("Daily Volatility", "daily_volatility_pct", "%", "Daily price volatility"),
        ("Downside Volatility", "downside_volatility_pct", "%", "Volatility of negative returns"),
        ("Max Drawdown", "max_dd_pct", "%", "Maximum peak-to-trough decline"),
        ("CVaR (95%)", "cvar_95_daily_pct", "%", "Conditional Value at Risk"),
        ("Best Day", "best_day_pct", "%", "Best single day return"),
        ("Worst Day", "worst_day_pct", "%", "Worst single day return"),
        ("Skewness", "skewness", "", "Distribution skewness"),
        ("Kurtosis", "kurtosis", "", "Distribution kurtosis"),
    ]

    for display_name, key, unit, description in metrics_config:
        portfolio_val = portfolio_metrics.get(key, 0)
        vnindex_val = vnindex_metrics.get(key, 0)

        # Handle extreme values display
        if abs(portfolio_val) > 1000:
            portfolio_display = f"{portfolio_val:.0f}{unit}"
        else:
            portfolio_display = f"{portfolio_val:.2f}{unit}"

        if abs(vnindex_val) > 1000:
            vnindex_display = f"{vnindex_val:.0f}{unit}"
        else:
            vnindex_display = f"{vnindex_val:.2f}{unit}"

        # Calculate difference
        diff = portfolio_val - vnindex_val
        if abs(diff) > 1000:
            diff_display = f"{diff:+.0f}{unit}"
        else:
            diff_display = f"{diff:+.2f}{unit}"

        risk_comparison_data.append(
            {
                "Metric": display_name,
                "Portfolio": portfolio_display,
                "VN-Index": vnindex_display,
                "Difference": diff_display,
                "Description": description,
            }
        )

    # Display table
    df_risk = pd.DataFrame(risk_comparison_data)
    st.dataframe(
        df_risk,
        use_container_width=True,
        hide_index=True,
        column_config={
            "Metric": st.column_config.TextColumn("Risk Metric", width="medium"),
            "Portfolio": st.column_config.TextColumn("Portfolio", width="small"),
            "VN-Index": st.column_config.TextColumn("VN-Index", width="small"),
            "Difference": st.column_config.TextColumn("Difference", width="small"),
            "Description": st.column_config.TextColumn("Description", width="large"),
        },
    )


def render_risk_return_chart(portfolio_metrics: dict, vnindex_metrics: dict):
    """Render risk vs return scatter plot"""
    st.subheader(" Risk vs Return Analysis")

    # Extract risk and return data
    portfolio_return = portfolio_metrics.get("total_return_pct", 0)
    portfolio_risk = portfolio_metrics.get("annualized_volatility_pct", 0)

    vnindex_return = vnindex_metrics.get("total_return_pct", 0)
    vnindex_risk = vnindex_metrics.get("annualized_volatility_pct", 0)

    # Create scatter plot
    fig = go.Figure()

    # Portfolio point
    fig.add_trace(
        go.Scatter(
            x=[portfolio_risk],
            y=[portfolio_return],
            mode="markers",
            name="Portfolio",
            marker=dict(size=15, color="#1f77b4", symbol="circle"),
            hovertemplate="<b>Portfolio</b><br>Risk: %{x:.1f}%<br>Return: %{y:.1f}%<extra></extra>",
        )
    )

    # VN-Index point
    fig.add_trace(
        go.Scatter(
            x=[vnindex_risk],
            y=[vnindex_return],
            mode="markers",
            name="VN-Index",
            marker=dict(size=15, color="#ff7f0e", symbol="diamond"),
            hovertemplate="<b>VN-Index</b><br>Risk: %{x:.1f}%<br>Return: %{y:.1f}%<extra></extra>",
        )
    )

    # Add efficient frontier line (mock)
    risk_range = [
        min(portfolio_risk, vnindex_risk) * 0.5,
        max(portfolio_risk, vnindex_risk) * 1.5,
    ]
    efficient_returns = [r * 0.4 for r in risk_range]  # Mock efficient frontier

    fig.add_trace(
        go.Scatter(
            x=risk_range,
            y=efficient_returns,
            mode="lines",
            name="Efficient Frontier (Estimated)",
            line=dict(dash="dash", color="gray"),
            hovertemplate="Risk: %{x:.1f}%<br>Expected Return: %{y:.1f}%<extra></extra>",
        )
    )

    fig.update_layout(
        title="Risk vs Return Positioning",
        xaxis_title="Risk (Annualized Volatility %)",
        yaxis_title="Return (%)",
        height=500,
        hovermode="closest",
        legend=dict(x=0.02, y=0.98),
        template="plotly_white",
    )

    st.plotly_chart(fig, use_container_width=True)

    # Risk-Return Analysis Insights
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("####  Risk-Return Insights")

        # Calculate risk-adjusted metrics
        portfolio_sharpe = portfolio_metrics.get("sharpe_ratio", 0)
        vnindex_sharpe = vnindex_metrics.get("sharpe_ratio", 0)

        if portfolio_sharpe > vnindex_sharpe:
            st.success(
                f" Portfolio has better risk-adjusted returns (Sharpe: {portfolio_sharpe:.2f} vs {vnindex_sharpe:.2f})"
            )
        else:
            st.warning(
                f" VN-Index has better risk-adjusted returns (Sharpe: {vnindex_sharpe:.2f} vs {portfolio_sharpe:.2f})"
            )

        if portfolio_return > vnindex_return:
            st.info(
                f" Portfolio outperformed VN-Index by {portfolio_return - vnindex_return:.1f}%"
            )
        else:
            st.info(
                f" Portfolio underperformed VN-Index by {vnindex_return - portfolio_return:.1f}%"
            )

    with col2:
        st.markdown("####  Risk Profile")

        # Risk level assessment
        if portfolio_risk < vnindex_risk:
            risk_level = "Lower Risk"
            risk_color = "success"
        elif portfolio_risk > vnindex_risk * 1.2:
            risk_level = "Higher Risk"
            risk_color = "error"
        else:
            risk_level = "Similar Risk"
            risk_color = "info"

        getattr(st, risk_color)(f"Portfolio Risk Level: {risk_level}")

        # Drawdown comparison
        portfolio_mdd = abs(portfolio_metrics.get("max_dd_pct", 0))
        vnindex_mdd = abs(vnindex_metrics.get("max_dd_pct", 0))

        if portfolio_mdd < vnindex_mdd:
            st.success(
                f" Lower maximum drawdown ({portfolio_mdd:.1f}% vs {vnindex_mdd:.1f}%)"
            )
        else:
            st.warning(
                f" Higher maximum drawdown ({portfolio_mdd:.1f}% vs {vnindex_mdd:.1f}%)"
            )


def render_detailed_risk_analysis_tab(portfolio_id: str):
    """Wrapper function for detailed risk analysis tab"""
    st.subheader(" Detailed Analysis")

    # Strategy selector
    strategy = st.radio(
        "Select Strategy for Detailed Analysis",
        options=["LongOnly", "MarketNeutral"],
        index=0,
        horizontal=True,
        key="detailed_strategy_selector",
    )

    # Check cache first
    cache_key = f"risk_metrics_{portfolio_id}_{strategy}"
    if cache_key in st.session_state:
        cached_data = st.session_state[cache_key]
        portfolio_data = cached_data["data"]
        cache_time = cached_data.get("timestamp", "unknown")

        if "risk_metrics" in portfolio_data:
            risk_metrics = portfolio_data["risk_metrics"]
            portfolio_metrics = risk_metrics.get("portfolio", {})
            vnindex_metrics = risk_metrics.get("vnindex", {})

            # Show cache info
            st.caption(f" Cached data from: {cache_time}")

            # Call the existing detailed analysis function
            render_detailed_risk_analysis(portfolio_metrics, vnindex_metrics)
            return

    # If no cache, fetch fresh data
    with st.spinner("Loading detailed risk analysis..."):
        try:
            portfolio_data = PortfolioService.get_portfolio_pnl(portfolio_id, strategy)
            if portfolio_data and "risk_metrics" in portfolio_data:
                # Cache the data
                st.session_state[cache_key] = {
                    "data": portfolio_data,
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                }

                risk_metrics = portfolio_data["risk_metrics"]
                portfolio_metrics = risk_metrics.get("portfolio", {})
                vnindex_metrics = risk_metrics.get("vnindex", {})

                render_detailed_risk_analysis(portfolio_metrics, vnindex_metrics)
            else:
                st.error("Unable to load risk metrics data")
        except Exception as e:
            st.error(f"Error loading data: {str(e)}")


def render_detailed_analysis_placeholder():
    """Render detailed analysis section (placeholder)"""
    st.subheader(" Detailed Analysis")

    st.info(" Detailed analysis features coming soon:")

    st.markdown(
        """
    **Planned Features:**
    - Monthly/Quarterly performance breakdown
    - Sector allocation analysis
    - Rolling metrics charts
    - Performance attribution
    - Risk-adjusted returns
    - Correlation analysis
    - Factor exposure analysis
    """
    )

    # Sample chart placeholder
    st.subheader(" Sample Rolling Sharpe Ratio (Demo)")

    # Create dummy rolling Sharpe data
    dates = pd.date_range(start="2023-01-01", end="2024-12-31", freq="D")[:200]
    sharpe_values = pd.Series(range(len(dates))).apply(
        lambda x: 0.8 + 0.5 * (x % 20) / 20
    )

    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=dates,
            y=sharpe_values,
            mode="lines",
            name="30-Day Rolling Sharpe",
            line=dict(color="green", width=2),
        )
    )

    fig.update_layout(
        title="30-Day Rolling Sharpe Ratio (Sample)",
        xaxis_title="Date",
        yaxis_title="Sharpe Ratio",
        height=400,
        template="plotly_white",
    )

    st.plotly_chart(fig, use_container_width=True)

================
File: frontend/components/analysis/portfolio_detailed_comparision.py
================
import streamlit as st
import plotly.express as px 
import pandas as pd
from typing import Dict, List
from frontend.utils.helpers import format_currency, safe_numeric_value


def render_detailed_comparison(account_data, portfolio_data, strategy):
    """Render detailed comparison between account and portfolio"""
    st.subheader(" Detailed Comparison")

    # Create comparison tabs
    comp_tab1, comp_tab2 = st.tabs(
        [" Holdings Comparison", " Recommendations"]
    )

    with comp_tab1:
        render_holdings_comparison(account_data, portfolio_data)


    with comp_tab2:
        render_rebalancing_recommendations(account_data, portfolio_data)

    # with comp_tab2:
    #     render_performance_comparison(account_data, portfolio_data)

def render_holdings_comparison(account_data, portfolio_data):

    current_positions = account_data.get("current_positions", [])
    target_positions = account_data.get("target_weights", [])

    display_current_positions(current_positions)

    st.markdown("####  Holdings Comparison")
    current_symbols = {pos["symbol"]: pos for pos in current_positions}

    target_symbols = {target["symbol"]: target for target in target_positions}

    comparison_data = []
    all_symbols = set(current_symbols.keys()) | set(target_symbols.keys())

    for symbol in all_symbols:
        current_pos = current_symbols.get(symbol, {})
        current_weight = 0
        if current_pos:
            weight_data = current_pos.get("weight", {})
            current_weight = (
                weight_data.get("percentage", 0)
                if isinstance(weight_data, dict)
                else weight_data
            )

        in_target = symbol in target_symbols
        target_weight = target_symbols[symbol]["weight"] if in_target and target_symbols.get(symbol) else 0

        comparison_data.append(
            {
                "M": symbol,
                "T trng / NAV (%)": f"{current_weight:.2f}",
                "T trng mc tiu (%)": f"{target_weight:.2f}" if in_target else "0.00",
                "Chnh lch": f"{current_weight - target_weight:.2f}",
                "Hnh ng": (
                    "Gi"
                    if abs(current_weight - target_weight) < 1
                    else ("Mua" if current_weight < target_weight else "Bn")
                ),
            }
        )

    if comparison_data:
        import pandas as pd

        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison, use_container_width=True)


def render_performance_comparison(account_data, portfolio_data):
    """Compare performance metrics"""
    st.markdown("####  Performance Comparison")

    account_balance = account_data.get("account_balance", {})
    risk_metrics = portfolio_data.get("risk_metrics", {})

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**Your Account**")
        nav = account_balance.get("net_asset_value", 0)
        cash_ratio = account_balance.get("cash_ratio", 0)
        st.metric("Net Asset Value", format_currency(nav))
        st.metric("Cash Ratio", f"{cash_ratio:.2%}")

    with col2:
        st.markdown("**Target Portfolio**")
        expected_return = risk_metrics.get("portfolio_expected_return", 0)
        volatility = risk_metrics.get("portfolio_volatility", 0)
        sharpe = risk_metrics.get("sharpe_ratio", 0)
        st.metric("Expected Return", f"{expected_return:.2%}")
        st.metric("Volatility", f"{volatility:.2%}")
        st.metric("Sharpe Ratio", f"{sharpe:.3f}")


def render_rebalancing_recommendations(account_data, portfolio_data):
    """Provide rebalancing recommendations"""
    st.markdown("####  Rebalancing Recommendations")

    current_positions = account_data.get("current_positions", [])
    target_positions = account_data.get("target_weights", [])
    target_symbols = {target["symbol"]: target for target in target_positions}

    if not target_symbols:
        st.info("No target portfolio symbols available for recommendations.")
        return

    # Calculate total account value for rebalancing
    account_balance = account_data.get("account_balance", {})
    total_value = account_balance.get("net_asset_value", 0)

    if total_value <= 0:
        st.warning("Cannot generate recommendations without valid account value.")
        return

    st.success(f" Based on your account value of {format_currency(total_value)}")

    # Show recommendations for each target symbol
    for symbol, target in target_symbols.items():
        current_pos = next(
            (pos for pos in current_positions if pos.get("symbol") == symbol), None
        )
        current_value = 0

        if current_pos:
            quantity = current_pos.get("quantity", 0)
            market_price_data = current_pos.get("market_price", {})
            price = (
                market_price_data.get("amount", 0)
                if isinstance(market_price_data, dict)
                else market_price_data
            )
            current_value = quantity * price

        target_value = target.get("weight", 0) * total_value / 100
        difference = target_value - current_value

        if (
            abs(difference) > total_value * 0.01
        ):  # Only recommend if difference > 1% of total value
            if difference > 0:
                st.write(
                    f" **{symbol}**: Buy {format_currency(abs(difference))} more"
                )
            else:
                st.write(
                    f" **{symbol}**: Sell {format_currency(abs(difference))} worth"
                )
        else:
            st.write(f" **{symbol}**: Current allocation is appropriate")


def display_current_positions(positions: List[Dict]):
    """Display current portfolio positions"""
    if not positions:
        st.info(" No current positions found")
        return

    st.subheader(" Current Portfolio Positions")

    # Create DataFrame
    df_positions = pd.DataFrame(positions)

    if "market_price" in df_positions.columns:
        df_positions = df_positions.drop(columns=["market_price"])

    df_positions["cost_price"] = df_positions["cost_price"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["break_even_price"] = df_positions["break_even_price"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["market_value"] = df_positions["market_value"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["unrealized_profit"] = df_positions["unrealized_profit"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["weight"] = df_positions["weight"].apply(lambda x: safe_numeric_value(x, default=0))
    df_positions["weight_over_sv"] = df_positions["weight_over_sv"].apply(lambda x: safe_numeric_value(x, default=0))


    # Convert numeric columns to proper dtypes
    numeric_columns = [
        "quantity",
        "cost_price",
        "break_even_price",
        "market_value",
        "unrealized_profit",
    ]
    weight_columns = ["weight_percentage", "weight"]

    for col in numeric_columns:
        if col in df_positions.columns:
            df_positions[col] = pd.to_numeric(df_positions[col], errors="coerce")

    for col in weight_columns:
        if col in df_positions.columns:
            df_positions[col] = pd.to_numeric(df_positions[col], errors="coerce")

    # Portfolio weight distribution chart
    col1, col2 = st.columns([1, 1])

    with col1:
        if "weight_percentage" in df_positions.columns:
            weight_col = "weight_percentage"
        elif "weight" in df_positions.columns:
            weight_col = "weight"
        else:
            weight_col = None

        if weight_col:
            fig_pie = px.pie(
                df_positions,
                values=weight_col,
                names="symbol",
                title="Portfolio Weight Distribution",
                color_discrete_sequence=px.colors.qualitative.Set3,
            )
            fig_pie.update_traces(textposition="inside", textinfo="percent+label")
            fig_pie.update_layout(showlegend=True, height=400)
            st.plotly_chart(fig_pie, use_container_width=True)

    with col2:
        if (
            "market_value" in df_positions.columns
            and not df_positions["market_value"].isna().all()
        ):
            try:
                top_positions = df_positions.nlargest(5, "market_value")
                fig_bar = px.bar(
                    top_positions,
                    x="symbol",
                    y="market_value",
                    title="Top 5 Positions by Value",
                    color="market_value",
                    color_continuous_scale="Blues",
                )
                fig_bar.update_layout(height=400)
                st.plotly_chart(fig_bar, use_container_width=True)
            except (TypeError, ValueError) as e:
                st.warning(
                    "Unable to display top positions chart due to data format issues"
                )

    st.markdown("####  Detailed Positions")

    display_df = df_positions.copy()

    column_config = {
        "symbol": st.column_config.TextColumn("M", width="small"),
        "quantity": st.column_config.NumberColumn("Khi lng", format="%d"),
        "cost_price": st.column_config.NumberColumn("Gi vn TB", format="%.2f"),
        "break_even_price": st.column_config.NumberColumn("Gi ha vn", format="%.2f"),
        "weight": st.column_config.NumberColumn("T trng (%)/ Ti sn rng", format="%.2f%%"),
        "weight_over_sv": st.column_config.NumberColumn("T trng (%)/ Gi tr c phiu", format="%.2f%%"),
        "market_value": st.column_config.NumberColumn("Gi tr th trng", format="%.2f"),
        "unrealized_profit": st.column_config.NumberColumn("Li cha thc hin", format="%.2f")
    }

    st.dataframe(
        display_df,
        use_container_width=True,
        hide_index=True,
        column_config=column_config,
    )

================
File: frontend/components/analysis/portfolio_selector.py
================
import streamlit as st

from frontend.services.portfolio import PortfolioService


def render_portfolio_selector_for_comparison():
    personal_portfolios = PortfolioService.get_my_portfolios().get("portfolios", [])
    system_portfolios = PortfolioService.get_system_portfolios().get("portfolios", [])
    portfolios = personal_portfolios + system_portfolios

    if not portfolios:
        st.warning("No portfolios available. Create a portfolio first.")
        return None

    # Create portfolio options
    portfolio_options = {}
    for portfolio in portfolios:
        try:
            name = portfolio["metadata"].get("portfolioName", "Unknown")
            portfolio_id = portfolio["portfolioId"]
            num_stocks = len(portfolio.get("records", []))
            display_name = f"{name} ({num_stocks} stocks)"
            portfolio_options[display_name] = portfolio_id
        except Exception as e:
            st.error(f"Error processing portfolio: {e}")

    if not portfolio_options:
        return None

    selected_display_name = st.selectbox(
        " Select Portfolio for Comparison",
        options=list(portfolio_options.keys()),
        key="comparison_portfolio_selector",
        help="Choose a portfolio to compare with your real account holdings",
    )

    return portfolio_options[selected_display_name] if selected_display_name else None

================
File: frontend/components/analysis/portfolio_summary_comparision.py
================
import streamlit as st


def display_portfolio_summary_comparison(portfolio_data, portfolio_id):
    st.write(f"**Portfolio ID:** {portfolio_id}")

    risk_metrics = portfolio_data["risk_metrics"]["portfolio"]
    if risk_metrics:
        st.metric("Win Rate", f"{risk_metrics.get('win_rate_pct', 0):.2f}%")
        st.metric("Max Return", f"{risk_metrics.get('max_return_pct', 0):.2f}%")
        st.metric("Max Drawdown", f"{risk_metrics.get('max_dd_pct', 0):.2f}%")
        st.metric("Daily Volatility", f"{risk_metrics.get('daily_volatility_pct', 0):.2f}%")
        st.metric("Sharpe Ratio", f"{risk_metrics.get('sharpe_ratio', 0):.3f}")
        st.metric("Sortino Ratio", f"{risk_metrics.get('sortino_ratio', 0):.3f}")

    portfolio_df = portfolio_data.get("portfolio_pnl_df")
    if portfolio_df and isinstance(portfolio_df, list) and len(portfolio_df) > 0:
        st.markdown("**Portfolio Holdings:**")
        latest_data = portfolio_df[-1] if portfolio_df else {}
        symbols = portfolio_data.get("metadata", {}).get("symbols", [])

        for symbol in symbols[:5]: 
            st.write(f" {symbol}")

        if len(symbols) > 5:
            st.write(f"... and {len(symbols) - 5} more stocks")

================
File: frontend/components/dashboard.py
================
import streamlit as st

from frontend.components.sidebar import render_sidebar
from frontend.pages.portfolio_management import portfolio_management_page
from frontend.pages.portfolio_analysis import portfolio_analysis_page
from frontend.pages.trade_execution import trade_execution_page
from frontend.pages.order_history import order_history_page
from frontend.pages.account_management import account_management_page


def render_dashboard():
    """Render main dashboard"""
    st.markdown(
        '<div class="main-header"><h1> CMV Portfolio Management Website</h1><p>Real-time Trading</p></div>',
        unsafe_allow_html=True,
    )

    render_sidebar()

    if "current_page" not in st.session_state:
        st.session_state.current_page = "Portfolio Analysis"

    if st.session_state.current_page == "Portfolio Analysis":
        portfolio_analysis_page()
    elif st.session_state.current_page == "Portfolio Management":
        portfolio_management_page()
    # elif st.session_state.current_page == "Trade Execution":
    #     trade_execution_page()
    # elif st.session_state.current_page == "Order History":
    #     order_history_page()
    elif st.session_state.current_page == "Account Management":
        account_management_page()

================
File: frontend/components/footer.py
================
import streamlit as st

def render_footer():
    """Render application footer"""
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 1rem;'>
            <p> CMV Portfolio Management App v1.0 | Enhanced Portfolio Management System</p>
            <p>Made with  by @nopnopla.aivay | 
            <a href='https://www.instagram.com/nopnopla.aivay/' style='color: #2a5298;'>Instagram</a> | 
            <p style='color: #2a5298;'>For Support: vinhkhang378@gmail.com</p>  
        </div>
        """,
        unsafe_allow_html=True
    )

================
File: frontend/components/login.py
================
import streamlit as st
import time
from ..services.auth import login_user

from backend.common.consts import CommonConsts
from backend.utils.jwt_utils import JWTUtils

def render_login_page():
    """Render login page"""
    st.markdown(
        '<div class="main-header"><h1> CMV Portfolio Management Website</h1><p>Professional Portfolio Management System</p></div>',
        unsafe_allow_html=True,
    )

    # Center the login form
    col1, col2, col3 = st.columns([1, 2, 1])

    with col2:
        with st.container():
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.subheader("Welcome Back")
            st.write("Please log in to access your trading dashboard")

            with st.form("login_form", clear_on_submit=False):
                username = st.text_input(" Username", placeholder="Enter your username")
                password = st.text_input(" Password", type="password", placeholder="Enter your password")

                col_a, col_b, col_c = st.columns([1, 2, 1])
                with col_b:
                    submit_button = st.form_submit_button(" Login", use_container_width=True)

                if submit_button:
                    if username and password:
                        with st.spinner(" Authenticating..."):
                            auth_data = login_user(username, password)
                            if auth_data:
                                # Set all auth data atomically
                                st.session_state.auth_token = auth_data.get("accessToken")
                                st.session_state.refresh_token = auth_data.get("refreshToken")
                                st.session_state.username = username

                                decoded_payload = JWTUtils.decode_token(
                                    token=st.session_state.auth_token, secret_key=CommonConsts.AT_SECRET_KEY
                                )

                                if decoded_payload:
                                    st.session_state.user_id = decoded_payload.get("userId")
                                    st.session_state.session_id = decoded_payload.get("sessionId")
                                    st.session_state.role = decoded_payload.get("role")

                                # Set authenticated LAST to ensure all data is in place
                                st.session_state.authenticated = True

                                # Initialize other required session state
                                if "order_history" not in st.session_state:
                                    st.session_state.order_history = []
                                if "selected_recommendations" not in st.session_state:
                                    st.session_state.selected_recommendations = []

                                # Fetch default account information
                                with st.spinner(" Loading account information..."):
                                    from ..services.auth import get_default_account

                                    account_data = get_default_account()
                                    if account_data:
                                        st.session_state.custody_code = account_data.get("custody_code")
                                        st.session_state.broker_account_id = (account_data.get("broker_account_id"))
                                        st.session_state.account_name = (account_data.get("name"))
                                        st.session_state.broker_name = (account_data.get("broker_name"))
                                        st.session_state.broker_investor_id = (account_data.get("broker_investor_id"))

                                st.success(" Login successful!")
                                time.sleep(1)
                                st.rerun()
                            else:
                                # Ensure authenticated is False on failed login
                                st.session_state.authenticated = False
                    else:
                        st.error(" Please enter both username and password")

            st.markdown("</div>", unsafe_allow_html=True)

================
File: frontend/components/management/__init__.py
================
from .portfolio_list import render_portfolio_list
from .portfolio_create import render_create_portfolio_form
from .portfolio_selector import render_portfolio_selector

================
File: frontend/components/management/analysis_cache.py
================
import streamlit as st
from frontend.services.portfolio import PortfolioService


def clear_portfolio_analysis_cache(portfolio_id: str):
    keys_to_remove = []
    for key in st.session_state.keys():
        if portfolio_id in key and any(
            prefix in key
            for prefix in ["portfolio_data", "risk_metrics", "performance_data"]
        ):
            keys_to_remove.append(key)

    for key in keys_to_remove:
        del st.session_state[key]


def clear_all_portfolio_service_cache():
    try:
        PortfolioService.get_my_portfolios.clear()
        PortfolioService.get_portfolio_pnl.clear()
        PortfolioService.get_system_portfolios.clear()
        PortfolioService.get_all_symbols.clear()
    except AttributeError:
        pass

================
File: frontend/components/management/portfolio_create.py
================
import streamlit as st
import time


from .analysis_cache import clear_all_portfolio_service_cache
from frontend.services.portfolio import PortfolioService


def render_create_portfolio_form():
    """Render create new portfolio form"""
    st.markdown("####  Create New Portfolio")

    with st.form("create_portfolio_form"):
        # Portfolio basic info
        col1, col2 = st.columns(2)

        with col1:
            portfolio_name = st.text_input(
                "Portfolio Name *",
                placeholder="My Tech Portfolio",
                help="Choose a descriptive name for your portfolio",
            )

        with col2:
            max_positions = st.number_input(
                "Max Positions",
                min_value=1,
                max_value=30,
                value=20,
                help="Maximum number of stocks in this portfolio",
            )

        portfolio_description = st.text_area(
            "Description (Optional)",
            placeholder="Brief description of your investment strategy...",
            height=100,
        )

        st.markdown("####  Select Stocks")

        # Initialize all symbols in session state if not already present
        if "all_symbols" not in st.session_state:
            st.session_state["all_symbols"] = PortfolioService.get_all_symbols()

        all_symbols = st.session_state["all_symbols"]
        selected_symbols = st.multiselect(
            "Select Symbols",
            options=all_symbols,
            default=[],
            max_selections=max_positions,
            placeholder="Choose symbols for your portfolio...",
            help=f"Select up to {max_positions} symbols from {len(all_symbols)} available stocks",
        )

        if selected_symbols:
            st.markdown(
                f"**Selected:** {len(selected_symbols)}/{max_positions} symbols"
            )

            cols = st.columns(5)
            for i, symbol in enumerate(selected_symbols):
                with cols[i % 5]:
                    st.success(f" {symbol}")

        col1, col2, col3 = st.columns([1, 1, 2])

        with col1:
            create_portfolio = st.form_submit_button(
                " Create Portfolio", type="primary"
            )

        with col2:
            clear_form = st.form_submit_button(" Clear")

        if clear_form:
            st.rerun()

        if create_portfolio:
            # Use selected symbols from multiselect
            symbols = selected_symbols

            # Validation
            if not portfolio_name:
                st.error("Portfolio name is required!")
            elif len(symbols) < 2:
                st.error("Please select at least 2 symbols!")
            elif len(symbols) > max_positions:
                st.error(f"Too many symbols! Maximum is {max_positions}")
            else:
                # Create portfolio
                with st.spinner("Creating portfolio..."):
                    result = PortfolioService.create_custom_portfolio(
                        portfolio_name=portfolio_name,
                        symbols=symbols,
                        description=portfolio_description,
                    )

                if result["success"]:
                    st.success(f" Portfolio '{portfolio_name}' created successfully!")
                    st.balloons()

                    # Clear service cache to refresh portfolio lists
                    clear_all_portfolio_service_cache()

                    # Auto-refresh portfolio list
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error(f" Failed to create portfolio: {result['error']}")

================
File: frontend/components/management/portfolio_edit.py
================
import streamlit as st
from typing import Dict


from .analysis_cache import (
    clear_all_portfolio_service_cache,
    clear_portfolio_analysis_cache,
)
from frontend.services.portfolio import PortfolioService


def render_edit_portfolio_form(portfolio: Dict):
    st.markdown("---")
    st.markdown("####  Edit Portfolio")

    if "all_symbols" not in st.session_state:
        st.session_state["all_symbols"] = PortfolioService.get_all_symbols()

    with st.form(f"edit_portfolio_{portfolio['portfolioId']}"):
        current_symbols = [r["symbol"] for r in portfolio["records"]]
        st.markdown(f"**Current symbols:** {', '.join(current_symbols)}")

        available_symbols = [
            s for s in st.session_state["all_symbols"] if s not in current_symbols
        ]

        new_symbols = st.multiselect(
            "Add new symbols",
            options=available_symbols,
            placeholder="Select symbols to add to portfolio...",
            key=f"new_symbols_{portfolio['portfolioId']}",
            help=f"Choose from {len(available_symbols)} available symbols",
        )

        # Remove symbols
        if current_symbols:
            symbols_to_remove = st.multiselect(
                "Select symbols to remove",
                options=current_symbols,
                key=f"remove_symbols_{portfolio['portfolioId']}",
            )
        else:
            symbols_to_remove = []

        col1, col2, col3 = st.columns(3)
        with col1:
            save_changes = st.form_submit_button(" Save Changes", type="primary")

        with col2:
            cancel_edit = st.form_submit_button(" Cancel")

        if save_changes:
            updated_symbols = current_symbols.copy()

            if new_symbols:
                for symbol in new_symbols:
                    if symbol not in updated_symbols:
                        updated_symbols.append(symbol)

                st.success(
                    f"Added {len(new_symbols)} new symbols: {', '.join(new_symbols)}"
                )

            updated_symbols = [s for s in updated_symbols if s not in symbols_to_remove]

            if symbols_to_remove:
                st.info(
                    f"Removed {len(symbols_to_remove)} symbols: {', '.join(symbols_to_remove)}"
                )

            if len(updated_symbols) < 2:
                st.error("Portfolio must contain at least 2 symbols")
            else:
                result = PortfolioService.update_portfolio(
                    portfolio["portfolioId"], updated_symbols
                )

                if result["success"]:
                    st.success("Portfolio updated successfully!")

                    # Clear analysis cache for this portfolio
                    clear_portfolio_analysis_cache(portfolio["portfolioId"])

                    # Clear service cache to refresh portfolio lists
                    clear_all_portfolio_service_cache()

                    st.session_state[f"editing_{portfolio['portfolioId']}"] = False
                    # Keep expander open after successful update
                    expander_key = f"expander_{portfolio['portfolioId']}"
                    if expander_key in st.session_state:
                        st.session_state[expander_key] = True

                    # Show additional info about cache clearing
                    st.info(
                        " Portfolio cache cleared - analysis page will show updated data"
                    )
                    st.rerun()
                else:
                    st.error(f"Failed to update: {result['error']}")

        if cancel_edit:
            st.session_state[f"editing_{portfolio['portfolioId']}"] = False
            # Keep expander open when exiting edit mode
            expander_key = f"expander_{portfolio['portfolioId']}"
            if expander_key in st.session_state:
                st.session_state[expander_key] = True
            st.rerun()

================
File: frontend/components/management/portfolio_list.py
================
import streamlit as st
import time
from typing import Dict, Optional


from .analysis_cache import (
    clear_all_portfolio_service_cache,
    clear_portfolio_analysis_cache,
)
from .portfolio_edit import render_edit_portfolio_form
from frontend.services.portfolio import PortfolioService


def render_portfolio_list():
    st.markdown("####  Your Portfolios")

    portfolios = PortfolioService.get_my_portfolios().get("portfolios")

    if not portfolios:
        st.info(" You don't have any custom portfolios yet. Create your first one!")
        return

    with st.expander(" System Portfolio (Default)", expanded=False):
        st.markdown(
            """
        **System Portfolio** contains the top 20 stocks selected by our algorithm each month:
        -  Automatically updated monthly
        -  Based on fundamental analysis
        -  Optimized for Vietnamese market
        """
        )

    custom_portfolios = [
        p for p in portfolios if p["metadata"].get("portfolioType") == "CUSTOM"
    ]

    if not custom_portfolios:
        st.info("No custom portfolios found.")
        return

    for portfolio in custom_portfolios:
        render_portfolio_card(portfolio)


def render_portfolio_card(portfolio: Dict):
    portfolio_id = portfolio["portfolioId"]

    expander_key = f"expander_{portfolio_id}"
    if expander_key not in st.session_state:
        st.session_state[expander_key] = False

    if st.session_state.get(f"confirm_delete_{portfolio_id}", False):
        st.session_state[expander_key] = True

    with st.expander(
        f" {portfolio['metadata']['portfolioName']}",
        expanded=st.session_state[expander_key],
    ):
        col1, col2, col3 = st.columns([2, 2, 1])

        with col1:
            st.markdown(f"**Portfolio ID:** `{portfolio['portfolioId'][:8]}...`")
            st.markdown(
                f"**Type:** {portfolio['metadata'].get('portfolioType', 'Unknown')}"
            )
            st.markdown(
                f"**Status:** {' Active' if portfolio['metadata'].get('isActive') else ' Inactive'}"
            )

        with col2:
            records = portfolio.get("records", [])
            st.markdown(f"**Stocks Count:** {len(records)}")
            if records:
                st.markdown(
                    f"**Symbols:** {', '.join(record.get('symbol', '') for record in records[:5])}"
                )
                if len(records) > 5:
                    st.markdown(f"*... and {len(records) - 5} more*")

        with col3:
            if st.button(" Edit", key=f"edit_{portfolio['portfolioId']}"):
                st.session_state[f"editing_{portfolio['portfolioId']}"] = True
                st.session_state[expander_key] = True  # Keep expander open
                st.rerun()

            # Delete button - show modal dialog
            if st.button(" Delete", key=f"delete_{portfolio['portfolioId']}"):
                confirm_delete_dialog(portfolio)

        # Edit mode
        if st.session_state.get(f"editing_{portfolio['portfolioId']}", False):
            render_edit_portfolio_form(portfolio)


@st.dialog("Delete Portfolio")
def confirm_delete_dialog(portfolio: Dict):
    """Modal dialog for delete confirmation"""
    st.markdown(f"###  Delete Portfolio")
    st.error(
        f"Are you sure you want to delete **{portfolio['metadata']['portfolioName']}**?"
    )
    st.markdown("This action cannot be undone!")

    col1, col2 = st.columns(2)

    with col1:
        if st.button(" Yes, Delete", type="primary", use_container_width=True):
            with st.spinner("Deleting portfolio..."):
                result = PortfolioService.delete_portfolio(portfolio["portfolioId"])

            if result["success"]:
                st.success(" Portfolio deleted successfully!")

                # Clear analysis cache for deleted portfolio
                clear_portfolio_analysis_cache(portfolio["portfolioId"])

                # Clear service cache to refresh portfolio lists
                clear_all_portfolio_service_cache()

                time.sleep(1.5)
                st.rerun()
            else:
                st.error(f" Failed to delete: {result['error']}")

    with col2:
        if st.button(" Cancel", use_container_width=True):
            st.rerun()

================
File: frontend/components/management/portfolio_selector.py
================
import streamlit as st
from typing import Optional

from frontend.services.portfolio import PortfolioService


def render_portfolio_selector() -> Optional[str]:
    """Render portfolio selector in sidebar"""
    personal_portfolios = PortfolioService.get_my_portfolios().get("portfolios")
    system_portfolios = PortfolioService.get_system_portfolios().get("portfolios")
    portfolios = personal_portfolios + system_portfolios

    portfolio_options = [
        f"{p['metadata'].get('portfolioName', 'Unknown')} ({len(p.get('records', []))})"
        for p in portfolios
    ]

    if not personal_portfolios:
        st.sidebar.info(
            " You can analyze the System Portfolio or create your first custom portfolio!"
        )

    selected_option = st.sidebar.selectbox(
        " Select Portfolio", portfolio_options, key="portfolio_selector"
    )

    for portfolio in portfolios:
        option_text = f"{portfolio['metadata'].get('portfolioName', 'Unknown')} ({len(portfolio.get('records', []))})"
        if option_text == selected_option:
            return portfolio.get("portfolioId")

    return None

================
File: frontend/components/sidebar.py
================
import streamlit as st

from frontend.services.portfolio import PortfolioService
from frontend.components.management import render_portfolio_selector


def render_sidebar():
    with st.sidebar:
        st.markdown(f"###  Welcome, {st.session_state.get('username', 'User')}!")

        st.markdown("###  Navigation")
        pages = [
            "Portfolio Analysis",
            "Portfolio Management",
            # "Trade Execution",
            # "Order History",
            "Account Management",
        ]

        selected_page = st.selectbox(
            "Go to",
            pages,
            index=pages.index(
                st.session_state.get("current_page", "Portfolio Analysis")
            ),
        )
        st.session_state.current_page = selected_page

        st.divider()

        # Portfolio Selection (only show on analysis page)
        if selected_page == "Portfolio Analysis":
            st.markdown("###  Portfolio Selection")
            render_portfolio_selector()

        # Account Configuration
        st.markdown("###  Account Settings")

        # Broker Account ID
        broker_account_id = st.text_input(
            " Broker Account ID",
            value=st.session_state.get("broker_account_id", ""),
            placeholder="Enter your account ID",
        )
        if broker_account_id:
            st.session_state.broker_account_id = broker_account_id

        # # Strategy Selection
        # strategy_type = st.selectbox(
        #     " Trading Strategy",
        #     ["MarketNeutral", "LongOnly"],
        #     index=(
        #         0
        #         if st.session_state.get("strategy_type", "MarketNeutral")
        #         == "MarketNeutral"
        #         else 1
        #     ),
        # )
        # st.session_state.strategy_type = strategy_type

        st.divider()

        # Portfolio Quick Stats
        if selected_page == "Portfolio Analysis":
            render_portfolio_quick_stats()

        # Rest of sidebar (existing code)
        render_quick_actions()
        render_system_status()
        render_logout_button()


def render_quick_actions():
    st.markdown("###  Quick Actions")
    col1, col2 = st.columns(2)

    with col1:
        if st.button(" Refresh", use_container_width=True):
            st.cache_data.clear()
            st.rerun()

    with col2:
        if st.button(" New Portfolio", use_container_width=True):
            st.session_state.current_page = "Portfolio Management"
            st.rerun()


def render_system_status():
    st.markdown("###  System Status")

    col1, col2 = st.columns(2)
    with col1:
        st.metric("API", " Online")
    with col2:
        st.metric("Portfolios", len(st.session_state.get("portfolios", [])))


def render_logout_button():
    st.divider()

    if st.button(" Logout", use_container_width=True, type="secondary"):
        from ..services.auth import logout_user

        logout_payload = {
            "sessionId": st.session_state.get("session_id", ""),
            "userId": st.session_state.get("user_id", ""),
            "role": st.session_state.get("role", ""),
        }
        logout_user(logout_payload)

        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()


def render_portfolio_quick_stats():
    st.markdown("###  Quick Stats")
    
    portfolios = PortfolioService.get_my_portfolios().get('portfolios')
    
    if portfolios:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("My Portfolios", len(portfolios))
        with col2:
            for portfolio in portfolios:
                portfolio_metadata = portfolio.get('metadata', {})
                st.text(f"Portfolio: {portfolio_metadata.get('portfolioName', 'Unnamed Portfolio')}")

================
File: frontend/pages/__init__.py
================
"""
Frontend pages for CMV Trading Bot
"""

================
File: frontend/pages/account_management.py
================
"""
Account management page for CMV Trading Bot frontend
"""

import streamlit as st
from ..services.api import setup_dnse_account


def account_management_page():
    """Account management and setup page"""
    st.subheader(" Account Management")

    # DNSE Account Setup
    st.markdown("####  DNSE Account Setup")

    with st.expander("Setup New DNSE Account", expanded=False):
        with st.form("dnse_setup_form"):
            custody_code = st.text_input(
                "Custody Code", placeholder="Enter your custody code"
            )
            password = st.text_input(
                "Password", type="password", placeholder="Enter your password"
            )

            submit_setup = st.form_submit_button(
                " Setup Account", use_container_width=True
            )

            if submit_setup and custody_code and password:
                with st.spinner("Setting up DNSE account..."):
                    success = setup_dnse_account(custody_code, password)
                    if success:
                        st.success(" DNSE account setup successful!")
                    else:
                        st.error(" DNSE account setup failed!")

    # Account Information
    st.markdown("####  Account Information")

    if st.session_state.get("broker_account_id"):
        st.info(f"**Current Account:** {st.session_state.broker_account_id}")

        # Account status check
        col1, col2 = st.columns(2)

        with col1:
            if st.button(" Check Account Status", use_container_width=True):
                st.success("Account is active and connected")

        with col2:
            if st.button(" Refresh Account Data", use_container_width=True):
                st.cache_data.clear()
                st.success("Account data refreshed")
    else:
        st.warning("Please set your broker account ID in the sidebar")

================
File: frontend/pages/order_history.py
================
import streamlit as st
import pandas as pd
from datetime import datetime
from ..services.trading import cancel_order


def order_history_page():
    """Order history and status page"""
    st.subheader(" Order History & Status")

    if not st.session_state.order_history:
        st.info("No orders placed yet")
        return

    # Order summary metrics
    orders = st.session_state.order_history
    pending_orders = [o for o in orders if o["status"] == "PENDING"]
    filled_orders = [o for o in orders if o["status"] == "FILLED"]
    cancelled_orders = [o for o in orders if o["status"] == "CANCELLED"]

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Total Orders", len(orders))

    with col2:
        st.metric("Pending", len(pending_orders))

    with col3:
        st.metric("Filled", len(filled_orders))

    with col4:
        st.metric("Cancelled", len(cancelled_orders))

    # Order history table
    df_orders = pd.DataFrame(orders)

    # Add action buttons
    for i, order in enumerate(orders):
        col1, col2, col3, col4, col5 = st.columns([2, 1, 1, 1, 1])

        with col1:
            status_class = f"status-{order['status'].lower()}"
            st.markdown(f"**{order['symbol']}** - {order['action']}")
            st.markdown(
                f"<span class='{status_class}'></span> {order['status']}",
                unsafe_allow_html=True,
            )

        with col2:
            st.write(f"Qty: {order['quantity']:,}")
            st.write(f"Price: ${order['price']:.2f}")

        with col3:
            st.write(f"Filled: {order['filled_quantity']:,}")
            st.write(f"Remaining: {order['remaining_quantity']:,}")

        with col4:
            order_time = datetime.fromisoformat(order["timestamp"])
            st.write(order_time.strftime("%m/%d %H:%M"))

        with col5:
            if order["status"] == "PENDING":
                if st.button("Cancel", key=f"cancel_{order['order_id']}"):
                    if cancel_order(order["order_id"]):
                        st.success("Order cancelled")
                        st.rerun()
                    else:
                        st.error("Failed to cancel")

        st.divider()

================
File: frontend/pages/portfolio_analysis.py
================
import streamlit as st
from datetime import datetime
from frontend.services.portfolio import PortfolioService

from frontend.components.analysis import render_portfolio_vs_account_comparison, render_portfolio_analysis_page
from frontend.utils.helpers import format_currency


def portfolio_analysis_page():
    """Main portfolio analysis page with enhanced functionality"""

    # Create tabs for different analysis views
    tab1, tab2 = st.tabs([" Enhanced Analysis", " Portfolio vs Account"])

    with tab1:
        render_portfolio_analysis_page()

    with tab2:
        render_portfolio_vs_account_comparison()

================
File: frontend/pages/portfolio_management.py
================
import streamlit as st

from frontend.components.management import (
    render_portfolio_list,
    render_create_portfolio_form,
)


def portfolio_management_page():
    """Main portfolio management page"""
    st.subheader(" Portfolio Management")

    tab1, tab2 = st.tabs([" My Portfolios", " Create New"])

    with tab1:
        render_portfolio_list()

    with tab2:
        render_create_portfolio_form()

================
File: frontend/pages/trade_execution.py
================
import streamlit as st
from ..services.trading import place_order_via_dnse


def trade_execution_page():
    """Trade execution and order management page"""
    st.subheader(" Trade Execution Center")

    # Quick order form
    st.markdown("####  Quick Order Entry")

    with st.form("quick_order_form"):
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            symbol = st.text_input("Symbol", placeholder="e.g., VIC")

        with col2:
            action = st.selectbox("Action", ["BUY", "SELL"])

        with col3:
            quantity = st.number_input("Quantity", min_value=1, value=100)

        with col4:
            price = st.number_input("Price", min_value=0.01, value=10.0, step=0.01)

        col_a, col_b, col_c = st.columns([1, 1, 1])
        with col_b:
            submit_order = st.form_submit_button(
                " Place Order", use_container_width=True
            )

        if submit_order and symbol and st.session_state.get("broker_account_id"):
            result = place_order_via_dnse(
                symbol=symbol,
                action=action,
                quantity=quantity,
                price=price,
                account_no=st.session_state.broker_account_id,
            )

            if result.get("status") in ["PENDING", "FILLED"]:
                st.success(f" Order placed! Order ID: {result['order_id']}")
            else:
                st.error(f" Order failed: {result.get('status', 'Unknown error')}")

================
File: frontend/services/api.py
================
import streamlit as st
import requests
from typing import Optional, Dict
from ..utils.config import API_BASE_URL
from .auth import get_auth_headers, handle_auth_error


def setup_dnse_account(custody_code: str, password: str) -> bool:
    """Setup DNSE account via API"""
    try:
        response = requests.post(
            f"{API_BASE_URL}/accounts-service/setup",
            json={"username": custody_code, "password": password},
            headers=get_auth_headers(),
            timeout=30,
        )

        if response.status_code == 200:
            return True
        elif handle_auth_error(response):
            return False
        else:
            error_msg = response.json().get("message", "Unknown error")
            st.error(f"Failed to setup DNSE account: {error_msg}")
            return False

    except requests.exceptions.RequestException as e:
        st.error(f"API connection error: {str(e)}")
        return False


def send_portfolio_notification(
    broker_account_id: str,
    strategy_type: str = "MarketNeutral",
    include_trade_plan: bool = True,
) -> bool:
    """Send portfolio notification to Telegram"""
    try:
        response = requests.post(
            f"{API_BASE_URL}/portfolio-service/notify/{broker_account_id}",
            headers=get_auth_headers(),
            params={
                "strategy_type": strategy_type,
                "include_trade_plan": include_trade_plan,
            },
            timeout=30,
        )
        return response.status_code == 200

    except requests.exceptions.RequestException as e:
        st.error(f"Failed to send notification: {str(e)}")
        return False

================
File: frontend/services/auth.py
================
"""
Authentication service for CMV Trading Bot frontend
"""

import streamlit as st
import requests
from typing import Optional, Dict
from ..utils.config import API_BASE_URL


def login_user(username: str, password: str) -> Optional[Dict]:
    try:
        response = requests.post(
            f"{API_BASE_URL}/auth-service/login",
            json={"account": username, "password": password},
            headers={"Content-Type": "application/json"},
            timeout=10,
        )

        if response.status_code == 200:
            return response.json().get("data")
        else:
            error_msg = response.json().get("message", "Unknown error")
            st.error(f"Login failed: {error_msg}")
            return None

    except requests.exceptions.RequestException as e:
        st.error(f"Connection error: {str(e)}")
        return None


def logout_user(logout_payload: Dict) -> bool:
    try:
        response = requests.post(
            f"{API_BASE_URL}/auth-service/logout",
            json=logout_payload,
            headers={"Content-Type": "application/json"},
            timeout=10,
        )
        return response.status_code == 200
    except:
        return False


def get_default_account() -> Optional[Dict]:
    try:
        response = requests.get(
            f"{API_BASE_URL}/accounts-service/default",
            headers=get_auth_headers(),
            timeout=10,
        )
        if response.status_code == 200:
            return response.json().get("data")
        else:
            st.error("Failed to fetch default account")
            return None
    except requests.exceptions.RequestException as e:
        st.error(f"Connection error: {str(e)}")
        return None


def get_auth_headers() -> Dict[str, str]:
    headers = {"Content-Type": "application/json"}
    debug_mode = st.session_state.get("debug_mode", False)

    if "auth_token" in st.session_state and st.session_state.auth_token:
        token = st.session_state.auth_token
        # Debug: Show token info in sidebar (first/last 10 chars only for security)
        if debug_mode:
            if len(token) > 20:
                st.sidebar.caption(f" Token: {token[:10]}...{token[-10:]}")
            else:
                st.sidebar.caption(f" Token: {token}")
        headers["Authorization"] = f"Bearer {token}"
    else:
        if debug_mode:
            st.sidebar.caption(" No valid token found")
            # Debug: Show what auth-related keys exist
            auth_keys = [
                k
                for k in st.session_state.keys()
                if "auth" in k.lower() or k in ["authenticated", "username"]
            ]
            if auth_keys:
                st.sidebar.caption(f" Auth keys: {auth_keys}")
            else:
                st.sidebar.caption(" No auth keys found at all")
    return headers


def handle_auth_error(response):
    if response.status_code == 401:
        debug_mode = st.session_state.get("debug_mode", False)
        if debug_mode:
            st.sidebar.error(f" Got 401 error from API")

        # Only clear auth-related session state, preserve other data
        auth_keys = ["auth_token", "refresh_token"]
        cleared_keys = []
        for key in auth_keys:
            if key in st.session_state:
                del st.session_state[key]
                cleared_keys.append(key)

        if cleared_keys and debug_mode:
            st.sidebar.caption(f" Cleared keys: {cleared_keys}")

        # Set authenticated to False instead of deleting everything
        st.session_state.authenticated = False
        st.error(" Authentication expired. Please log in again.")

        # Don't call st.rerun() here - let the main app handle the redirect
        # st.rerun() causes infinite loops
        return True
    return False

================
File: frontend/services/portfolio.py
================
import streamlit as st
import requests
from typing import Optional, Dict, List
from .auth import get_auth_headers, handle_auth_error
from ..utils.config import API_BASE_URL


class PortfolioService:
    @staticmethod
    def create_custom_portfolio(
        portfolio_name: str, symbols: List[str], description: str = None
    ) -> Dict:
        """Create a new custom portfolio"""
        try:
            response = requests.post(
                f"{API_BASE_URL}/portfolio-service/create",
                json={
                    "portfolio_name": portfolio_name,
                    "portfolio_desc": description,
                    "symbols": symbols,
                },
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 201:
                # Clear cache after successful creation
                PortfolioService.get_my_portfolios.clear()
                return {"success": True, "data": response.json().get("data")}
            elif handle_auth_error(response):
                return {"success": False, "error": "Authentication failed"}
            else:
                error_msg = response.json().get("message", "Unknown error")
                return {"success": False, "error": error_msg}

        except requests.exceptions.RequestException as e:
            return {"success": False, "error": f"Network error: {str(e)}"}

    @staticmethod
    @st.cache_data(ttl=60)
    def get_my_portfolios() -> Optional[List[Dict]]:
        try:
            response = requests.get(
                f"{API_BASE_URL}/portfolio-service/me",
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 200:
                return response.json().get("data", [])
            elif handle_auth_error(response):
                return None
            else:
                st.error(f"Failed to get portfolios: {response.json().get('message')}")
                return None

        except requests.exceptions.RequestException as e:
            st.error(f"Network error: {str(e)}")
            return None

    @staticmethod
    @st.cache_data(ttl=60)
    def get_system_portfolios() -> Optional[List[Dict]]:
        try:
            response = requests.get(
                f"{API_BASE_URL}/portfolio-service/system",
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 200:
                return response.json().get("data", [])
            elif handle_auth_error(response):
                return None
            else:
                st.error(f"Failed to get portfolios: {response.json().get('message')}")
                return None

        except requests.exceptions.RequestException as e:
            st.error(f"Network error: {str(e)}")
            return None

    @staticmethod
    @st.cache_data(ttl=60)
    def get_my_portfolio_by_id(portfolio_id: str) -> Optional[Dict]:
        try:
            response = requests.get(
                f"{API_BASE_URL}/portfolio-service/{portfolio_id}",
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 200:
                return response.json().get("data", [])
            elif handle_auth_error(response):
                return None
            else:
                st.error(f"Failed to get portfolios: {response.json().get('message')}")
                return None

        except requests.exceptions.RequestException as e:
            st.error(f"Network error: {str(e)}")
            return None

    @staticmethod
    @st.cache_data(ttl=30)
    def get_portfolio_analysis(
        broker_account_id: str, portfolio_id: str, strategy_type: str = "MarketNeutral"
    ) -> Optional[Dict]:
        """Get analysis for specific portfolio"""
        try:
            response = requests.post(
                f"{API_BASE_URL}/portfolio-service/analysis/{broker_account_id}",
                json={
                    "portfolio_id": portfolio_id,
                    "strategy_type": strategy_type
                },
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 200:
                return response.json().get("data")
            elif handle_auth_error(response):
                return None
            else:
                st.error(f"Failed to get analysis: {response.json().get('message')}")
                return None

        except requests.exceptions.RequestException as e:
            st.error(f"Network error: {str(e)}")
            return None

    @staticmethod
    def update_portfolio(portfolio_id: str, symbols: List[str]) -> Dict:
        """Update symbols in a custom portfolio"""
        try:
            response = requests.put(
                f"{API_BASE_URL}/portfolio-service/update/",
                json={"portfolio_id": portfolio_id, "symbols": symbols},
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 200:
                # Clear cache after successful update
                PortfolioService.get_my_portfolios.clear()
                return {"success": True, "data": response.json().get("data")}
            elif handle_auth_error(response):
                return {"success": False, "error": "Authentication failed"}
            else:
                error_msg = response.json().get("message", "Unknown error")
                return {"success": False, "error": error_msg}

        except requests.exceptions.RequestException as e:
            return {"success": False, "error": f"Network error: {str(e)}"}

    @staticmethod
    def delete_portfolio(portfolio_id: str) -> Dict:
        """Delete a custom portfolio"""
        try:
            response = requests.delete(
                f"{API_BASE_URL}/portfolio-service/{portfolio_id}",
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 200:
                # Clear cache after successful deletion
                PortfolioService.get_my_portfolios.clear()
                return {"success": True}
            elif handle_auth_error(response):
                return {"success": False, "error": "Authentication failed"}
            else:
                error_msg = response.json().get("message", "Unknown error")
                return {"success": False, "error": error_msg}

        except requests.exceptions.RequestException as e:
            return {"success": False, "error": f"Network error: {str(e)}"}

    @staticmethod
    @st.cache_data(ttl=300)
    def get_portfolio_pnl(
        portfolio_id: str, strategy: str = "LongOnly"
    ) -> Optional[Dict]:
        """Get portfolio PnL data for comparison with VN-Index"""
        try:
            response = requests.get(
                f"{API_BASE_URL}/portfolio-service/pnl/{portfolio_id}",
                params={"strategy": strategy},
                headers=get_auth_headers(),
                timeout=60,  # Longer timeout for PnL calculation
            )

            if response.status_code == 200:
                return response.json().get("data")
            elif handle_auth_error(response):
                return None
            else:
                error_msg = response.json().get("message", "Unknown error")
                st.error(f"Failed to get portfolio PnL: {error_msg}")
                return None

        except requests.exceptions.RequestException as e:
            st.error(f"Network error while fetching PnL: {str(e)}")
            return None

        except requests.exceptions.RequestException as e:
            return {"success": False, "error": f"Network error: {str(e)}"}

    @staticmethod
    @st.cache_data(ttl=300)
    def get_all_symbols() -> List[str]:
        vietnam_symbols = [
            "VIC",
            "VHM",
            "VRE",
            "TCB",
            "VCB",
            "BID",
            "CTG",
            "MBB",
            "HPG",
            "MSN",
            "VNM",
            "SAB",
            "GAS",
            "PLX",
            "POW",
            "REE",
            "GMD",
            "DXG",
            "KDH",
            "NVL",
            "PDR",
            "STB",
            "TPB",
            "ACB",
            "EIB",
            "HDB",
            "LPB",
            "NAB",
            "OCB",
            "SHB",
            "SSB",
            "VIB",
            "APG",
            "BCM",
            "BVH",
            "CII",
            "DGC",
            "FPT",
            "GEX",
            "HNG",
            "IDI",
            "IJC",
            "KBC",
            "MWG",
            "NLG",
            "PNJ",
            "ROS",
            "SSI",
        ]

        try:
            response = requests.get(
                f"{API_BASE_URL}/portfolio-service/symbols",
                headers=get_auth_headers(),
                timeout=30,
            )

            if response.status_code == 200:
                data = response.json().get("data", {})
                if len(data["records"]) > 0:
                    return data["records"]
                else:
                    return vietnam_symbols
            elif handle_auth_error(response):
                return vietnam_symbols
            else:
                return vietnam_symbols

        except requests.exceptions.RequestException as e:
            return vietnam_symbols

================
File: frontend/services/trading.py
================
import streamlit as st
import time
import random
from datetime import datetime
from typing import Dict, List
from ..utils.helpers import safe_numeric_value


def place_order_via_dnse(
    symbol: str, action: str, quantity: int, price: float, account_no: str
) -> Dict:
    """
    Place order via DNSE API (Simulated)
    In production, this would use your DNSE API client
    """
    # Simulate order placement with some validation
    order_id = f"DNSE_{int(time.time() * 1000)}"

    # Simulate order status based on various factors
    statuses = ["PENDING", "FILLED", "PARTIALLY_FILLED"]
    weights = [0.7, 0.2, 0.1]
    status = random.choices(statuses, weights=weights)[0]

    order = {
        "order_id": order_id,
        "symbol": symbol,
        "action": action,
        "quantity": quantity,
        "price": price,
        "account_no": account_no,
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "filled_quantity": (
            quantity
            if status == "FILLED"
            else (quantity // 2 if status == "PARTIALLY_FILLED" else 0)
        ),
        "remaining_quantity": (
            0
            if status == "FILLED"
            else (quantity // 2 if status == "PARTIALLY_FILLED" else quantity)
        ),
    }

    # Add to order history
    st.session_state.order_history.append(order)

    return order


def get_order_status(order_id: str) -> Dict:
    for order in st.session_state.order_history:
        if order["order_id"] == order_id:
            return order
    return {}


def cancel_order(order_id: str) -> bool:
    for order in st.session_state.order_history:
        if order["order_id"] == order_id and order["status"] == "PENDING":
            order["status"] = "CANCELLED"
            return True
    return False


def execute_single_order(recommendation: Dict, action_type: str):
    if not st.session_state.get("broker_account_id"):
        st.error("Please set broker account ID first")
        return

    with st.spinner(f"Executing {action_type} order for {recommendation['symbol']}..."):
        result = place_order_via_dnse(
            symbol=recommendation["symbol"],
            action=action_type,
            quantity=int(safe_numeric_value(recommendation.get("action_quantity", 0))),
            price=safe_numeric_value(recommendation.get("action_price", 0)),
            account_no=st.session_state.broker_account_id,
        )

        if result.get("status") in ["PENDING", "FILLED"]:
            st.success(f" Order placed successfully! Order ID: {result['order_id']}")
        else:
            st.error(f" Order failed: {result.get('status', 'Unknown error')}")


def execute_bulk_orders(recommendations: List[Dict]):
    if not st.session_state.get("broker_account_id"):
        st.error("Please set broker account ID first")
        return

    progress_bar = st.progress(0)
    status_text = st.empty()

    results = []
    successful_orders = 0

    for i, rec in enumerate(recommendations):
        status_text.text(
            f"Executing order {i+1}/{len(recommendations)}: {rec['symbol']}"
        )

        result = place_order_via_dnse(
            symbol=rec["symbol"],
            action=rec["action"],
            quantity=int(safe_numeric_value(rec.get("action_quantity", 0))),
            price=safe_numeric_value(rec.get("action_price", 0)),
            account_no=st.session_state.broker_account_id,
        )

        results.append(result)
        if result.get("status") in ["PENDING", "FILLED"]:
            successful_orders += 1

        progress_bar.progress((i + 1) / len(recommendations))
        time.sleep(0.5)  # Simulate API delay

    status_text.empty()
    progress_bar.empty()

    # Display results summary
    st.subheader(" Execution Summary")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Orders", len(results))
    with col2:
        st.metric("Successful", successful_orders)
    with col3:
        st.metric("Failed", len(results) - successful_orders)

    # Detailed results
    for result in results:
        status = result.get("status", "UNKNOWN")
        if status in ["PENDING", "FILLED"]:
            st.success(f" {result['symbol']}: Order {result['order_id']} - {status}")
        else:
            st.error(f" {result['symbol']}: Order failed - {status}")

================
File: frontend/styles/main.py
================
MAIN_CSS = """
<style>

   /* Original styles */
   .main-header {
       background: linear-gradient(135deg, #006064 0%, #00bcd4 100%);
       padding: 1.5rem;
       border-radius: 15px;
       color: white;
       text-align: center;
       margin-bottom: 2rem;
       box-shadow: 0 4px 6px rgba(0,0,0,0.1);
   }
   
   .metric-card {
       background: linear-gradient(135deg, #1e1e1e 0%, #2d2d30 100%);
       padding: 1.5rem;
       border-radius: 12px;
       box-shadow: 0 4px 6px rgba(0,0,0,0.1);
       border-left: 5px solid #00bcd4;
       margin-bottom: 1rem;
       transition: transform 0.2s;
       color: #fafafa;
   }
   
   .metric-card:hover {
       transform: translateY(-2px);
   }
   
   /* Enhanced Button Styles */
   .stButton > button {
       border-radius: 8px;
       border: none;
       background: linear-gradient(135deg, #00bcd4 0%, #006064 100%);
       color: white;
       padding: 0.5rem 1rem;
       transition: all 0.2s;
       font-weight: 500;
   }
   
   .stButton > button:hover {
       transform: translateY(-2px);
       box-shadow: 0 4px 8px rgba(0,188,212,0.3);
   }
   
   /* Tab Styling */
   .stTabs [data-baseweb="tab-list"] {
       gap: 8px;
   }
   
   .stTabs [data-baseweb="tab"] {
       border-radius: 8px 8px 0 0;
       background: linear-gradient(135deg, #2d2d30 0%, #1e1e1e 100%);
       border: 1px solid #444;
       color: #fafafa;
       font-weight: 500;
       padding: 8px 16px !important;  /* Thm padding  to khong cch */
       min-width: auto;
   }
   
   .stTabs [aria-selected="true"] {
       background: linear-gradient(135deg, #00bcd4 0%, #006064 100%);
       color: white;
       border-color: #00bcd4;
   }
   
   /* Form Styling */
   .stTextInput > div > div > input {
       border-radius: 8px;
       border: 2px solid #444;
       background-color: #2d2d30;
       color: #fafafa;
       transition: border-color 0.2s;
   }
   
   .stTextInput > div > div > input:focus {
       border-color: #00bcd4;
       box-shadow: 0 0 0 0.2rem rgba(0,188,212,0.25);
   }

    div[data-testid="InputInstructions"] {
        display: none !important;
    }
    
   .stSelectbox > div > div > div {
       border-radius: 8px;
       border: 2px solid #444;
       background-color: #2d2d30;
       color: #fafafa;
   }
   
   .stTextArea > div > div > textarea {
       border-radius: 8px;
       border: 2px solid #444;
       background-color: #2d2d30;
       color: #fafafa;
   }

   /* Sidebar Enhancement */
   .sidebar .sidebar-content {
       background: linear-gradient(135deg, #2d2d30 0%, #1e1e1e 100%);
   }
    .sidebar-section {
        margin-bottom: 1rem;
    }
    
    .compact-input {
        margin-bottom: 0.5rem;
    }
    
    .sidebar .stExpander {
        border: 1px solid #00bcd4;
        border-radius: 8px;
        margin-bottom: 0.5rem;
        background-color: #2d2d30;
    }
   
   /* Alert Boxes */
   .alert-success {
       background: linear-gradient(135deg, #1b4332 0%, #2d5a4a 100%);
       border: 1px solid #40916c;
       color: #52b788;
       padding: 1rem;
       border-radius: 8px;
       margin: 1rem 0;
   }
   
   .alert-warning {
       background: linear-gradient(135deg, #663300 0%, #995500 100%);
       border: 1px solid #cc7700;
       color: #ff9900;
       padding: 1rem;
       border-radius: 8px;
       margin: 1rem 0;
   }
   
   .alert-error {
       background: linear-gradient(135deg, #5c1e24 0%, #8b1c23 100%);
       border: 1px solid #c13030;
       color: #ff5252;
       padding: 1rem;
       border-radius: 8px;
       margin: 1rem 0;
   }
   
   .alert-info {
       background: linear-gradient(135deg, #004d5c 0%, #006064 100%);
       border: 1px solid #00bcd4;
       color: #26c6da;
       padding: 1rem;
       border-radius: 8px;
       margin: 1rem 0;
   }
   
   /* Loading Spinner */
   .loading-container {
       display: flex;
       justify-content: center;
       align-items: center;
       padding: 2rem;
   }
   
   .spinner {
       border: 4px solid #444;
       border-top: 4px solid #00bcd4;
       border-radius: 50%;
       width: 40px;
       height: 40px;
       animation: spin 1s linear infinite;
   }
   
   @keyframes spin {
       0% { transform: rotate(0deg); }
       100% { transform: rotate(360deg); }
   }

   /* Portfolio Cards */
   .portfolio-card {
       background: linear-gradient(135deg, #2d2d30 0%, #1e1e1e 100%);
       border: 1px solid #00bcd4;
       border-radius: 12px;
       padding: 1.5rem;
       margin: 1rem 0;
       box-shadow: 0 4px 6px rgba(0,0,0,0.3);
       transition: all 0.3s ease;
       color: #fafafa;
   }
   
   .portfolio-card:hover {
       transform: translateY(-5px);
       box-shadow: 0 8px 15px rgba(0,188,212,0.2);
   }
   
   .portfolio-card.system {
       border-left: 5px solid #00bcd4;
       background: linear-gradient(135deg, #1a4449 0%, #1e3a3f 100%);
   }
   
   .portfolio-card.custom {
       border-left: 5px solid #4caf50;
       background: linear-gradient(135deg, #1b2f1e 0%, #1f3023 100%);
   }
   
   /* Portfolio Stats */
   .portfolio-stats {
       display: flex;
       justify-content: space-around;
       background: rgba(45,45,48,0.8);
       border-radius: 8px;
       padding: 1rem;
       margin: 1rem 0;
       border: 1px solid #00bcd4;
   }
   
   .stat-item {
       text-align: center;
       flex: 1;
   }
   
   .stat-value {
       font-size: 1.5rem;
       font-weight: bold;
       color: #00bcd4;
   }
   
   .stat-label {
       font-size: 0.9rem;
       color: #bdbdbd;
       margin-top: 0.25rem;
   }
   
   /* Symbol Tags */
   .symbol-tag {
       display: inline-block;
       background: linear-gradient(135deg, #00bcd4 0%, #006064 100%);
       color: white;
       padding: 0.25rem 0.75rem;
       border-radius: 20px;
       font-size: 0.85rem;
       font-weight: 500;
       margin: 0.25rem;
       transition: all 0.2s ease;
   }
   
   .symbol-tag:hover {
       transform: scale(1.05);
       box-shadow: 0 2px 8px rgba(0,188,212,0.3);
   }
   
   .symbol-tag.removable {
       background: linear-gradient(135deg, #f44336 0%, #d32f2f 100%);
       cursor: pointer;
   }
   
   .symbol-tag.addable {
       background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);
       cursor: pointer;
   }
   
   /* Portfolio Creation Form */
   .portfolio-form {
       background: linear-gradient(135deg, #2d2d30 0%, #1e1e1e 100%);
       border: 1px solid #00bcd4;
       border-radius: 12px;
       padding: 2rem;
       margin: 1rem 0;
       color: #fafafa;
   }
   
   .form-section {
       margin-bottom: 1.5rem;
       padding-bottom: 1rem;
       border-bottom: 1px solid #444;
   }
   
   .form-section:last-child {
       border-bottom: none;
       margin-bottom: 0;
   }
   
   /* Symbol Search */
   .symbol-search-results {
       background: #2d2d30;
       border: 1px solid #00bcd4;
       border-radius: 8px;
       padding: 1rem;
       margin: 0.5rem 0;
       max-height: 200px;
       overflow-y: auto;
       color: #fafafa;
   }
   
   .symbol-search-item {
       background: #1e1e1e;
       border: 1px solid #444;
       border-radius: 6px;
       padding: 0.5rem 1rem;
       margin: 0.25rem;
       display: inline-block;
       cursor: pointer;
       transition: all 0.2s ease;
       color: #fafafa;
   }
   
   .symbol-search-item:hover {
       background: #006064;
       border-color: #00bcd4;
       transform: translateY(-1px);
   }
   
   /* Portfolio Comparison */
   .comparison-table {
       background: white;
       border-radius: 12px;
       overflow: hidden;
       box-shadow: 0 4px 6px rgba(0,0,0,0.1);
       margin: 1rem 0;
   }
   
   .comparison-header {
       background: linear-gradient(135deg, #2a5298 0%, #1e3c72 100%);
       color: white;
       padding: 1rem;
       font-weight: bold;
   }
   
   .comparison-row {
       padding: 0.75rem 1rem;
       border-bottom: 1px solid #e9ecef;
       transition: background-color 0.2s ease;
   }
   
   .comparison-row:hover {
       background-color: #f8f9fa;
   }
   
   .comparison-row:last-child {
       border-bottom: none;
   }
   
   /* Action Buttons */
   .action-button {
       border-radius: 8px;
       border: none;
       padding: 0.5rem 1rem;
       font-weight: 500;
       transition: all 0.2s ease;
       cursor: pointer;
       margin: 0.25rem;
   }
   
   .action-button.primary {
       background: linear-gradient(135deg, #2a5298 0%, #1e3c72 100%);
       color: white;
   }
   
   .action-button.primary:hover {
       transform: translateY(-2px);
       box-shadow: 0 4px 8px rgba(42,82,152,0.3);
   }
   
   .action-button.success {
       background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);
       color: white;
   }
   
   .action-button.success:hover {
       transform: translateY(-2px);
       box-shadow: 0 4px 8px rgba(76,175,80,0.3);
   }
   
   .action-button.danger {
       background: linear-gradient(135deg, #f44336 0%, #d32f2f 100%);
       color: white;
   }
   
   .action-button.danger:hover {
       transform: translateY(-2px);
       box-shadow: 0 4px 8px rgba(244,67,54,0.3);
   }
   
   .action-button.secondary {
       background: linear-gradient(135deg, #6c757d 0%, #495057 100%);
       color: white;
   }
   
   /* Portfolio Performance Charts */
   .performance-chart-container {
       background: white;
       border-radius: 12px;
       padding: 1.5rem;
       margin: 1rem 0;
       box-shadow: 0 4px 6px rgba(0,0,0,0.1);
   }
   
   /* Template Cards */
   .template-card {
       background: linear-gradient(135deg, #e8f5e8 0%, #f1f8e9 100%);
       border: 1px solid #c8e6c9;
       border-radius: 8px;
       padding: 1rem;
       margin: 0.5rem 0;
       cursor: pointer;
       transition: all 0.2s ease;
   }
   
   .template-card:hover {
       border-color: #4caf50;
       transform: translateY(-2px);
       box-shadow: 0 4px 8px rgba(76,175,80,0.2);
   }
   
   .template-name {
       font-weight: bold;
       color: #2e7d32;
       margin-bottom: 0.5rem;
   }
   
   .template-symbols {
       font-size: 0.9rem;
       color: #424242;
   }
   
   /* Status Indicators */
   .status-active {
       color: #4caf50;
       font-weight: bold;
   }
   
   .status-inactive {
       color: #f44336;
       font-weight: bold;
   }
   
   .status-system {
       color: #2a5298;
       font-weight: bold;
   }
   
   /* Responsive Design */
   @media (max-width: 768px) {
       .portfolio-card {
           padding: 1rem;
           margin: 0.5rem 0;
       }
       
       .portfolio-stats {
           flex-direction: column;
       }
       
       .stat-item {
           margin: 0.5rem 0;
       }
       
       .symbol-tag {
           font-size: 0.8rem;
           padding: 0.2rem 0.5rem;
       }
       
       .action-button {
           width: 100%;
           margin: 0.25rem 0;
       }
   }
   
   /* Animation Classes */
   .fade-in {
       animation: fadeIn 0.5s ease-in;
   }
   
   @keyframes fadeIn {
       from { opacity: 0; transform: translateY(20px); }
       to { opacity: 1; transform: translateY(0); }
   }
   
   .slide-in {
       animation: slideIn 0.3s ease-out;
   }
   
   @keyframes slideIn {
       from { transform: translateX(-100%); }
       to { transform: translateX(0); }
   }
   
   /* Loading States */
   .loading-skeleton {
       background: linear-gradient(90deg, #f0f0f0 25%, #e0e0e0 50%, #f0f0f0 75%);
       background-size: 200% 100%;
       animation: loading 1.5s infinite;
   }
   
   @keyframes loading {
       0% { background-position: 200% 0; }
       100% { background-position: -200% 0; }
   }
   
   /* Tooltip Styles */
   .tooltip {
       position: relative;
       display: inline-block;
   }
   
   .tooltip .tooltiptext {
       visibility: hidden;
       width: 200px;
       background-color: #333;
       color: #fff;
       text-align: center;
       border-radius: 6px;
       padding: 5px 10px;
       position: absolute;
       z-index: 1;
       bottom: 125%;
       left: 50%;
       margin-left: -100px;
       opacity: 0;
       transition: opacity 0.3s;
       font-size: 0.85rem;
   }
   
   .tooltip:hover .tooltiptext {
       visibility: visible;
       opacity: 1;
   }
</style>
"""

================
File: frontend/utils/__init__.py
================
"""
Frontend utilities for CMV Trading Bot
"""

================
File: frontend/utils/config.py
================
import os


# API Configuration
API_BASE_URL = os.getenv("BACKEND_API_URL", "http://localhost:8000/api/v1")

# Streamlit Configuration
STREAMLIT_CONFIG = {
    "page_title": "CMV Portfolio Management",
    "page_icon": "",
    "layout": "wide",
    "initial_sidebar_state": "expanded",
}

# Application Settings
APP_SETTINGS = {
    "auto_refresh_interval": 30,  # seconds
    "order_simulation_delay": 0.5,  # seconds
    "cache_ttl": 30,  # seconds
}

================
File: frontend/utils/helpers.py
================
import streamlit as st
from typing import Dict, Any


def format_currency(amount: float) -> str:
    """Format currency with proper formatting"""
    return f"{amount:,.1f}"  


def format_percentage(percentage: float) -> str:
    """Format percentage with proper formatting"""
    return f"{percentage:.1f}%"


def safe_numeric_value(value, default=0) -> float:
    """Safely extract numeric value from various data types"""
    if value is None:
        return default
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        try:
            return float(value)
        except (ValueError, TypeError):
            return default
    if isinstance(value, dict):
        # If it's a dict, try to extract a numeric value from common keys
        for key in ["value", "amount", "total", "price", "weight", "percentage"]:
            if key in value:
                return safe_numeric_value(value[key], default)
            
        return default
    return default


def get_priority_color(priority: str) -> str:
    """Get color based on priority"""
    colors = {"HIGH": "#f44336", "MEDIUM": "#ff9800", "LOW": "#4caf50"}
    return colors.get(priority.upper(), "#6c757d")


def track_session_state_change(key: str, old_value=None, new_value=None):
    """Track when important session state changes"""

    debug_mode = st.session_state.get("debug_mode", False)
    if debug_mode:
        st.sidebar.caption(f" {key}: {old_value}  {new_value}")


def preserve_auth_state():
    auth_backup = {
        "authenticated": st.session_state.get("authenticated", False),
        "auth_token": st.session_state.get("auth_token"),
        "refresh_token": st.session_state.get("refresh_token"),
        "username": st.session_state.get("username"),
    }
    return auth_backup


def restore_auth_state(auth_backup: dict):
    for key, value in auth_backup.items():
        if value is not None:
            st.session_state[key] = value


def init_session_state():
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if "order_history" not in st.session_state:
        st.session_state.order_history = []
    if "selected_recommendations" not in st.session_state:
        st.session_state.selected_recommendations = []

    if "current_page" not in st.session_state:
        st.session_state.current_page = "Portfolio Analysis"

    if "token_missing_count" not in st.session_state:
        st.session_state.token_missing_count = 0

    if "broker_account_id" not in st.session_state:
        st.session_state.broker_account_id = None
    if "account_name" not in st.session_state:
        st.session_state.account_name = None
    if "broker_name" not in st.session_state:
        st.session_state.broker_name = None
    if "broker_investor_id" not in st.session_state:
        st.session_state.broker_investor_id = None

    if st.session_state.get("authenticated", False):
        if not st.session_state.get("auth_token"):
            st.session_state.token_missing_count += 1

    if "selected_symbols" not in st.session_state:
        st.session_state.selected_symbols = []
    
    if "portfolios" not in st.session_state:
        st.session_state.portfolios = []
    
    if "current_portfolio_id" not in st.session_state:
        st.session_state.current_portfolio_id = None
    
    if "portfolio_analysis_cache" not in st.session_state:
        st.session_state.portfolio_analysis_cache = {}

================
File: pipeline_daily.py
================
import asyncio
import sys
from pathlib import Path

from backend.modules.portfolio.services.portfolio_daily_pipeline_service import DailyDataPipelineService
from backend.utils.logger import LOGGER


async def main():
    """Main entry point for the daily data pipeline service"""
    try:
        LOGGER.info("Starting Daily Data Pipeline Service...")
        LOGGER.info("Pipeline will run automatically at 7:00 PM Vietnam time daily")

        # Start the scheduler
        await DailyDataPipelineService.schedule_daily_run()

    except KeyboardInterrupt:
        LOGGER.info("Daily Data Pipeline Service stopped by user")
    except Exception as e:
        LOGGER.error(f"Fatal error in Daily Data Pipeline Service: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())

================
File: pipeline_manual.py
================
import asyncio

from backend.modules.portfolio.services.portfolio_daily_pipeline_service import DailyDataPipelineService
from backend.utils.logger import LOGGER


async def main():
    try:
        LOGGER.info("Starting Daily Data Pipeline Service...")
        LOGGER.info("Pipeline will run automatically at 7:00 PM Vietnam time daily")

        # Start the scheduler
        await DailyDataPipelineService.run_manual()

    except KeyboardInterrupt:
        LOGGER.info("Daily Data Pipeline Service stopped by user")
    except Exception as e:
        LOGGER.error(f"Fatal error in Daily Data Pipeline Service: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())

================
File: server.py
================
import argparse
import uvicorn
from uvicorn.config import LOGGING_CONFIG
from backend.app import app

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("-a", "--app", required=True, help="app name")
    ap.add_argument("-p", "--port", required=False, help="port", default="8000")
    ap.add_argument("-w", "--workers", required=False, help="number workers", default="1")
    args = vars(ap.parse_args())
    LOGGING_CONFIG["formatters"]["default"]["fmt"] = "%(asctime)s [%(name)s] %(levelprefix)s %(message)s"
    uvicorn.run(args["app"], host="0.0.0.0", port=int(args["port"]), workers=int(args["workers"]))

================
File: test/export_portfolio.py
================
from backend.db.sessions import backend_session_scope
import os
import pandas as pd
from dotenv import load_dotenv


def test():
    with backend_session_scope() as session:
        sql = """
            SELECT [id]
                ,[date]
                ,[symbol]
                ,[initialWeight]
                ,[neutralizedWeight]
            FROM [CMVTradingBot].[BotPortfolio].[portfolios]
            ORDER BY [date] ASC
        """

        records = session.connection().exec_driver_sql(sql)

        # Organize data by month
        monthly_data = {}
        for record in records:
            date_str = record[1]
            symbol = record[2]
            initial_weight = record[3]
            neutralized_weight = record[4]

            # Convert string date to datetime object
            date = pd.to_datetime(date_str)

            # Extract year-month for grouping
            year_month = f"{date.year}-{date.month:02d}"

            if year_month not in monthly_data:
                monthly_data[year_month] = []

            monthly_data[year_month].append(
                {
                    "date": date,  # Use converted datetime object
                    "symbol": symbol,
                    "initialWeight": (
                        float(initial_weight) if initial_weight is not None else None
                    ),
                    "neutralizedWeight": (
                        float(neutralized_weight)
                        if neutralized_weight is not None
                        else None
                    ),
                }
            )

        # Create DataFrame for each month and store in dictionary
        excel_data = {}
        for year_month in sorted(monthly_data.keys()):
            # Convert to DataFrame
            df_month = pd.DataFrame(monthly_data[year_month])

            # Get all unique symbols for this month
            all_symbols = sorted(df_month["symbol"].unique())

            # Create multi-level columns: (symbol, weight_type)
            columns = []
            for symbol in all_symbols:
                columns.extend(
                    [(symbol, "initialWeight"), (symbol, "neutralizedWeight")]
                )

            # Create DataFrame with date as index and multi-level columns
            dates = sorted(df_month["date"].unique())
            result_df = pd.DataFrame(
                index=dates, columns=pd.MultiIndex.from_tuples(columns)
            )

            # Fill the DataFrame
            for _, row in df_month.iterrows():
                date = row["date"]
                symbol = row["symbol"]
                initial_weight = row["initialWeight"]
                neutralized_weight = row["neutralizedWeight"]

                # Ensure values are numeric (float) for Excel
                result_df.loc[date, (symbol, "initialWeight")] = (
                    float(initial_weight) if pd.notna(initial_weight) else None
                )
                result_df.loc[date, (symbol, "neutralizedWeight")] = (
                    float(neutralized_weight) if pd.notna(neutralized_weight) else None
                )

            # Convert all columns to numeric, ensuring proper data types for Excel
            for col in result_df.columns:
                result_df[col] = pd.to_numeric(result_df[col], errors="coerce")

            # Round to 3 decimal places
            result_df = result_df.round(3)

            # Store in dictionary for Excel export
            excel_data[year_month] = result_df

            print(f"Added {year_month} to Excel data")

        # Export all months to single Excel file with multiple sheets
        excel_filename = "tmp/portfolio_weights_all_months_2.xlsx"
        with pd.ExcelWriter(excel_filename, engine="openpyxl") as writer:
            for sheet_name, df in excel_data.items():
                # Ensure numeric data types before writing to Excel
                df_copy = df.copy()

                # Convert all data to float64 to ensure Excel recognizes as numbers
                for col in df_copy.columns:
                    df_copy[col] = pd.to_numeric(df_copy[col], errors="coerce").astype(
                        "float64"
                    )

                df_copy.to_excel(writer, sheet_name=sheet_name)

        print(
            f"\n Successfully exported to {excel_filename} with {len(excel_data)} sheets"
        )


if __name__ == "__main__":
    # main()
    test()

================
File: test/test_cmv.py
================
import time
import pandas as pd
import numpy as np

import matplotlib
matplotlib.use('TkAgg')

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from scipy.ndimage import gaussian_filter1d
import cvxpy as cp

# Read the CSV file
file_path = 'adjusted_close_price_5y.csv'
df = pd.read_csv(file_path)


start_time = time.time()
pivoted_df = df.pivot(index='date', columns='ticker', values='closePriceAdjusted')
T = pivoted_df.shape[0]
n_assets = pivoted_df.shape[1]


# covariance_matrix = pivoted_df.cov()
# pct_change_df = pivoted_df.pct_change(fill_method=None).fillna(0)
# pct_change_df.columns = [f"{col}_pct_change" for col in pct_change_df.columns]
# expected_return_df = pct_change_df.ewm(span=20, adjust=False).mean()
# expected_return_df.columns = [f"{col}_expected_return" for col in expected_return_df.columns]
# result_df = pd.concat([pivoted_df, pct_change_df, expected_return_df], axis=1)
# elapsed_time = time.time() - start_time
# print(f"Pivoting and calculating percentage change took {elapsed_time:.5f} seconds")

days = 63
# calculate cov matrix every 63 days
x_t = []
mu_t = []
entropy_t = []
expected_return_t = []
portfolio_variance_t = []
for i in range(days - 1, T):
    window = pivoted_df.iloc[i - days + 1:i + 1, :]
    returns_df = window.pct_change(fill_method=None).fillna(0)
    expected_returns_df = returns_df.ewm(span=21, adjust=False).mean()
    Q = window.cov().values
    mu = expected_returns_df.mean().values
    lambda_ = 0.01

    A = np.linalg.pinv(Q)  # S dng nghch o Moore-Penrose  n nh hn
    B = np.sum(A)
    C = A @ mu
    C_sum = np.sum(C)

    nu = (2 * lambda_ * (C_sum - 1)) / B

    x = (1 / (2 * lambda_)) * A @ (mu - nu * np.ones(n_assets))

    # m bo trng s khng m (iu kin khng bn khng)
    x = np.clip(x, 0, None)

    # Chun ha li  tng trng s bng 1
    x_sum = np.sum(x)
    if x_sum > 1e-12:
        x_CEMV = x / x_sum
        # m bo tng chnh xc bng 1 bng cch iu chnh phn t ln nht
        x_CEMV = x_CEMV / np.sum(x_CEMV)

        # Kim tra v iu chnh nu vn c sai s
        if np.sum(x_CEMV) != 1.0:
            # Tm phn t ln nht v iu chnh
            max_idx = np.argmax(x_CEMV)
            x_CEMV[max_idx] += 1.0 - np.sum(x_CEMV)
    else:
        # Nu tt c trng s u gn bng 0, phn phi u
        x_CEMV = np.ones(n_assets) / n_assets

    # m bo khng c trng s m (do iu chnh)
    x_CEMV = np.clip(x_CEMV, 0, None)

    # Chun ha ln cui  m bo tng = 1
    x_CEMV = x_CEMV / np.sum(x_CEMV)

    # Entropy
    eps = 1e-12
    p = x_CEMV / (np.sum(x_CEMV) + eps)
    p = np.clip(p, 1e-10, None)
    entropy = -np.sum(p * np.log(p + eps))
    entropy_t.append(entropy)

    x_t.append(x_CEMV)
    mu_t.append(mu)
    portfolio_variance_t.append(x_CEMV.T @ Q @ x_CEMV)
    expected_return_t.append(mu @ x_CEMV)


# Convert to numpy arrays
x_t = np.array(x_t)
entropy_t = np.array(entropy_t)
expected_return_t = np.array(expected_return_t)
portfolio_variance_t = np.array(portfolio_variance_t)
mu_t = np.array(mu_t)

final_sums = np.sum(x_t, axis=1)  # Sum across assets for each time step
if np.any(final_sums > 1):
    print("There exists a time step where the sum of x_t is greater than 1.")
    print(final_sums)
    print(final_sums[final_sums > 1])
    print("Debug")
else:
    print("The sum of x_t is less than or equal to 1 for all time steps.")

# === Plotting ===
# fig = plt.figure(figsize=(16, 10))
# gs = gridspec.GridSpec(3, 3, figure=fig)
#
# ax1 = fig.add_subplot(gs[0, 0:3])
# ax2 = fig.add_subplot(gs[1, 0:3])
# ax3 = fig.add_subplot(gs[2, 0])
# ax4 = fig.add_subplot(gs[2, 1])
# ax5 = fig.add_subplot(gs[2, 2])
#
# markers = ['o', 's', '^', 'D', 'x']
# # Ensure time_steps matches the number of rows in x_t
# time_steps = np.arange(x_t.shape[0])

# # Subplot 1: Portfolio Weights
# bottom = np.zeros(x_t.shape[0])
# for i in range(n_assets):
#     ax1.bar(time_steps, x_t[:, i], bottom=bottom, label=f"Asset {i+1}", alpha=0.6)
#     bottom += x_t[:, i]
#     smoothed = gaussian_filter1d(x_t[:, i], sigma=5)
#     ax1.plot(time_steps, smoothed, lw=2, linestyle='-', marker=markers[i], markersize=5)
#
# ax1.set_xlabel("Time")
# ax1.set_ylabel("Weight")
# ax1.legend(ncol=8, loc='lower left', bbox_to_anchor=(0, 1))
# ax1.grid(True)
#
#
# # Subplot 2: Expected Returns
# bar_width = 0.1
# for i in range(n_assets):
#     offset = (i - n_assets / 2) * bar_width
#     ax2.bar(time_steps + offset, mu_t[:, i], width=bar_width, label=f"Asset {i+1}", alpha=0.6)
#     smoothed = gaussian_filter1d(mu_t[:, i], sigma=5)
#     ax2.plot(time_steps, smoothed, lw=2, linestyle='-', marker=markers[i], markersize=5)
#
# ax2.set_xlabel("Time")
# ax2.set_ylabel("Expected Return")
# ax2.legend(ncol=2, loc='lower left')
# ax2.grid(True)
#
# # Subplot 3: Entropy
# ax3.plot(entropy_t, label="Entropy", color="darkgreen", marker='x', markersize=3)
# ax3.set_title("(A)", fontsize=12, weight='bold')
# ax3.set_xlabel("Time", fontsize=12, weight='bold')
# ax3.legend()
# ax3.grid(True)
#
# # Subplot 4: Return and Risk
# ax4.plot(expected_return_t, label="Expected Return", color="blue", marker='o', markersize=3)
# ax4.plot(portfolio_variance_t, label="Portfolio Variance", color="red", marker='x', markersize=3)
# ax4.set_title("(B)", fontsize=12, weight='bold')
# ax4.set_xlabel("Time", fontsize=12, weight='bold')
# ax4.legend()
# ax4.grid(True)
#
# # Subplot 5: Efficient Frontier
# sc = ax5.scatter(portfolio_variance_t, expected_return_t, c=range(T), cmap='coolwarm', alpha=0.8)
# fig.colorbar(sc, ax=ax5, label='Time Step')
# ax5.set_title("(C)", fontsize=12, weight='bold')
# ax5.set_xlabel("Portfolio Variance (Risk)", fontsize=12, weight='bold')
# ax5.set_ylabel("Expected Return", fontsize=12, weight='bold')
# ax5.grid(True)
#
# plt.tight_layout()
# plt.show()

print("DONE")

================
File: test/test_daily_portfolio_notification.py
================
import asyncio

from backend.modules.portfolio.services import PortfolioNotificationService


if __name__ == "__main__":
    # For testing
    async def main():
        await PortfolioNotificationService.send_daily_system_portfolio()

        # Or start the scheduler (uncomment to run continuously)
        # await service.run_scheduler()

    asyncio.run(main())

================
File: test/test_dnse_session.py
================
import asyncio
import os
from enum import Enum

from backend.modules.dnse.trading_session import TradingSession
from backend.utils.logger import LOGGER


class OrderSide(Enum):
    BUY = "NB"
    SELL = "NS"


class OrderType(Enum):
    MARKET = "MP"
    LIMIT = "LO"


async def main_with_session():
    username = os.getenv("TEST_DNSE_ACCOUNT")
    password = os.getenv("TEST_DNSE_PASSWORD")
    async with TradingSession(account=username) as session:

        if await session.authenticate(password=password):

            auth_status = session.get_auth_status()
            print(f"Auth status: {auth_status}")

            # if not session.trading_token:
            #     await session.send_otp()
            #     otp = input("Enter OTP: ")
            #     await session.complete_auth(otp)

        if session.is_jwt_authenticated():
            async with session.users_client() as users_client:
                users_info = await users_client.get_users_info()
                accounts_info = await users_client.get_user_accounts()
                deals_info = await users_client.get_account_deals(
                    account_no="0001895009"
                )
                # buying_power = await users_client.get_buying_power(
                #     account_no=accounts_info["default"]["id"],
                #     symbol="OCB",
                #     price=12000,
                #     load_package_id="1036",
                # )

        else:
            print(" No JWT token - cannot access user info")

        if session.is_fully_authenticated():
            async with session.orders_client() as orders_client:
                # Place an order
                order = await orders_client.place_order(
                    account_no=accounts_info["default"]["id"],
                    side=OrderSide.BUY.value,
                    order_type=OrderType.LIMIT.value,
                    symbol="MBS",
                    price=35200,
                    quantity=1,
                    loan_package_id="1036"
                )
                if order:
                    print(f" Order placed successfully!")
                else:
                    print(" Failed to place order")

                # Get order book and order detail
                order_books = await orders_client.get_order_book(
                    account_no=accounts_info["default"]["id"]
                )
                if order_books:
                    print(f" Order books retrieved successfully!")
                else:
                    print(" Failed to retrieve order books")

                order_detail = await orders_client.get_order_detail(
                    order_id=order["id"],
                    account_no=accounts_info["default"]["id"]
                )
                if order_detail:
                    print(f" Order detail retrieved successfully!")
                else:
                    print(" Failed to retrieve order detail")

                # Cancel an order
                cancel_order = await orders_client.cancel_order(
                    order_id=order["id"],
                    account_no=accounts_info["default"]["id"]
                )
                if cancel_order:
                    print(f" Order cancelled successfully!")
                else:
                    print(" Failed to cancel order")

            print(" Trading session completed successfully!")
        else:
            print(" No trading token - cannot place orders")


if __name__ == "__main__":
    asyncio.run(main_with_session())

================
File: test/test_pnl_chart.py
================
import time
import pandas as pd
import numpy as np
import cvxpy as cp
from scipy.linalg import pinv, LinAlgError

import matplotlib

matplotlib.use('TkAgg')

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from scipy.ndimage import gaussian_filter1d


def make_psd(matrix, min_eigenvalue=1e-8):
    """
    Make a matrix positive semi-definite by adjusting eigenvalues
    """
    try:
        eigenvals, eigenvecs = np.linalg.eigh(matrix)
        eigenvals = np.maximum(eigenvals, min_eigenvalue)
        return eigenvecs @ np.diag(eigenvals) @ eigenvecs.T
    except:
        # Fallback: add small diagonal
        return matrix + np.eye(matrix.shape[0]) * min_eigenvalue


def solve_portfolio_cvxpy(mu, Q, lambda_):
    """
    Solve portfolio optimization using CVXPY with robust handling
    """
    n_assets = len(mu)

    try:
        # Make Q positive semi-definite
        Q_psd = make_psd(Q)

        # Define optimization variable
        x = cp.Variable(n_assets)

        # Objective: maximize expected return - lambda * variance
        objective = cp.Maximize(mu.T @ x - lambda_ * cp.quad_form(x, Q_psd))

        # Constraints
        constraints = [
            cp.sum(x) == 1,  # Sum to 1
            x >= 0  # Long-only
        ]

        # Create and solve problem
        prob = cp.Problem(objective, constraints)
        prob.solve(solver=cp.CLARABEL, verbose=False)

        if prob.status == cp.OPTIMAL and x.value is not None:
            weights = x.value
            # Additional normalization to ensure exact sum = 1
            weights = np.maximum(weights, 0)  # Ensure non-negative
            weights = weights / np.sum(weights)  # Normalize
            return weights, True
        else:
            return None, False

    except Exception as e:
        return None, False


def solve_portfolio_analytical(mu, Q, lambda_):
    """
    Analytical solution with robust handling
    """
    n_assets = len(mu)

    try:
        # Make Q invertible
        Q_reg = Q + np.eye(n_assets) * 1e-8

        A = pinv(Q_reg)  # Use pseudo-inverse for stability
        B = np.sum(A)
        C = A @ mu
        C_sum = np.sum(C)

        if abs(B) < 1e-12:
            # If B is too small, use equal weights
            return np.ones(n_assets) / n_assets

        nu = (2 * lambda_ * (C_sum - 1)) / B
        x = (1 / (2 * lambda_)) * A @ (mu - nu * np.ones(n_assets))

        # Ensure non-negative and normalize
        x = np.maximum(x, 0)
        x_sum = np.sum(x)

        if x_sum > 1e-12:
            return x / x_sum
        else:
            return np.ones(n_assets) / n_assets

    except Exception as e:
        return np.ones(n_assets) / n_assets


def normalize_weights_exact(weights):
    """
    Ensure weights sum to exactly 1.0
    """
    weights = np.maximum(weights, 0)  # Ensure non-negative
    weights_sum = np.sum(weights)

    if weights_sum > 1e-15:
        weights = weights / weights_sum

        # Fix any remaining floating point error
        residual = 1.0 - np.sum(weights)
        if abs(residual) > 1e-15:
            # Add residual to the largest weight
            max_idx = np.argmax(weights)
            weights[max_idx] += residual
    else:
        weights = np.ones(len(weights)) / len(weights)

    return weights


# Read the CSV file
file_path = 'adjusted_close_price_5y.csv'
df = pd.read_csv(file_path)

# Portfolio parameters
BOOK_SIZE = 10_000_000  # 10 million USD

start_time = time.time()
pivoted_df = df.pivot(index='date', columns='ticker', values='closePriceAdjusted')
T = pivoted_df.shape[0]
n_assets = pivoted_df.shape[1]

days = 63
x_t = []
mu_t = []
entropy_t = []
expected_return_t = []
portfolio_variance_t = []
portfolio_values = []  # Track portfolio value over time
daily_returns = []  # Track daily returns
dates = []  # Track dates for plotting

success_count = 0
fallback_count = 0
current_portfolio_value = BOOK_SIZE  # Initialize portfolio value

# Store prices for PnL calculation
price_data = pivoted_df.values
dates_index = pivoted_df.index

for i in range(days - 1, T):
    window = pivoted_df.iloc[i - days + 1:i + 1, :]
    returns_df = window.pct_change(fill_method=None).fillna(0)
    expected_returns_df = returns_df.ewm(span=21, adjust=False).mean()
    Q = window.cov().values
    mu = expected_returns_df.mean().values
    lambda_ = 0.01

    # Replace any NaN or inf values
    Q = np.nan_to_num(Q, nan=0.0, posinf=1e6, neginf=-1e6)
    mu = np.nan_to_num(mu, nan=0.0, posinf=1e6, neginf=-1e6)

    # Try CVXPY first
    x_CEMV, cvxpy_success = solve_portfolio_cvxpy(mu, Q, lambda_)

    if cvxpy_success:
        success_count += 1
    else:
        # Use analytical method
        x_CEMV = solve_portfolio_analytical(mu, Q, lambda_)
        fallback_count += 1

    # Final normalization to ensure exact sum = 1
    x_CEMV = normalize_weights_exact(x_CEMV)

    # Calculate daily return if not the first period
    if i > days - 1:  # We have a previous day to compare
        # Get prices for current and previous day
        prev_prices = price_data[i - 1]
        curr_prices = price_data[i]

        # Calculate daily return based on previous weights
        prev_weights = x_t[-1] if len(x_t) > 0 else np.ones(n_assets) / n_assets

        # Calculate price returns
        price_returns = (curr_prices - prev_prices) / prev_prices
        price_returns = np.nan_to_num(price_returns, nan=0.0, posinf=0.0, neginf=0.0)

        # Portfolio return
        portfolio_return = np.sum(prev_weights * price_returns)
        daily_returns.append(portfolio_return)

        # Update portfolio value
        current_portfolio_value *= (1 + portfolio_return)
        portfolio_values.append(current_portfolio_value)
        dates.append(dates_index[i])
    else:
        # First period - no return to calculate
        portfolio_values.append(BOOK_SIZE)
        daily_returns.append(0.0)
        dates.append(dates_index[i])

    x_t.append(x_CEMV)
    mu_t.append(mu)

    # Entropy calculation
    eps = 1e-12
    p = x_CEMV + eps  # Add small epsilon to avoid log(0)
    entropy = -np.sum(p * np.log(p))
    entropy_t.append(entropy)

    portfolio_variance_t.append(x_CEMV.T @ Q @ x_CEMV)
    expected_return_t.append(mu @ x_CEMV)

print(f"CVXPY successful: {success_count}/{T - days + 1} times")
print(f"Analytical fallback: {fallback_count}/{T - days + 1} times")

# Calculate PnL metrics
total_pnl = portfolio_values[-1] - BOOK_SIZE
total_return = (portfolio_values[-1] / BOOK_SIZE - 1) * 100
annualized_return = ((portfolio_values[-1] / BOOK_SIZE) ** (252 / len(portfolio_values)) - 1) * 100
volatility = np.std(daily_returns) * np.sqrt(252) * 100
sharpe_ratio = (annualized_return / 100) / (volatility / 100) if volatility > 0 else 0
max_drawdown = np.max(
    (np.maximum.accumulate(portfolio_values) - portfolio_values) / np.maximum.accumulate(portfolio_values)) * 100

# Portfolio Performance Summary
print(f"\n=== Portfolio Performance (Book Size: ${BOOK_SIZE:,.0f}) ===")
print(f"Final Portfolio Value: ${portfolio_values[-1]:,.2f}")
print(f"Total PnL: ${total_pnl:,.2f}")
print(f"Total Return: {total_return:.2f}%")
print(f"Annualized Return: {annualized_return:.2f}%")
print(f"Annualized Volatility: {volatility:.2f}%")
print(f"Sharpe Ratio: {sharpe_ratio:.3f}")
print(f"Maximum Drawdown: {max_drawdown:.2f}%")

# Convert to numpy arrays
x_t = np.array(x_t)
entropy_t = np.array(entropy_t)
expected_return_t = np.array(expected_return_t)
portfolio_variance_t = np.array(portfolio_variance_t)
mu_t = np.array(mu_t)
portfolio_values = np.array(portfolio_values)
daily_returns = np.array(daily_returns)

# Calculate PnL metrics
total_pnl = portfolio_values[-1] - BOOK_SIZE
total_return = (portfolio_values[-1] / BOOK_SIZE - 1) * 100
annualized_return = ((portfolio_values[-1] / BOOK_SIZE) ** (252 / len(portfolio_values)) - 1) * 100
volatility = np.std(daily_returns) * np.sqrt(252) * 100
sharpe_ratio = (annualized_return / 100) / (volatility / 100) if volatility > 0 else 0
max_drawdown = np.max(
    (np.maximum.accumulate(portfolio_values) - portfolio_values) / np.maximum.accumulate(portfolio_values)) * 100

# Check weight sums
final_sums = np.sum(x_t, axis=1)

print(f"\n=== Weight Sum Analysis ===")
print(f"Number of time steps: {len(final_sums)}")
print(f"Min sum: {np.min(final_sums):.15f}")
print(f"Max sum: {np.max(final_sums):.15f}")
print(f"Mean sum: {np.mean(final_sums):.15f}")
print(f"Std dev: {np.std(final_sums):.2e}")

# Check for exact equality to 1.0
exact_ones = np.sum(final_sums == 1.0)
print(f"Exactly equal to 1.0: {exact_ones}/{len(final_sums)}")

# Check within tolerance
tolerance = 1e-12
deviations = np.abs(final_sums - 1.0)
max_deviation = np.max(deviations)
within_tolerance = np.sum(deviations <= tolerance)

print(f"Max deviation: {max_deviation:.2e}")
print(f"Within tolerance ({tolerance:.0e}): {within_tolerance}/{len(final_sums)}")

if max_deviation <= tolerance:
    print(" SUCCESS: All portfolio weights sum to 1 within tolerance!")
else:
    problematic = np.where(deviations > tolerance)[0]
    print(f" {len(problematic)} time steps still have sum > 1 + tolerance")
    for idx in problematic[:5]:  # Show first 5
        print(f"  Step {idx}: sum = {final_sums[idx]:.15f}")

# Plot PnL Chart
plt.figure(figsize=(15, 10))

# Create subplots
gs = gridspec.GridSpec(3, 2, height_ratios=[2, 1, 1], hspace=0.3, wspace=0.3)

# 1. Portfolio Value Over Time
ax1 = plt.subplot(gs[0, :])
plt.plot(dates, portfolio_values, linewidth=2, color='blue', label='Portfolio Value')
plt.axhline(y=BOOK_SIZE, color='red', linestyle='--', alpha=0.7, label='Initial Value')
plt.title(f'Portfolio Value Over Time (Starting: ${BOOK_SIZE:,.0f})', fontsize=14, fontweight='bold')
plt.ylabel('Portfolio Value ($)', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

# Format y-axis to show values in millions
ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x / 1e6:.1f}M'))

# 2. Daily Returns
ax2 = plt.subplot(gs[1, 0])
plt.plot(dates, np.array(daily_returns) * 100, linewidth=1, color='green', alpha=0.7)
plt.title('Daily Returns (%)', fontsize=12, fontweight='bold')
plt.ylabel('Return (%)', fontsize=10)
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

# 3. Rolling Sharpe Ratio (30-day window)
ax3 = plt.subplot(gs[1, 1])
window_size = 30
if len(daily_returns) > window_size:
    rolling_sharpe = []
    for i in range(window_size, len(daily_returns)):
        window_returns = daily_returns[i - window_size:i]
        if np.std(window_returns) > 0:
            sharpe = np.mean(window_returns) / np.std(window_returns) * np.sqrt(252)
        else:
            sharpe = 0
        rolling_sharpe.append(sharpe)

    plt.plot(dates[window_size:], rolling_sharpe, linewidth=1, color='orange')
    plt.title(f'{window_size}-Day Rolling Sharpe Ratio', fontsize=12, fontweight='bold')
    plt.ylabel('Sharpe Ratio', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)

# 4. Drawdown
ax4 = plt.subplot(gs[2, :])
running_max = np.maximum.accumulate(portfolio_values)
drawdown = (running_max - portfolio_values) / running_max * 100
plt.fill_between(dates, drawdown, 0, color='red', alpha=0.3)
plt.plot(dates, drawdown, linewidth=1, color='red')
plt.title('Portfolio Drawdown (%)', fontsize=12, fontweight='bold')
plt.ylabel('Drawdown (%)', fontsize=10)
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

print("DONE")

================
File: test/test_redis.py
================
from backend.redis.client import REDIS_CLIENT


conn = REDIS_CLIENT.get_conn()

================
File: test/test_service_update_data_monthly.py
================
import asyncio
import time

from backend.modules.portfolio.services import (
    StocksUniverseService,
    PortfoliosService,
)


if __name__ == "__main__":
    try:
        # asyncio.run(StocksUniverseService.update_newest_data_all_monthly())
        start_time = time.time()
        print(" Updating Portfolio Weight Daily data...")
        asyncio.run(PortfoliosService.update_newest_data_all_daily())
        end_time = time.time()
        print(
            f" Universe Top Monthly data updated successfully in {end_time - start_time:.2f} seconds"
        )
    except Exception as e:
        print(f"An error occurred: {e}")
    else:
        print("Data updated successfully.")

================
File: test/test_time.py
================
import sys
import os
import datetime
import pytz

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backend.utils.time_utils import TimeUtils
from backend.modules.portfolio.services import PortfoliosService


# current_datetime = TimeUtils.get_current_vn_time()
# print(current_datetime)

# current_time = current_datetime.hour
# print(f"Current time in VN timezone: {current_time}")
# print(type(current_time))

# utcnow = datetime.datetime.now()
# print(utcnow)
# print(datetime.datetime.now(datetime.timezone.utc))

# print(pytz.timezone('Asia/Ho_Chi_Minh').utcoffset(utcnow))


from backend.modules.portfolio.services.data_providers.price_data_provider import (
    PriceDataProvider,
)


async def test_get_market_data():
    from_date = "2025-01-01"
    df = await PriceDataProvider.get_market_data()

    if df.empty:
        print("No data returned.")
    else:
        print(f"Data shape: {df.shape}")
        print(df.head())


async def test_update_portfolio_weights():
    user_id = 2
    portfolio_name = "Test Portfolio"
    symbols = ["VIC", "VHM", "MWG", "FPT", "VNM"]

    await PortfoliosService.create_custom_portfolio(
        user_id=user_id, portfolio_name=portfolio_name, symbols=symbols
    )
    print("Portfolio weights updated successfully.")


async def test_metadata_repo():
    from backend.modules.portfolio.repositories.portfolio_metadata import (
        PortfolioMetadataRepo,
    )

    conditions = {"portfolioId": "CUSTOM-3-PhuAnCut-78bcd64c"}
    records = await PortfolioMetadataRepo.get_by_condition(conditions=conditions)
    print(f"Records found: {records}")

    all_records = await PortfolioMetadataRepo.get_all()
    for record in all_records:
        print(record)


def test():
    unique_portfolio_ids = ["CUSTOM-3-PhuNgu-20b9a2cc", "CUSTOM-3-PhuOcCho-50fbf756"]
    portfolio_ids_string = "'" + "','".join(unique_portfolio_ids) + "'"
    print(portfolio_ids_string)


async def test_pnl_df():
    import json
    from backend.modules.portfolio.services import PortfoliosService

    portfolio_id = "SYSTEM-2025-08"
    portfolio_id = "CUSTOM-2-BANK-6CCD4D73"
    pnl_data = await PortfoliosService.get_portfolio_pnl(portfolio_id=portfolio_id)
    risk_metrics = pnl_data.get("risk_metrics", {})
    print(json.dumps(risk_metrics, indent=4, ensure_ascii=False))


if __name__ == "__main__":
    import asyncio

    # asyncio.run(test_update_portfolio_weights())
    # test()
    # asyncio.run(test_metadata_repo())
    asyncio.run(test_pnl_df())
